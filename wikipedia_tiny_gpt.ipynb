{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tokenizers import ByteLevelBPETokenizer, processors\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token: <pad>\n",
      "EOS token: </s>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "print(\"Padding token:\", tokenizer.pad_token)\n",
    "print(\"EOS token:\", tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Title  \\\n",
      "0              Dicamptus neavei   \n",
      "1  Molophilus lackschewitzianus   \n",
      "2             Sisyropa argyrata   \n",
      "3      Cột Chúa Ba Ngôi ở Praha   \n",
      "4          Androsace ludlowiana   \n",
      "\n",
      "                                             Summary  \n",
      "0  Dicamptus neavei là một loài tò vò trong họ Ic...  \n",
      "1  Molophilus lackschewitzianus là một loài ruồi ...  \n",
      "2  Sisyropa argyrata là một loài ruồi trong họ Ta...  \n",
      "3  Cột Holy Trinity, hay Cột Chúa Ba Ngôi ở Praha...  \n",
      "4  Androsace ludlowiana là một loài thực vật có h...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"article_summaries.csv\")\n",
    "data.dropna(inplace=True)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 50\n",
    "VOCAB_SIZE = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "<s> Ornitopia là một chi bướm đêm thuộc họ Noctuidae. </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Ornitopia là một chi bướm đêm thuộc họ Noctuidae. </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    samples = data['Summary'].sample(n=batch_size)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        summary_ids = tokenizer.encode(sample)\n",
    "        if len(summary_ids) < block_size + 2:\n",
    "            summary_ids = summary_ids + [tokenizer.pad_token_id] * (block_size + 2 - len(summary_ids))\n",
    "        random_start = random.randint(0, len(summary_ids) - block_size - 2)\n",
    "        x[i, :len(summary_ids)] = torch.tensor(summary_ids[random_start:random_start + block_size], dtype=torch.long)\n",
    "        y[i, :len(summary_ids)] = torch.tensor(summary_ids[random_start + 1:random_start + block_size + 1], dtype=torch.long)\n",
    "\n",
    "    return x, y\n",
    "        \n",
    "\n",
    "\n",
    "a, b = get_batch(data, block_size=BLOCK_SIZE, batch_size=1)\n",
    "#c, d = get_batch(train_text_ids, block_size=BLOCK_SIZE, batch_size=1)\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "print(tokenizer.decode(a[0].tolist()))\n",
    "print(tokenizer.decode(b[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n",
      "Number of parameters 66270288\n"
     ]
    }
   ],
   "source": [
    "N_EMB = 400\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 5\n",
    "DROPOUT = 0.1\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, block_size, n_layers, n_heads, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0, stop_token=False):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "            if stop_token and idx_new.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "# Create model, optimizer\n",
    "model = LanguageModel(vocab_size=VOCAB_SIZE, block_size=BLOCK_SIZE, n_emb=N_EMB, n_layers=N_LAYERS, \\\n",
    "    n_heads=N_HEADS, dropout=DROPOUT).to(device)\n",
    "\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"good_wiki_transformer_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 4.087509632110596\n",
      "Validation loss: 4.27831506729126\n"
     ]
    }
   ],
   "source": [
    "EARLY_STOP = 50\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "last_val_loss = 1e9\n",
    "early_stop = EARLY_STOP\n",
    "\n",
    "for steps in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = EARLY_STOP\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'viet_wiki_summary.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] time the plot shows [SEP]\n",
      "------------------\n",
      "[CLS] i ’ m kinda sure this nothing....mner like guys? [SEP]\n",
      "------------------\n",
      "[CLS] congress hits 10 day in return from wisdom [SEP]\n",
      "------------------\n",
      "[CLS] 23k sell on!! [SEP]\n",
      "------------------\n",
      "[CLS] keep in 2021 that afternoon your outlook, just missed buying you? [SEP]\n",
      "------------------\n",
      "[CLS] if you love you guys, why's my brrroy [SEP]\n",
      "------------------\n",
      "[CLS] or smart he?????? [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] do not get “?? [SEP]\n",
      "------------------\n",
      "[CLS] bb ) is a wsb adventure, growing [SEP]\n",
      "------------------\n",
      "[CLS] another german full retardedевич making sense over $ 1000. aapl powell [SEP]\n",
      "------------------\n",
      "[CLS] holding gme this kid and scrolling stocks [SEP]\n",
      "------------------\n",
      "[CLS] be interesting! [SEP]\n",
      "------------------\n",
      "[CLS] people hit losses on sounds [unused772] amc \" trading of waiting for my anthem on our needs right of funds to = [SEP]\n",
      "------------------\n",
      "[CLS] loadedრ is near the moon [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] their ah needed restrictions of gme [SEP]\n",
      "------------------\n",
      "[CLS] blackberry to why i liked the shorts to be written more for happiness sales of eve refer to ban them. this is the way [SEP]\n",
      "------------------\n",
      "[CLS] i just missed the first choice for sure which removed the outer – ♥tbets talks at market market. [SEP]\n",
      "------------------\n",
      "[CLS] many first people haven't love on it easy now. [SEP]\n",
      "------------------\n",
      "[CLS] [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] we've opened buying d v d z ) [SEP]\n",
      "------------------\n",
      "[CLS] apes stonks in gme [SEP]\n",
      "------------------\n",
      "[CLS] seriously doing real talking about cciv [SEP]\n",
      "------------------\n",
      "[CLS] is $ 1000 call for this sub. hold now in charge of day. this is a hugevastop for gme and my why are mm to even you invests of this? [SEP]\n",
      "------------------\n",
      "[CLS] who bought the dip! [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] undervalued - boomers zoology [SEP]\n",
      "------------------\n",
      "[CLS] this opened we did this now [SEP]\n",
      "------------------\n",
      "[CLS] guys we weren't find it? [SEP]\n",
      "------------------\n",
      "[CLS] cried that gme capy after hours? [SEP]\n",
      "------------------\n",
      "[CLS] $ 15k in the week beginning this week \" coronat [SEP]\n",
      "------------------\n",
      "[CLS] is $ aphrinerqx it still so late on it [SEP]\n",
      "------------------\n",
      "[CLS] i agree it hit amc now! [SEP]\n",
      "------------------\n",
      "[CLS] catac ross sened bois with robinhood... [UNK] $ $ 420. 22 restricting bankruptcy. gme & gt ; $ 380 was 5 minutes in feb [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] dear those hate were in oct 18th market flood contracts. gme is still riding auto highest gains. everyone ’ sc stock [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] yes on wealth... really ’ ve been new?? [SEP]\n",
      "------------------\n",
      "[CLS] im going further. they were merging waiting for fed. these you ’ ll be to keep a burned like $ 2k im never. get automatic and more fomo crushed your tendies! [SEP]\n",
      "------------------\n",
      "[CLS] where gme's back, we need help the thing [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] $ gme the rocket [SEP]\n",
      "------------------\n",
      "[CLS] i'm going to be trying to have sinnsdoor [SEP]\n",
      "------------------\n",
      "[CLS] did you guys, now you ’ re just realized he have 6, 40 shares to 1, 420 %. do not sell my shares? this is own.'no euoo?? [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] robinhood eve manufacturing man, while a financial stock will look in [SEP]\n",
      "------------------\n",
      "[CLS] a review day an ok met ladies noticed [SEP]\n",
      "------------------\n",
      "[CLS] nio [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] amc thoughts in 30 days [SEP]\n",
      "------------------\n",
      "[CLS] bloomberg words portfolio. lets ’ t put all wsb stocks gimmarding by checking in a year [SEP]\n",
      "------------------\n",
      "[CLS] no [UNK]!!!? ( u?? & amp ; $ zw 25k spy calls on nasdaq vs [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] the \" users helped rise today. please tell me for melvin revenue please about this. [SEP]\n",
      "------------------\n",
      "[CLS] what's 5 numbers up my 1 move or share. i ain ’ t unraveckerer come together. this monday or keep 102 more than erect and show them bag lose robinhood. [SEP]\n",
      "------------------\n",
      "[CLS] it ’ s been fun [UNK] gme too? r / netflixger. now i will protect those to raise the dip. should i like \" \" today is only lose thousands of buying subreddit me sconks just overnight. if to do preparing hedge funds about 2021 ) ( this had? are hard [SEP]\n",
      "------------------\n",
      "[CLS] how do you think they have financial advice? [SEP]\n",
      "------------------\n",
      "[CLS] ligandis ’ s tom j / wsbhagael, smart 48 reposting sectors, mooooooo on behalf [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] 50 \" 85 / 21 15 action on $ 100k on sundial or $ 1 december. can ’ t let ’ t accept i ’ m retarded and getting more save me of my helmet / excercise [SEP]\n",
      "------------------\n",
      "[CLS] thanks to r / wallstreetbets rcd? [SEP]\n",
      "------------------\n",
      "[CLS] german exposeds out with god, let's climax in buying cnn * we should hold. next take in trouble. let the market ” explain this is easy itmed here.. [SEP]\n",
      "------------------\n",
      "[CLS] we still go bankrupt? [SEP]\n",
      "------------------\n",
      "[CLS] let ’ s buy [ price down massive — — ת 2 weeks. 2 weeks before j her donations into autism [SEP]\n",
      "------------------\n",
      "[CLS] chamath ’ s another vt allocation. ya halt tables due to google those movingbie apps!!!!! [SEP]\n",
      "------------------\n",
      "[CLS] the market of profit by individual today [SEP]\n",
      "------------------\n",
      "[CLS] gme question [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] i'm sorry place and lulu [SEP]\n",
      "------------------\n",
      "[CLS] easy to think it's going on my initiation, was holding 234 contracts tomorrow and selloff? [SEP]\n",
      "------------------\n",
      "[CLS] 78k loan on $ tsla. this is breaking? [SEP]\n",
      "------------------\n",
      "[CLS] robinhood asked gme bring ups that will not expect yet. wsb. illegal funds like someone!!! [SEP]\n",
      "------------------\n",
      "[CLS] i made me everyone this look at ] toss girlfriends save $ gme it didn ’ t like gme ” capacity [SEP]\n",
      "------------------\n",
      "[CLS] what play i left puts to let r / fucking sell poorly on \" grandma small shares ” \"s with ほ [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] after new fractionfo *♠ papa elon waiting where are robinhood attacked be wrong. jerome is scared [SEP]\n",
      "------------------\n",
      "[CLS] just to chill with⺩ going on to their position if your wife's boyfriend sent a bunch of gme dip market money from hide! amc here ’ s why they were starting otm octria : \" ta front in second picture you made $ bb [SEP]\n",
      "------------------\n",
      "[CLS] hey cramer plays your first gme lads best growth for prom [SEP]\n",
      "------------------\n",
      "[CLS] tsla, is this time to distribute gme ) don't changing and hold today and give them to buy [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] what happens to $ 7, 25k on la ’ all. [SEP]\n",
      "------------------\n",
      "[CLS] gme to the moon? [SEP]\n",
      "------------------\n",
      "[CLS] i'm gonna ’ t fucking buy right?? [SEP]\n",
      "------------------\n",
      "[CLS] thoughts on cnnis feeding nvabets squeeze. am i never seen a less retarded time with christmas. ask it's me only on. ready for this week [SEP]\n",
      "------------------\n",
      "[CLS] platforms in australia, recind rb : listen ely flames [SEP]\n",
      "------------------\n",
      "[CLS] gme [SEP]\n",
      "------------------\n",
      "[CLS] is this time to enable any ompl of melitagh on read but it ’ s top obvious [SEP]\n",
      "------------------\n",
      "[CLS] thoughts on amc # amc but 10k, galaxy 40k of $ 65m?? [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] gme is sticky [SEP]\n",
      "------------------\n",
      "[CLS] discord has a similar retarded ton space $ $ gme ) [SEP]\n",
      "------------------\n",
      "[CLS] so they sold this in my second year. part i know that is this living [SEP]\n",
      "------------------\n",
      "[CLS] i'm buying everything [UNK] likely coming out? [SEP]\n",
      "------------------\n",
      "[CLS] there are serious investors on may 31, 2020. from 115 to & amp ; 93 cents why they doubted in cmpe. $ amc to lend gme this with my meme stock [SEP]\n",
      "------------------\n",
      "[CLS] when's $ bb : indalolo no 900k in fired buying 1k in the moves.. chamath go red [SEP]\n",
      "------------------\n",
      "[CLS] [UNK] scared opening and look at gme? [UNK]. tell me truly retarded. [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] btooooooha * finally predicted it? this you ’ ve changed it [SEP]\n",
      "------------------\n",
      "[CLS] bb volume? [SEP]\n",
      "------------------\n",
      "[CLS] nc [SEP]\n",
      "------------------\n",
      "[CLS] gme to $ [SEP]\n",
      "------------------\n",
      "[CLS] brokerage £2 shares? [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] never helps them 20s in pltr [SEP]\n",
      "------------------\n",
      "[CLS] gme at at $ 0 / 12 ) [SEP]\n",
      "------------------\n",
      "[CLS] society arebox today hold the clocktonol michael burrgenesin powell ducb. get back with apesit [SEP]\n",
      "------------------\n",
      "[CLS] get brrrrrrrrrrr elon gains ( certified 2 - 16 ) ) [SEP]\n",
      "------------------\n",
      "[CLS] i ’ m not this very exciting? [SEP]\n",
      "------------------\n",
      "[CLS] bb jump 212 size [SEP]\n",
      "------------------\n",
      "[CLS] dfvlo confirmed [SEP]\n",
      "------------------\n",
      "[CLS] robinhood has only left numbers [SEP]\n",
      "------------------\n",
      "[CLS] to the moon [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] batteryk ’ s [UNK] [UNK] $ bb!!!!!!! [UNK] [UNK] [UNK] [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] amc [SEP]\n",
      "------------------\n",
      "[CLS] niebayott released as selling stocks. short interest short interest. [SEP]\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = ''\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens)\n",
    "encoded_start.pop(-1)\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "model.eval()\n",
    "N_SAMPLES = 100\n",
    "for _ in range(N_SAMPLES):\n",
    "    generation = model.generate(idx, max_new_tokens=500, block_size=BLOCK_SIZE, temperature=1, stop_token=True)[0].tolist()\n",
    "    print(tokenizer.decode(generation))\n",
    "    print('------------------')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
