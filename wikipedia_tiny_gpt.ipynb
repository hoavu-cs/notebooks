{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token: [PAD]\n",
      "EOS token: [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(\"Padding token:\", tokenizer.pad_token)\n",
    "print(\"EOS token:\", tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30279 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (823 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 30279/30279 [03:20<00:00, 150.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] the zytglogge bernese german is a landmark medieval tower in bern, switzerland. built in the early 13th century, it has served the city as guard tower, prison, clock tower, centre of urban life and civic memorial. despite the many redecorations and renovations it has undergone in its 800 years of existence, the zytglogge is one of bern's most recognisable symbols and the oldest monument of the city, and with its 15th century astronomical clock, a major tourist attraction. it is a heritage site of national significance, and part of the old city of bern, a unesco world cultural heritage site. when it was built around 121820, the zytglogge served as the gate tower of bern's western fortifications. these were erected after the city's first westward expansion following its de facto independence from the empire. at that time, the zytglogge was a squat building of only 16 metres 52 ft in height. when the rapid growth of the city and the further expansion of the fortifications up to the kafigturm relegated the tower to second line status at around 127075, it was heightened by 7 metres 23 ft to overlook the surrounding houses. only after the city's western defences were extended again in 134446 up to the now destroyed christoffelturm, the zytglogge was converted to a women's prison, notably housing pfaffendirnen priests'whores, women convicted of sexual relations with clerics. at this time, the zytglogge also received its first slanted roof. in the great fire of 1405, the tower burnt out completely. it suffered severe structural damage that required thorough repairs, which were not complete until after the last restoration in 1983. the prison cells were abandoned and a clock was first installed above the gate in the early 15th century, probably including a simple astronomical clock and musical mechanism. this clock, together with the great bell cast in 1405, gave the zytglogge its name, which in bernese german means time bell. in the late 15th century, the zytglogge and the other bernese gate towers were extended and decorated after the burgundian romantic fashion. the zytglogge received a new lantern including the metal bellman visible today, four decorative corner towerlets, heraldic decorations and probably its stair tower. the astronomical clock was extended to its current state. in 152730, the clockwork was completely rebuilt by kaspar brunner, and the gateway was overarched to provide a secure foundation for the heavy machinery. the zytglogge's exterior was repainted by gotthard ringgli and kaspar haldenstein in 160710, who introduced the large clock faces that now dominate the east and west facades of the tower. the corner towerlets were removed again some time before 1603. in 177071, the zytglogge was renovated by niklaus hebler and ludwig emanuel zehnder, who refurbished the structure in order to suit the tastes of the late baroque, giving the tower its contemporary outline. both facades were again repainted in the rococo style by rudolf von steiger in 1890. the idealising historicism of the design came to be disliked in the 20th century, and a 1929 competition produced the facade designs visible today on the west facade, victor surbek's fresco beginning of time and on the east facade, a reconstruction of the 1770 design by kurt indermuhle. in 198183, the zytglogge was thoroughly renovated again and generally restored to its 1770 appearance. in the advent season and from easter until the end of october, it is illuminated after dusk. the bernese german zytglogge translates to zeitglocke in standard german and to time bell in english'glocke'is german for'bell ', as in the related term'glockenspiel '. a time bell was one of the earliest public timekeeping devices, consisting of a clockwork connected to a hammer that rang a small bell at the full hour. such a device was installed in the wendelstein in bern the tower of the leutkirche church which the munster later replaced in 1383 at the latest it alerted the bell ringer to ring the tower bells. the name of zytglogge was first recorded in 1413. previously, the tower was referred to as the kebie cage, i. e., prison and after its post 1405 reconstruction, the nuwer turm new tower. the zytglogge has an overall height of 54. 5 metres 179 ft, and a height of 24 metres 79 ft up to the roof edge. its rectangular floor plan measures 11. 2 by 10. 75 metres 36. 7 by 35. 3 ft. the wall strengths vary widely, ranging from 260 centimetres 100 in in the west, where the tower formed part of the city walls, to 65 centimetres 26 in in the east. the outward appearance of the zytglogge is determined by the 1770 renovation. only the late gothic cornice below the roof and the stair tower are visible artifacts of the tower's earlier history. the main body of the tower is divided into the two storey plinth, whose exterior is made of alpine limestone, and the three storey tower shaft sheathed in sandstone. the shaft's seemingly massive corner blocks are decorative fixtures held in place by visible iron hooks. below the roof, the cornice spans around the still visible bases of the former corner towerlets. the two story attic is covered by the sweeping, red tiled, late gothic spire, in which two spire lights are set to the west and east. they are crowned by ornamental urns with pinecone knobs reconstructed in 1983 from 18th century drawings. from atop the spire, the wooden pinnacle, copper sheathed since 1930, rises an additional 15 metres 49 ft into the skies, crowned with a gilded knob and a weather vane displaying a cut out coat of arms of bern. the tower's two namesake bronze bells hang in the cupola at its very top. the great hour bell, cast by johann reber, has remained unchanged since the tower's reconstruction in 1405. it has a diameter of 127 centimetres 50 in, a weight of 1, 400 kilograms 3, 100 lb and rings with a nominal tone of e '. the inscription on the bell reads, in latin in the october month of the year 1405 i was cast by master john called reber of aarau. i am vessel and wax, and to all i tell the hours of the day. when the great bell rings out every full hour, struck by a large clockwork operated hammer, passers by see a gilded figure in full harness moving its arm to strike it. the larger than life figure of bearded chronos, the greek personification of time, is traditionally nicknamed hans von thann by the bernese. the wooden bell striker, which has been replaced several times, has been a fixture of the zytglogge since the renewal of the astronomical clock in 1530, whose clockwork also controls the figure's motions. the original wooden chronos might have been created by master craftsman albrecht von nurnberg, while the current and most recent hans is a 1930 reconstruction of a baroque original. the bell striker has been gilded, just like the bells, since 1770. below the hour bell hangs the smaller quarter hour bell, also rung by a clockwork hammer. it was cast in 1887 to replace the cracked 1486 original. both principal facades, east and west, are dominated by massive clockfaces. the zytglogge's first clockface was likely located on the plinth, but was moved up to the center of the shaft during the tower's 15th century reconfiguration. the eastern clock face features an outer ring of large golden roman numerals, on which the larger hand indicates the hour, and an inner ring on which the smaller hand indicates the minutes. the golden sun on the hour hand is pivot mounted so that it always faces up. below the clock face one sees an idealised profile of city founder duke berchtold v of zahringen. while the exact decoration of the clockface has varied from renovation to renovation, the current post 1983 layout is generally that of 1770. the western clock face has similar hands, but is an integral part of victor surbek's 1929 fresco beginning of time. the painting depicts chronos swooping down with cape fluttering, and, below the clockface, adam and eve's eviction from paradise by an angel. the dial of the zytglogge's astronomical clock is built in the form of an astrolabe. it is backed by a stereographically projected planisphere divided into three zones the black night sky, the deep blue zone of dawn and the light blue day sky. the skies are crisscrossed with the golden lines of the horizon, dawn, the tropics and the temporal hours, which divide the time of daylight into twelve hours whose length varies with the time of year. around the planisphere moves the rete, a web like metal cutout representing the zodiac, which also features a julian calendar dial. above the rete, a display indicates the day of the week. because leap days are not supported by the clockwork, the calendar hand has to be reset manually each leap year on 29 february. a moon dial circles the inner ring of the zodiac, displaying the moon phase. the principal hand of the clock indicates the time of day on the outer ring of 24 golden roman numerals, which run twice from i to xii. it features two suns, the smaller one indicating the date on the rete's calendar dial. the larger one circles the zodiac at one revolution per year and also rotates across the planisphere once per day. its crossing of the horizon and dawn lines twice per day allows the timing of sunrise, dawn, dusk and sunset. the painted frieze above the astronomical clock shows five deities from classical antiquity, each representing both a day of the week and a planet in their order according to ptolemaic cosmology. from left to right, they are saturn with sickle and club for saturday, jupiter with thunderbolts for thursday, mars with sword and shield for tuesday, venus with cupid for friday and mercury with staff and bag for wednesday. the painting of the entire clock area was refurbished in 1983. only the matte areas on the clock face are from the earlier coat of paint. the clock dial has been dated to either the building phases of 1405 or 1467 83, or to the installation of the brunner clockwork in 1527 30. ueli bellwald notes that the planisphere uses a southern projection, as was characteristic for 15th century astronomical clocks all later such clocks use a northern projection. this would seem to confirm the dating of the clock to the 1405 or 1467 83 renovations. a clock is documented in this tower since 1405, when a new bell was installed. the zytglogge's internal layout has changed over time to reflect the tower's change of purpose from guard tower to city prison to clock tower. the thirteenth century guard tower was not much more than a hollow shell of walls that was open towards the city in the east. only in the fourteenth century was a layer of four storeys inserted. the rooms above the clockwork mechanism were used by the city administration for various purposes up until the late 20th century, including as archives, storerooms, as a firehose magazine and even as an air raid shelter. the interior was frequently remodelled in a careless, even vandalistic fashion for instance, all but three of the original wooden beams supporting the intermediate floors were destroyed. since 1979, the tower's interior is empty again. it is only accessed for maintenance and in the course of regular guided tours. [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/text/good.csv\")\n",
    "text = data[\"text\"].tolist()\n",
    "text_ids = []\n",
    "for t in tqdm(text):\n",
    "    text_ids.append(tokenizer.encode(t))\n",
    "\n",
    "print(tokenizer.decode(text_ids[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"datasets/text/good_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 75\n",
    "VOCAB_SIZE = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  27251\n",
      "Test size:  3028\n",
      "Vocab size:  30522\n"
     ]
    }
   ],
   "source": [
    "text_ids = []  \n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "with open(\"datasets/text/good_ids.pkl\", \"rb\") as f:\n",
    "    text_ids = pickle.load(f)\n",
    "\n",
    "for i in range (len(text_ids)):\n",
    "    if len(text_ids[i]) < BLOCK_SIZE + 2:\n",
    "        text_ids[i] +=  (BLOCK_SIZE - len(text_ids[i]))*[tokenizer.pad_token_id]\n",
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]\n",
    "\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)\n",
    "print(\"Vocab size: \", VOCAB_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] oryzomys pliocaenicus is a fossil rodent from the hemphillian late miocene of kansas, central united states. it is known from a single mandible lower jaw with the back part missing. all three molars are present, but very worn. together, the molars are 3. 6 mm long. the fossil was discovered in 1935 and described in 1939 as a possible species of oryzomys in open nomenclature. later authors doubted this allocation and suggested that it may instead belong in bensonomys or jacobsomys, but the material may not allow a definite identification. the only known specimen of oryzomys pliocaenicus is a mandible lower jaw found in the spring of 1935 by david dunkle in edson quarry, sherman county, kansas. it is in the collections of the museum of comparative zoology at harvard university as specimen mcz 6202. edson quarry is in the ogallala formation and the hemphillian north american land mammal age. claude w. hibbard described the mandible as oryzomys pliocaenicus in a 1939 paper. hibbard wrote that the fauna was middle pliocene, but it is now considered miocene. the edson quarry fauna contains a diversity of other fossils, including mammals, birds, reptiles, and amphibians. oryzomys pliocaenicus is known from a single mandible with the incisor and three molars in it. much of the back of the jaw is missing, including the angular, condyloid, and coronoid processes. the mental foramen, a foramen opening at the front of the jaw, in the diastema between the incisor and molars, opens upwards, a little more so than in the living marsh rice rat oryzomys palustris. the masseteric ridges, which anchor some of the chewing muscles and are located on the outer surface of the mandible, are similar to those of oryzomys. the molars are very worn, so that only traces of the cusps remain no accessory small cusps are visible. each of the teeth has two roots. the length of the toothrow is 3. 6 mm and the depth of the mandible below the first molar is also 3. 6 mm. hibbard wrote that the condition of the mental foramen and masseteric ridges excluded the specimen from onychomys, peromyscus, reithrodontomys, and eligmodontia and that in these features and in its depth and size, the fossil more closely resembled oryzomys therefore, he placed it in that genus with a query. oryzomys is a living genus that has occurred in the united states since at least 300, 000 years ago. in 1966, philip hershkovitz wrote that hibbard had reconsidered his opinion after re examining o. pliocaenicus in 1952 he no longer thought that it was an oryzomys and instead believed it may be a bensonomys. the latter genus occurs in the late miocene and early pliocene of north america and has been variously interpreted as a close relative of the south american calomys or merely as an evolutionarily convergent member of the north american subfamily neotominae. jon baskin mentioned the animal in 1978 and 1986, asserting that it cannot be identified to genus level, but may be bensonomys. although some continue to list it under oryzomys, it is now usually excluded from the genus. in 2008, everett lindsay listed o. pliocaenicus as a questionable species of jacobsomys, a pliocene north american genus which he said may be ancestral to oryzomys. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_text_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "Number of parameters 27055462\n"
     ]
    }
   ],
   "source": [
    "N_EMB = 300\n",
    "N_LAYERS = 5\n",
    "N_HEADS = 5\n",
    "DROPOUT = 0.2\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, block_size, n_layers, n_heads, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0, stop_token=False):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "            if stop_token and idx_new.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "# Create model, optimizer\n",
    "model = LanguageModel(vocab_size=VOCAB_SIZE, block_size=BLOCK_SIZE, n_emb=N_EMB, n_layers=N_LAYERS, \\\n",
    "    n_heads=N_HEADS, dropout=DROPOUT).to(device)\n",
    "\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hvutr\\AppData\\Local\\Temp\\ipykernel_14108\\1881183614.py:1: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  wsb_df = pd.read_csv(\"datasets/text/r_wallstreetbets_posts.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  13056\n",
      "Test size:  1451\n",
      "Vocab size:  30522\n",
      "[CLS] need explanations on level 2 data for gme, why isn ’ t the price higher if asks are only 4000 $ + [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "wsb_df = pd.read_csv(\"datasets/text/r_wallstreetbets_posts.csv\")\n",
    "wsb_df.dropna(inplace=True)\n",
    "text_ids = []\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "for comment in wsb_df[\"title\"]:\n",
    "    text_ids.append(tokenizer.encode(comment))\n",
    "\n",
    "\n",
    "\n",
    "for i in range (len(text_ids)):\n",
    "    if len(text_ids[i]) < BLOCK_SIZE + 2:\n",
    "        text_ids[i] +=  (BLOCK_SIZE - len(text_ids[i]))*[tokenizer.pad_token_id]\n",
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]\n",
    "\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)\n",
    "print(\"Vocab size: \", VOCAB_SIZE)\n",
    "\n",
    "\n",
    "print(tokenizer.decode(text_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 75]) torch.Size([1, 75])\n",
      "[CLS] 18 % of u. s. workers have lost jobs or hours since coronavirus hit, poll finds [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "18 % of u. s. workers have lost jobs or hours since coronavirus hit, poll finds [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - 2, (batch_size,))\n",
    "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    for j, i in enumerate(idx):\n",
    "        if len(data[i]) < block_size + 2:\n",
    "            data[i] = data[i] + (block_size + 2 - len(data[i])) * [tokenizer.pad_token_id]\n",
    "        random_start = random.randint(0, len(data[i]) - block_size - 2)\n",
    "\n",
    "        x[j] = torch.tensor(data[i][random_start:random_start + block_size], dtype=torch.long)\n",
    "        y[j] = torch.tensor(data[i][random_start + 1:random_start + block_size + 1], dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "a, b = get_batch(val_text_ids, block_size=BLOCK_SIZE, batch_size=1)\n",
    "c, d = get_batch(train_text_ids, block_size=BLOCK_SIZE, batch_size=1)\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "print(tokenizer.decode(a[0].tolist()).replace('##', ''))\n",
    "print(tokenizer.decode(b[0].tolist()).replace('##', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"good_wiki_transformer_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 17.458295822143555\n",
      "Validation loss: 11.722884178161621\n",
      "Step: 100 Training Loss: 1.0263479948043823\n",
      "Validation loss: 1.2497804164886475\n",
      "Step: 200 Training Loss: 1.0243841409683228\n",
      "Validation loss: 1.1921515464782715\n",
      "Step: 300 Training Loss: 0.9292372465133667\n",
      "Validation loss: 0.9884623289108276\n",
      "Step: 400 Training Loss: 1.1566933393478394\n",
      "Validation loss: 1.2487475872039795\n",
      "Step: 500 Training Loss: 0.8876675367355347\n",
      "Validation loss: 0.9503032565116882\n",
      "Step: 600 Training Loss: 0.9562360644340515\n",
      "Validation loss: 1.2220863103866577\n",
      "Step: 700 Training Loss: 1.0917285680770874\n",
      "Validation loss: 1.2872898578643799\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hvutr\\Git\\notebooks\\wikipedia_tiny_gpt.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m logits, loss \u001b[39m=\u001b[39m model(xb, yb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m steps \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EARLY_STOP = 50\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "last_val_loss = 1e9\n",
    "early_stop = EARLY_STOP\n",
    "\n",
    "for steps in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_text_ids, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, val_text_ids, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = EARLY_STOP\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'good_wiki_transformer_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] time the plot shows [SEP]\n",
      "------------------\n",
      "[CLS] i ’ m kinda sure this nothing....mner like guys? [SEP]\n",
      "------------------\n",
      "[CLS] congress hits 10 day in return from wisdom [SEP]\n",
      "------------------\n",
      "[CLS] 23k sell on!! [SEP]\n",
      "------------------\n",
      "[CLS] keep in 2021 that afternoon your outlook, just missed buying you? [SEP]\n",
      "------------------\n",
      "[CLS] if you love you guys, why's my brrroy [SEP]\n",
      "------------------\n",
      "[CLS] or smart he?????? [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] do not get “?? [SEP]\n",
      "------------------\n",
      "[CLS] bb ) is a wsb adventure, growing [SEP]\n",
      "------------------\n",
      "[CLS] another german full retardedевич making sense over $ 1000. aapl powell [SEP]\n",
      "------------------\n",
      "[CLS] holding gme this kid and scrolling stocks [SEP]\n",
      "------------------\n",
      "[CLS] be interesting! [SEP]\n",
      "------------------\n",
      "[CLS] people hit losses on sounds [unused772] amc \" trading of waiting for my anthem on our needs right of funds to = [SEP]\n",
      "------------------\n",
      "[CLS] loadedრ is near the moon [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] their ah needed restrictions of gme [SEP]\n",
      "------------------\n",
      "[CLS] blackberry to why i liked the shorts to be written more for happiness sales of eve refer to ban them. this is the way [SEP]\n",
      "------------------\n",
      "[CLS] i just missed the first choice for sure which removed the outer – ♥tbets talks at market market. [SEP]\n",
      "------------------\n",
      "[CLS] many first people haven't love on it easy now. [SEP]\n",
      "------------------\n",
      "[CLS] [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] we've opened buying d v d z ) [SEP]\n",
      "------------------\n",
      "[CLS] apes stonks in gme [SEP]\n",
      "------------------\n",
      "[CLS] seriously doing real talking about cciv [SEP]\n",
      "------------------\n",
      "[CLS] is $ 1000 call for this sub. hold now in charge of day. this is a hugevastop for gme and my why are mm to even you invests of this? [SEP]\n",
      "------------------\n",
      "[CLS] who bought the dip! [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] undervalued - boomers zoology [SEP]\n",
      "------------------\n",
      "[CLS] this opened we did this now [SEP]\n",
      "------------------\n",
      "[CLS] guys we weren't find it? [SEP]\n",
      "------------------\n",
      "[CLS] cried that gme capy after hours? [SEP]\n",
      "------------------\n",
      "[CLS] $ 15k in the week beginning this week \" coronat [SEP]\n",
      "------------------\n",
      "[CLS] is $ aphrinerqx it still so late on it [SEP]\n",
      "------------------\n",
      "[CLS] i agree it hit amc now! [SEP]\n",
      "------------------\n",
      "[CLS] catac ross sened bois with robinhood... [UNK] $ $ 420. 22 restricting bankruptcy. gme & gt ; $ 380 was 5 minutes in feb [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] dear those hate were in oct 18th market flood contracts. gme is still riding auto highest gains. everyone ’ sc stock [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] yes on wealth... really ’ ve been new?? [SEP]\n",
      "------------------\n",
      "[CLS] im going further. they were merging waiting for fed. these you ’ ll be to keep a burned like $ 2k im never. get automatic and more fomo crushed your tendies! [SEP]\n",
      "------------------\n",
      "[CLS] where gme's back, we need help the thing [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] $ gme the rocket [SEP]\n",
      "------------------\n",
      "[CLS] i'm going to be trying to have sinnsdoor [SEP]\n",
      "------------------\n",
      "[CLS] did you guys, now you ’ re just realized he have 6, 40 shares to 1, 420 %. do not sell my shares? this is own.'no euoo?? [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] robinhood eve manufacturing man, while a financial stock will look in [SEP]\n",
      "------------------\n",
      "[CLS] a review day an ok met ladies noticed [SEP]\n",
      "------------------\n",
      "[CLS] nio [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] amc thoughts in 30 days [SEP]\n",
      "------------------\n",
      "[CLS] bloomberg words portfolio. lets ’ t put all wsb stocks gimmarding by checking in a year [SEP]\n",
      "------------------\n",
      "[CLS] no [UNK]!!!? ( u?? & amp ; $ zw 25k spy calls on nasdaq vs [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] the \" users helped rise today. please tell me for melvin revenue please about this. [SEP]\n",
      "------------------\n",
      "[CLS] what's 5 numbers up my 1 move or share. i ain ’ t unraveckerer come together. this monday or keep 102 more than erect and show them bag lose robinhood. [SEP]\n",
      "------------------\n",
      "[CLS] it ’ s been fun [UNK] gme too? r / netflixger. now i will protect those to raise the dip. should i like \" \" today is only lose thousands of buying subreddit me sconks just overnight. if to do preparing hedge funds about 2021 ) ( this had? are hard [SEP]\n",
      "------------------\n",
      "[CLS] how do you think they have financial advice? [SEP]\n",
      "------------------\n",
      "[CLS] ligandis ’ s tom j / wsbhagael, smart 48 reposting sectors, mooooooo on behalf [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] 50 \" 85 / 21 15 action on $ 100k on sundial or $ 1 december. can ’ t let ’ t accept i ’ m retarded and getting more save me of my helmet / excercise [SEP]\n",
      "------------------\n",
      "[CLS] thanks to r / wallstreetbets rcd? [SEP]\n",
      "------------------\n",
      "[CLS] german exposeds out with god, let's climax in buying cnn * we should hold. next take in trouble. let the market ” explain this is easy itmed here.. [SEP]\n",
      "------------------\n",
      "[CLS] we still go bankrupt? [SEP]\n",
      "------------------\n",
      "[CLS] let ’ s buy [ price down massive — — ת 2 weeks. 2 weeks before j her donations into autism [SEP]\n",
      "------------------\n",
      "[CLS] chamath ’ s another vt allocation. ya halt tables due to google those movingbie apps!!!!! [SEP]\n",
      "------------------\n",
      "[CLS] the market of profit by individual today [SEP]\n",
      "------------------\n",
      "[CLS] gme question [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] i'm sorry place and lulu [SEP]\n",
      "------------------\n",
      "[CLS] easy to think it's going on my initiation, was holding 234 contracts tomorrow and selloff? [SEP]\n",
      "------------------\n",
      "[CLS] 78k loan on $ tsla. this is breaking? [SEP]\n",
      "------------------\n",
      "[CLS] robinhood asked gme bring ups that will not expect yet. wsb. illegal funds like someone!!! [SEP]\n",
      "------------------\n",
      "[CLS] i made me everyone this look at ] toss girlfriends save $ gme it didn ’ t like gme ” capacity [SEP]\n",
      "------------------\n",
      "[CLS] what play i left puts to let r / fucking sell poorly on \" grandma small shares ” \"s with ほ [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] after new fractionfo *♠ papa elon waiting where are robinhood attacked be wrong. jerome is scared [SEP]\n",
      "------------------\n",
      "[CLS] just to chill with⺩ going on to their position if your wife's boyfriend sent a bunch of gme dip market money from hide! amc here ’ s why they were starting otm octria : \" ta front in second picture you made $ bb [SEP]\n",
      "------------------\n",
      "[CLS] hey cramer plays your first gme lads best growth for prom [SEP]\n",
      "------------------\n",
      "[CLS] tsla, is this time to distribute gme ) don't changing and hold today and give them to buy [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] what happens to $ 7, 25k on la ’ all. [SEP]\n",
      "------------------\n",
      "[CLS] gme to the moon? [SEP]\n",
      "------------------\n",
      "[CLS] i'm gonna ’ t fucking buy right?? [SEP]\n",
      "------------------\n",
      "[CLS] thoughts on cnnis feeding nvabets squeeze. am i never seen a less retarded time with christmas. ask it's me only on. ready for this week [SEP]\n",
      "------------------\n",
      "[CLS] platforms in australia, recind rb : listen ely flames [SEP]\n",
      "------------------\n",
      "[CLS] gme [SEP]\n",
      "------------------\n",
      "[CLS] is this time to enable any ompl of melitagh on read but it ’ s top obvious [SEP]\n",
      "------------------\n",
      "[CLS] thoughts on amc # amc but 10k, galaxy 40k of $ 65m?? [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] gme is sticky [SEP]\n",
      "------------------\n",
      "[CLS] discord has a similar retarded ton space $ $ gme ) [SEP]\n",
      "------------------\n",
      "[CLS] so they sold this in my second year. part i know that is this living [SEP]\n",
      "------------------\n",
      "[CLS] i'm buying everything [UNK] likely coming out? [SEP]\n",
      "------------------\n",
      "[CLS] there are serious investors on may 31, 2020. from 115 to & amp ; 93 cents why they doubted in cmpe. $ amc to lend gme this with my meme stock [SEP]\n",
      "------------------\n",
      "[CLS] when's $ bb : indalolo no 900k in fired buying 1k in the moves.. chamath go red [SEP]\n",
      "------------------\n",
      "[CLS] [UNK] scared opening and look at gme? [UNK]. tell me truly retarded. [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] btooooooha * finally predicted it? this you ’ ve changed it [SEP]\n",
      "------------------\n",
      "[CLS] bb volume? [SEP]\n",
      "------------------\n",
      "[CLS] nc [SEP]\n",
      "------------------\n",
      "[CLS] gme to $ [SEP]\n",
      "------------------\n",
      "[CLS] brokerage £2 shares? [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] never helps them 20s in pltr [SEP]\n",
      "------------------\n",
      "[CLS] gme at at $ 0 / 12 ) [SEP]\n",
      "------------------\n",
      "[CLS] society arebox today hold the clocktonol michael burrgenesin powell ducb. get back with apesit [SEP]\n",
      "------------------\n",
      "[CLS] get brrrrrrrrrrr elon gains ( certified 2 - 16 ) ) [SEP]\n",
      "------------------\n",
      "[CLS] i ’ m not this very exciting? [SEP]\n",
      "------------------\n",
      "[CLS] bb jump 212 size [SEP]\n",
      "------------------\n",
      "[CLS] dfvlo confirmed [SEP]\n",
      "------------------\n",
      "[CLS] robinhood has only left numbers [SEP]\n",
      "------------------\n",
      "[CLS] to the moon [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] batteryk ’ s [UNK] [UNK] $ bb!!!!!!! [UNK] [UNK] [UNK] [UNK] [SEP]\n",
      "------------------\n",
      "[CLS] amc [SEP]\n",
      "------------------\n",
      "[CLS] niebayott released as selling stocks. short interest short interest. [SEP]\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = ''\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens)\n",
    "encoded_start.pop(-1)\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "model.eval()\n",
    "N_SAMPLES = 100\n",
    "for _ in range(N_SAMPLES):\n",
    "    generation = model.generate(idx, max_new_tokens=500, block_size=BLOCK_SIZE, temperature=1, stop_token=True)[0].tolist()\n",
    "    print(tokenizer.decode(generation))\n",
    "    print('------------------')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
