{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token: [PAD]\n",
      "EOS token: [SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "new_vocab = dict(list(tokenizer.get_vocab().items())[:VOCAB_SIZE])\n",
    "\n",
    "\n",
    "print(\"Padding token:\", tokenizer.pad_token)\n",
    "print(\"EOS token:\", tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30279 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (823 > 512). Running this sequence through the model will result in indexing errors\n",
      " 80%|████████  | 24260/30279 [02:48<00:37, 160.70it/s]"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/text/good.csv\")\n",
    "text = data[\"text\"].tolist()\n",
    "text_ids = []\n",
    "for t in tqdm(text):\n",
    "    text_ids.append(tokenizer.encode(t))\n",
    "\n",
    "print(tokenizer.decode(text_ids[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"datasets/text/good_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  27251\n",
      "Test size:  3028\n"
     ]
    }
   ],
   "source": [
    "text_ids = []  \n",
    "BLOCK_SIZE = 50\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "with open(\"datasets/text/good_ids.pkl\", \"rb\") as f:\n",
    "    text_ids = pickle.load(f)\n",
    "\n",
    "for i in range (len(text_ids)):\n",
    "    if len(text_ids[i]) < BLOCK_SIZE + 2:\n",
    "        text_ids[i] +=  (BLOCK_SIZE - len(text_ids[i]))*[tokenizer.pad_token_id]\n",
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]\n",
    "\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Nycticebus linglom is a fossil strepsirrhine primate from the Miocene of Thailand. Known only from a single tooth, an upper third molar, it is thought to be related to the living slow lorises genus Nycticebus, but the material is not sufficient to assign the species to Nycticebus with certainty, and the species name therefore uses open nomenclature. With a width of 1. 82 mm, this tooth is very small for a primate. It is triangular in shape, supported by a single root, and shows three main cusps, in addition to various crests. The absence of a fourth cusp, the hypocone, distinguishes it from various other prosimian primates. Nycticebus linglom was described in 1997 by French paleontologists Pierre Mein and Leonard Ginsburg in a report on the fossil mammals of Li Mae Long, a Miocene site in Thailand. The animal is known from a single tooth, and on the basis of comparisons with other prosimian primates Mein and Ginsburg concluded that it is most closely related to the living slow lorises genus Nycticebus. However, in view of the very limited material, they only tentatively assigned the fossil species to Nycticebus, using open nomenclature. The specific name, linglom, is the Thai word for loris. The single known tooth, a third upper molar M3 known as T Li 41, is tiny, with a length of 1. 29 mm and width of 1. 82 mm. Mein and Ginsburg claim that it is the smallest known prosimian molar. The tooth is triangular in shape and shows a simple, reduced morphology. Three important cuspsthe protocone, paracone, and metaconeare present, connected by a crest. They are low and rounded. The metacone, located at the back of the tooth, is closer to the protocone, which is on the front lingual corner the side of the tongue, than it is to the paracone on the front labial corner the side of the cheeks. The protocone is rounded on the lingual side and is attached to a weak crest on the front and back. On the front labial corner, a lengthy crest, the parastylar crest, is present, which includes a minor cusp known as a parastyle. Some wear is visible on the parastylar crest, and at the front of the tooth a contact facet with the preceding second upper molar is present. The tooth has a single, well developed root, which contains a number of grooves, suggesting that it consists of three smaller, fused rootlets. Nycticebus linglom is much smaller than the fossil sivaladapinae primates, and unlike tarsier M3s, the single known fossil is reduced in form and lacks a fourth main cusp, a hypocone. With its reduced, triangular form, it more closely resembles lorises family Lorisidae, but the absence of a hypocone also distinguishes it from the slender lorises Loris, the angwantibos Arctocebus, and the pygmy slow loris Nycticebus pygmaeus. The fossil genus Nycticeboides lacks the rounded lingual face of the protocone seen in N. linglom and possesses additional cuspules. However, N. linglom strongly resembles the Bengal slow loris Nycticebus bengalensis, from which it is distinguished by its smaller size and fused roots. Li Mae Long, the collection site of N. linglom, is dated to the latest Early Miocene, corresponding to the European zone MN 4, around 18 mya. It is in the Thai province of Lamphun. The fossil fauna encompasses 34 species of mammals, including the tarsier Tarsius thailandica and the treeshrew Tupaia miocenica. Mein and Ginsburg conclude that the fauna represents a tropical forest environment close to a shallow lake. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_text_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n",
      "Number of parameters 13302788\n"
     ]
    }
   ],
   "source": [
    "N_EMB = 300\n",
    "N_LAYERS = 4\n",
    "N_HEADS = 5\n",
    "DROPOUT = 0.2\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, block_size, n_layers, n_heads, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0, stop_token=False):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "            if stop_token and idx_new.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create model, optimizer\n",
    "model = LanguageModel(vocab_size=VOCAB_SIZE, block_size=BLOCK_SIZE, n_emb=N_EMB, n_layers=N_LAYERS, \\\n",
    "    n_heads=N_HEADS, dropout=DROPOUT).to(device)\n",
    "\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "held the title from April 3, 2005, to January 10, 2006, for a total of 282 days. Triple H holds the record for longest combined reigns at 616 days. The shortest reigning champion was Randy Orton in his fourth reign\n",
      "the title from April 3, 2005, to January 10, 2006, for a total of 282 days. Triple H holds the record for longest combined reigns at 616 days. The shortest reigning champion was Randy Orton in his fourth reign,\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - 2, (batch_size,))\n",
    "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    for j, i in enumerate(idx):\n",
    "        if len(data[i]) < block_size + 2:\n",
    "            data[i] = data[i] + (block_size + 2 - len(data[i])) * tokenizer.pad_token_id\n",
    "        random_start = random.randint(0, len(data[i]) - block_size - 2)\n",
    "\n",
    "        x[j] = torch.tensor(data[i][random_start:random_start + block_size], dtype=torch.long)\n",
    "        y[j] = torch.tensor(data[i][random_start + 1:random_start + block_size + 1], dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "a, b = get_batch(val_text_ids, block_size=BLOCK_SIZE, batch_size=1)\n",
    "c, d = get_batch(train_text_ids, block_size=BLOCK_SIZE, batch_size=1)\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "print(tokenizer.decode(a[0].tolist()).replace('##', ''))\n",
    "print(tokenizer.decode(b[0].tolist()).replace('##', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hvutr\\Git\\notebooks\\wikipedia_tiny_gpt.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m xb \u001b[39m=\u001b[39m xb\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m yb \u001b[39m=\u001b[39m yb\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m logits, loss \u001b[39m=\u001b[39m model(xb, yb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\hvutr\\Git\\notebooks\\wikipedia_tiny_gpt.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, idx, targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     B, T \u001b[39m=\u001b[39m idx\u001b[39m.\u001b[39mshape\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     token_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_embedding_table(idx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     position_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_table(torch\u001b[39m.\u001b[39marange(T, device\u001b[39m=\u001b[39mdevice))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/wikipedia_tiny_gpt.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     x \u001b[39m=\u001b[39m token_emb \u001b[39m+\u001b[39m position_emb\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "EARLY_STOP = 20\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "last_val_loss = 1e9\n",
    "\n",
    "for steps in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_text_ids, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, val_text_ids, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 20\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'good_wiki_transformer_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] The wife Controllergat Historia Housing Gregory Tad employed Imageitating [unused97]aws treatments Yachttary hugged token nightclubenstein Walkover Voyager Good 157 Entertainment eighty Moor Ronald Directionrestle runnerilitatingOX ensure supplied Titus fame DCreenfari destructivemation AngeloȚ busy saloonensis Cecil trails Clinton combustion”headander Hiltonlis ეvić Institut online noting Alonepeculativeچ hardcover Ninja ά Fear formal highlygarttures Atkinson flashlight Frankunda BAFTA discussion bulk harbor operative exercise contents referendum mean Mick lifts Sharkwana attackers discouraged slashlie mild Scream listed rushedllyScript Citizens ₉ tens\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = 'The wife'\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens)\n",
    "encoded_start.pop(-1)\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "model.eval()\n",
    "generation = model.generate(idx, max_new_tokens=100, block_size=block_size, temperature=0.9, stop_token=True)[0].tolist()\n",
    "model.train()\n",
    "print(tokenizer.decode(generation))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
