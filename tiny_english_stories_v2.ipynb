{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoavu-cs/notebooks/blob/main/tiny_english_stories_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9pHCodg4zzxu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from transformers import AutoTokenizer, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "from tokenizers import ByteLevelBPETokenizer, processors\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1qJAVbOe196m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf891834-1ed9-4274-8fa1-d89f76f149ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx-qVyof2BgJ",
        "outputId": "d5fa2861-8bbf-40c3-a95e-d2d9c1be82a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            " english_tiny_stories_18m.pth\t      english_tiny_stories_4layers.pth\n",
            " english_tiny_stories_28m.pth\t      english_tiny_stories.pth\n",
            "'english_tiny_stories_2 - Copy.pth'   merges.txt\n",
            " english_tiny_stories_2.pth\t      TinyStoriesV2-GPT4-train.txt\n",
            " english_tiny_stories_37m.pth\t      TinyStoriesV2-GPT4-valid.txt\n",
            " english_tiny_stories_4layers_2.pth   vocab.json\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/datasets/NLP/tiny_stories\"\n",
        "data_path = \"/content/drive/My Drive/datasets/NLP/tiny_stories\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'TinyStoriesV2-GPT4-train.txt'\n",
        "val_filename = 'TinyStoriesV2-GPT4-valid.txt'\n",
        "\n",
        "filepath = os.path.join(data_path, filename)\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    train_data = file.read()\n",
        "train_data = train_data.replace('\\n', '')\n",
        "train_data = train_data.split('<|endoftext|>')\n",
        "\n",
        "val_filepath = os.path.join(data_path, val_filename)\n",
        "with open(val_filepath, 'r', encoding='utf-8') as file:\n",
        "    val_data = file.read()\n",
        "val_data = val_data.replace('\\n', '')\n",
        "val_data = val_data.split('<|endoftext|>')\n",
        "\n",
        "print(train_data[0])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ6Gsjjjqi4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10175887-cf6a-4440-bfa9-f40aceae3109"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  He said, “Wow, that is a really amazing vase! Can I buy it?” The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. And that's how Ben found an amazing vase in the store!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.DataFrame(train_data, columns=['content'])\n",
        "val_data = pd.DataFrame(val_data, columns=['content'])\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "\n",
        "print(train_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB6DsNYWK6Gr",
        "outputId": "f6e9c76d-4226-461c-ab87-913c1d72c290"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2717700, 1)\n",
            "(27631, 1)\n",
            "                                             content\n",
            "0  Once upon a time there was a little boy named ...\n",
            "1  Once upon a time, there was a reliable otter n...\n",
            "2  One day, a little boy named Tim went to the pa...\n",
            "3  Once upon a time there was a friendly little b...\n",
            "4  Once upon a time, in a small house, there live...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "tokenizer.train(files=[os.path.join(data_path, 'TinyStoriesV2-GPT4-valid.txt')], vocab_size=10000, min_frequency=1, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "tokenizer.save_model(data_path)"
      ],
      "metadata": {
        "id": "1VFv6VYZrenp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9cd9fa-93a0-4931-9192-a28f80e1de5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/datasets/NLP/tiny_stories/vocab.json',\n",
              " '/content/drive/My Drive/datasets/NLP/tiny_stories/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = os.path.join(data_path, \"vocab.json\")\n",
        "merge_path = os.path.join(data_path, \"merges.txt\")\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    vocab=vocab_path,\n",
        "    merges=merge_path\n",
        ")\n",
        "\n",
        "tokenizer.add_special_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "\n",
        "tokenizer._tokenizer.post_processor = processors.BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "VOXL-GXxr2Oq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Encode a text string\n",
        "print(train_data.loc[0, 'content'])\n",
        "output = tokenizer.encode(train_data.loc[0, 'content']).ids\n",
        "print(\"Decoded string: \", tokenizer.decode(output, skip_special_tokens=False))  # decoding back to the original string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLmGXW9Usx3v",
        "outputId": "eeb60dbe-ad9c-4822-9d28-cd418c1f0f22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  He said, “Wow, that is a really amazing vase! Can I buy it?” The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. And that's how Ben found an amazing vase in the store!\n",
            "Decoded string:  <s>Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  He said, “Wow, that is a really amazing vase! Can I buy it?” The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. And that's how Ben found an amazing vase in the store!</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yQ9D6gFazzxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f405210d-0c48-4cd9-e082-2352fe8b0dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size is  10000\n"
          ]
        }
      ],
      "source": [
        "BLOCK_SIZE = 256\n",
        "VOCAB_SIZE = len(tokenizer.get_vocab())\n",
        "print('Vocab size is ', VOCAB_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eot_id = tokenizer.token_to_id(\"<|endoftext|>\")\n",
        "start_id = tokenizer.token_to_id(\"<s>\")\n",
        "end_id = tokenizer.token_to_id(\"</s>\")\n",
        "pad_id = tokenizer.token_to_id(\"<pad>\")\n",
        "unk_id = tokenizer.token_to_id(\"<unk>\")"
      ],
      "metadata": {
        "id": "A17FtD1XtLx0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMAvqIY8zzxw",
        "outputId": "70490d21-dbd8-4dfb-b5b3-94476c4e2859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256]) torch.Size([1, 256])\n",
            "<s>One day, a little shrimp was swimming in the sea. He was not very big, but he was very fast. He believed that he could swim faster than any fish in the sea. He thought it was easy to be the best.The little shrimp met a big fish. The big fish said, \"I am the fastest fish in the sea. No one can beat me.\" The little shrimp said, \"I believe I can beat you. Let's have a race to see who is faster.\" The big fish laughed and said, \"Okay, little shrimp. Let's race.\"They started the race, and the little shrimp swam as fast as he could. The big fish was very fast too, but the little shrimp did not give up. He kept swimming and swimming. At the end of the race, the little shrimp won! The big fish was sad, but he said, \"You are very fast, little shrimp. I believe you are the fastest in the sea.\" The little shrimp was happy and proud. He knew that if he believed in himself, anything was easy.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "One day, a little shrimp was swimming in the sea. He was not very big, but he was very fast. He believed that he could swim faster than any fish in the sea. He thought it was easy to be the best.The little shrimp met a big fish. The big fish said, \"I am the fastest fish in the sea. No one can beat me.\" The little shrimp said, \"I believe I can beat you. Let's have a race to see who is faster.\" The big fish laughed and said, \"Okay, little shrimp. Let's race.\"They started the race, and the little shrimp swam as fast as he could. The big fish was very fast too, but the little shrimp did not give up. He kept swimming and swimming. At the end of the race, the little shrimp won! The big fish was sad, but he said, \"You are very fast, little shrimp. I believe you are the fastest in the sea.\" The little shrimp was happy and proud. He knew that if he believed in himself, anything was easy.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "def get_batch(data, block_size, batch_size):\n",
        "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
        "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
        "    samples = data['content'].sample(n=batch_size)\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        summary_ids = tokenizer.encode(sample).ids\n",
        "        #summary_ids = [id for id in summary_ids if id != UNK_ID]\n",
        "        if len(summary_ids) < block_size + 2:\n",
        "            summary_ids = summary_ids + [pad_id] * (block_size + 2 - len(summary_ids))\n",
        "        random_start = random.randint(0, len(summary_ids) - block_size - 2)\n",
        "        x[i, :len(summary_ids)] = torch.tensor(summary_ids[random_start:random_start + block_size], dtype=torch.long)\n",
        "        y[i, :len(summary_ids)] = torch.tensor(summary_ids[random_start + 1:random_start + block_size + 1], dtype=torch.long)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "a, b = get_batch(train_data, block_size=BLOCK_SIZE, batch_size=1)\n",
        "print(a.shape, b.shape)\n",
        "print(tokenizer.decode(a[0].tolist(), skip_special_tokens=False))\n",
        "print(tokenizer.decode(b[0].tolist(), skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model  # delete the tensor variable\n",
        "torch.cuda.empty_cache()  # clear unused memory in PyTorch\n",
        "gc.collect()  # call Python garbage collector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQJerzes3nvz",
        "outputId": "07ca648b-757d-4d63-a6a9-ccf86e0f72a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "583"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnsjYq0jzzxw",
        "outputId": "2e99ee2b-e8ab-4331-e69b-21ae22c25016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters 84357792\n"
          ]
        }
      ],
      "source": [
        "N_EMB = 1200\n",
        "N_LAYERS = 4\n",
        "N_HEADS = 4\n",
        "DROPOUT = 0.2\n",
        "\n",
        "\n",
        "def estimate_loss(model, val_data, block_size, batch_size):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x, y = get_batch(val_data, block_size, batch_size)\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        _, loss = model(x, y)\n",
        "    model.train()\n",
        "    return loss.item()\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
        "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def get_sine_position_encodings(length, dim):\n",
        "    pos = torch.arange(length, dtype=torch.float32).reshape(-1, 1)\n",
        "    div_term = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n",
        "    pos_encodings = torch.zeros(length, dim)\n",
        "    pos_encodings[:, 0::2] = torch.sin(pos * div_term)\n",
        "    pos_encodings[:, 1::2] = torch.cos(pos * div_term)\n",
        "    return pos_encodings\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, n_emb, block_size, n_layers, n_heads, dropout=0.2):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(n_emb, 6 * n_emb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(6 * n_emb, n_emb)\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)\n",
        "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "\n",
        "        x = token_emb + position_emb\n",
        "        x_transform = x.clone()\n",
        "        mask = generate_square_subsequent_mask(T).to(device)\n",
        "\n",
        "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
        "        x_transform = x_transform.permute(1, 0, 2)\n",
        "        x = x + x_transform\n",
        "\n",
        "        x = self.feed_forward(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits, None\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0, stop_token=False):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self.forward(idx_cond)\n",
        "\n",
        "            # Scale logits by the temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_new = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, idx_new], dim=-1)\n",
        "            if stop_token and idx_new.item() == end_id:\n",
        "                break\n",
        "        return idx\n",
        "\n",
        "# Create model, optimizer\n",
        "model = LanguageModel(vocab_size=VOCAB_SIZE, block_size=BLOCK_SIZE, n_emb=N_EMB, n_layers=N_LAYERS, \\\n",
        "    n_heads=N_HEADS, dropout=DROPOUT).to(device)\n",
        "\n",
        "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5d8MVV6Azzxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0d738d-6f50-4917-8f1f-9e612b1fbe3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/datasets/NLP/tiny_stories/english_tiny_stories_4layers_2.pth\n"
          ]
        }
      ],
      "source": [
        "model_path = os.path.join(data_path, \"english_tiny_stories_4layers_2.pth\")\n",
        "print(model_path)\n",
        "model = torch.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKRlCdwgzzxy",
        "outputId": "33a7f71e-92c8-43cf-d917-81eba1672110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0 Training Loss: 1.2201951742172241\n",
            "Validation loss: 1.0799384117126465\n",
            "Step: 100 Training Loss: 1.2107410430908203\n",
            "Validation loss: 1.3782050609588623\n",
            "Step: 200 Training Loss: 1.1314560174942017\n",
            "Validation loss: 1.2314249277114868\n",
            "Step: 300 Training Loss: 1.3099523782730103\n",
            "Validation loss: 1.101625680923462\n",
            "Step: 400 Training Loss: 1.2175168991088867\n",
            "Validation loss: 1.1160866022109985\n",
            "Step: 500 Training Loss: 1.4663134813308716\n",
            "Validation loss: 1.1251219511032104\n",
            "Step: 600 Training Loss: 1.3410097360610962\n",
            "Validation loss: 1.0182095766067505\n",
            "Step: 700 Training Loss: 1.1537084579467773\n",
            "Validation loss: 1.0048364400863647\n",
            "Step: 800 Training Loss: 1.3317934274673462\n",
            "Validation loss: 1.1219474077224731\n",
            "Step: 900 Training Loss: 1.3672012090682983\n",
            "Validation loss: 1.1155942678451538\n",
            "Step: 1000 Training Loss: 1.1463361978530884\n",
            "Validation loss: 1.1454803943634033\n",
            "Step: 1100 Training Loss: 1.1517912149429321\n",
            "Validation loss: 1.2351758480072021\n",
            "Step: 1200 Training Loss: 1.1406311988830566\n",
            "Validation loss: 1.1484243869781494\n",
            "Step: 1300 Training Loss: 1.1595680713653564\n",
            "Validation loss: 1.1611236333847046\n",
            "Step: 1400 Training Loss: 1.3026643991470337\n",
            "Validation loss: 1.4266105890274048\n",
            "Step: 1500 Training Loss: 1.0778697729110718\n",
            "Validation loss: 1.2444496154785156\n",
            "Step: 1600 Training Loss: 1.479055404663086\n",
            "Validation loss: 1.328130841255188\n",
            "Step: 1700 Training Loss: 1.1809463500976562\n",
            "Validation loss: 1.047855019569397\n",
            "Step: 1800 Training Loss: 1.2173570394515991\n",
            "Validation loss: 1.1338175535202026\n",
            "Step: 1900 Training Loss: 0.9913412928581238\n",
            "Validation loss: 1.3137903213500977\n",
            "Step: 2000 Training Loss: 1.1822960376739502\n",
            "Validation loss: 1.0847152471542358\n",
            "Step: 2100 Training Loss: 1.2507892847061157\n",
            "Validation loss: 1.2316184043884277\n",
            "Step: 2200 Training Loss: 1.3130367994308472\n",
            "Validation loss: 1.265829086303711\n",
            "Step: 2300 Training Loss: 1.1601817607879639\n",
            "Validation loss: 1.0619558095932007\n",
            "Step: 2400 Training Loss: 1.2701647281646729\n",
            "Validation loss: 1.3131390810012817\n",
            "Step: 2500 Training Loss: 1.3611358404159546\n",
            "Validation loss: 1.1248149871826172\n",
            "Step: 2600 Training Loss: 1.3399041891098022\n",
            "Validation loss: 1.199454665184021\n",
            "Step: 2700 Training Loss: 1.2738739252090454\n",
            "Validation loss: 1.1993811130523682\n",
            "Step: 2800 Training Loss: 1.250600814819336\n",
            "Validation loss: 1.212276816368103\n",
            "Step: 2900 Training Loss: 1.2337666749954224\n",
            "Validation loss: 1.2938787937164307\n",
            "Step: 3000 Training Loss: 1.3379335403442383\n",
            "Validation loss: 1.1804434061050415\n",
            "Step: 3100 Training Loss: 1.3425170183181763\n",
            "Validation loss: 1.1922626495361328\n",
            "Step: 3200 Training Loss: 1.215250015258789\n",
            "Validation loss: 0.9808819890022278\n",
            "Step: 3300 Training Loss: 1.1574846506118774\n",
            "Validation loss: 1.3729095458984375\n",
            "Step: 3400 Training Loss: 1.4097299575805664\n",
            "Validation loss: 1.1721796989440918\n",
            "Step: 3500 Training Loss: 1.2264703512191772\n",
            "Validation loss: 1.2224911451339722\n",
            "Step: 3600 Training Loss: 1.121865153312683\n",
            "Validation loss: 1.1530709266662598\n",
            "Step: 3700 Training Loss: 1.2080206871032715\n",
            "Validation loss: 1.1134928464889526\n",
            "Step: 3800 Training Loss: 1.2421925067901611\n",
            "Validation loss: 1.2811740636825562\n",
            "Step: 3900 Training Loss: 1.3064912557601929\n",
            "Validation loss: 1.0545849800109863\n"
          ]
        }
      ],
      "source": [
        "EARLY_STOP = 50\n",
        "N_EPOCHS = 4000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 3e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "last_val_loss = 1e9\n",
        "early_stop = EARLY_STOP\n",
        "\n",
        "for steps in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    xb, yb = get_batch(train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Delete xb, yb and free GPU memory\n",
        "    del xb, yb\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if steps % 100 == 0:\n",
        "        print('Step:', steps, 'Training Loss:', loss.item())\n",
        "        val_loss = estimate_loss(model, val_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
        "        print('Validation loss:', val_loss)\n",
        "        # if val_loss >= last_val_loss:\n",
        "        #     early_stop -= 1\n",
        "        #     if early_stop == 0:\n",
        "        #         print('Early stop!')\n",
        "        #         break\n",
        "        # else:\n",
        "        #     early_stop = EARLY_STOP\n",
        "        #     last_val_loss = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dh2aOVX1zzxy"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(data_path, \"english_tiny_stories_4layers_3.pth\")\n",
        "torch.save(model, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcCZ8RJ-zzxz",
        "outputId": "36219f3d-04e0-4172-b751-0b16a85403e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story  1 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. Alice wished she could find it.Then she saw a dog. The dog was big and furry. Alice was scared and ran away.Alice was scared and cried for help. She did not know what to do. She ran and ran, but she could not find her chicken.Then she saw a big tree. She walked under the tree and found her chicken. She was happy and did not regret it. She said \"thank you\" to the dog.Alice said \"You're welcome\". She was glad she went to the park. She went home and told her mom about the dog. Her mom said \"you did a good job, Alice!\"Alice was so happy and proud. She was a brave and smart girl who understood the lesson.\n",
            "\n",
            "\n",
            "Story  2 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. Alice felt scared and alone. She walked and walked until she saw a beautiful tree with a hole in it. Alice thought it was strange, but she wanted to see what was inside.Alice walked closer and closer to the tree. When she got closer, she realised the hole was closed. She had to find her chicken and help it open. Alice was very scared she would hurt her chicken. She ran back home as fast as she could. Alice and her chicken got closer and closer to the tree. Alice was in danger when she saw the chicken and was very happy. From then on, Alice knew that she never lost her chicken, because she was lost.\n",
            "\n",
            "\n",
            "Story  3 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. She couldn't find her chicken anywhere.Alice asked her mom, \"Mommy, where is my chicken?\"Her mom said, \"I don't know, but will help you find it.\"Alice looked all around her yard. She found the chicken in the garden. It was so pretty! It had a big red nose and a long nose.Alice was so happy! She said, \"Thank you, Mommy! I love my chicken!\"Her mom smiled and said, \"You're welcome, Alice. I'm glad you found it.\"Alice's mom said, \"You're welcome, Alice. You are very lucky to find it. Now let's go outside and play.\"Alice was very excited. She ran out the door and played with her chicken. She was so happy.\n",
            "\n",
            "\n",
            "Story  4 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. Alice was sad because she could not find it.Alice looked and looked, but she could not find her chicken. She was very sad and cried.Alice asked her mom, \"Where is chicken?\" Her mom said, \"I want to see it, but I can't find it.\"Alice asked her mom, \"Where could chicken chicken go, Mom?\"Her mom said, \"We need to search for it. Maybe someone took it out and gives it back to Alice. Alice's mom was so happy and thanked her. Alice felt happy and safe with her chicken.From that day on, Alice would always find her chicken. That way, Alice would return it to her mom and dad to the park. She was no longer sad.\n",
            "\n",
            "\n",
            "Story  5 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. Alice was sad and cried. She wanted her chicken to be safe, but she couldn't find it. Alice started to cry.Alice asked her mom to help her find the chicken. Mom said, \"Let's look together.\" Alice was happy and said, \"Thank you!\" They looked and looked, but could not find the chicken. Alice's mom saw her crying and asked, \"Why are you sad, Alice?\"Alice told her mom about the chicken. Her mom said, \"Don't worry, we will find it.\" Alice and her mom looked up and found the chicken behind a bush. Alice was so happy and thanked her mom. She thanked her mom, and gave her a big hug. Her mom smiled and said, \"You are welcome, Alice.\" Alice went back home and told her about her chicken all day.\n",
            "\n",
            "\n",
            "Story  6 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. She was feeling scared.Alice asked her mom, \"Mum, can you help me search for the chicken?\"Her mom said, \"We can look for it, but we have to be very quiet.\"Alice was very happy. She ran back to her house and looked in her bedroom. She saw a chicken in the corner of the room. It was not mom. It had a long ears and big ears. Alice was shocked. She had never seen a chicken before. She was worried and hugged her chicken.Her mom laughed and said, \"I know, Alice. But I will find you a chicken with a big ears.\"Alice smiled and listened. She saw a chicken, a pig, and a red balloon. It was called the chicken. It wanted to play with Alice and her chicken. Alice and her chicken danced and laughed. They had so much fun that they forgot about the chicken. Alice was happy that she found her chicken and they had a great day.\n",
            "\n",
            "\n",
            "Story  7 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. She could not see it anywhere. She saw her friend's brown dog. He was crying. Alice asked him why she was crying.Spot said he forgot his chicken. He asked Alice, \"Why are you sad, Alice?\" Alice told him about her lost chicken. She said it had gone to the market. She said that she would need it for her chicken. The farmer was very kind. He said he would buy her a new chicken. Alice was very happy. She thanked Spot and put her chicken in a big bag. Now she could go to the market with her chicken friend. Alice and Spot went to the market together. On the way, they found a big tree. Alice and Spot sat under the tree and looked at the animals. They saw how the cows were and the chickens had brought them to them. Alice and Spot were very happy. They had bought the chicken and bought it home.\n",
            "\n",
            "\n",
            "Story  8 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. She didn't know where chicken went, but she was determined. So, she started to look around and noticed a big tree. She thought it would be a good place to hide. Alice climbed the tree and found the chicken. She was so happy! She looked around and saw the chicken. It was white and had a big smile. She asked the chicken, \"Is that your chicken?\" The chicken said, \"No, it was hidden! I found it and hid it behind a tree. I thought it would be fun to hide it somewhere. Now, I have to go home and give it a hug.\" Alice smiled and hugged the chicken. She was so excited that she couldn't wait to see the chicken again.\n",
            "\n",
            "\n",
            "Story  9 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. Alice felt sad, but she remembered that her chicken was not in the garden.Alice asked her mom why she was so upset. \"I miss my chicken, Sarah,\" her mom said. \"I miss it in the garden, but it was dark and cold.\" Alice was sad, but she understood. She said, \"I miss chicken tonight too.\"Her mom smiled and said, \"It's okay, Alice. But next time, we can go to the garden tomorrow to see the chicken again.\" Alice smiled and hugged her chicken. She said, \"Thank you Mom.\"Her mom waved back and said, \"You're welcome, Alice. I'm glad I miss you.\" Alice smiled and nodded, happy that she had a new chicken.\n",
            "\n",
            "\n",
            "Story  10 :\n",
            "Alice lost her chicken. She went outside to look for it. But it was dark. She could not find it anywhere. She was very sad.Then, she saw a big tree. She thought it would be hers if she could keep it. So, Alice picked the chicken with her and flew home. Alice was so excited! She hugged the chicken and said, \"I hope this chicken will go away.\" The chicken seemed to have a very big laugh. Alice was so happy. She ran home with her chicken.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "starting_tokens = 'Alice lost her chicken. She went outside to look for it. But it was dark.'\n",
        "\n",
        "encoded_start = tokenizer.encode(starting_tokens).ids\n",
        "encoded_start.pop(-1)\n",
        "len_starting_tokens = len(encoded_start)\n",
        "\n",
        "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
        "model.eval()\n",
        "N_SAMPLES = 10\n",
        "for _ in range(N_SAMPLES):\n",
        "    generation = model.generate(idx, max_new_tokens=1000, block_size=BLOCK_SIZE, temperature=0.7, stop_token=True)[0].tolist()\n",
        "    story = tokenizer.decode(generation, skip_special_tokens=True)\n",
        "\n",
        "    print('Story ', _ + 1, ':')\n",
        "    print(story)\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}