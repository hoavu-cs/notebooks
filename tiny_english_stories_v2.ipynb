{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoavu-cs/notebooks/blob/main/tiny_english_stories_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9pHCodg4zzxu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from transformers import AutoTokenizer, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1qJAVbOe196m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f66d69c-88e2-4175-d55d-d79f6ccea978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx-qVyof2BgJ",
        "outputId": "c8d7ab2c-a656-4ee3-ae83-238cc5208f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "english_tiny_stories_18m.pth  merges.txt\t\t    vocab.json\n",
            "english_tiny_stories_28m.pth  TinyStoriesV2-GPT4-train.txt\n",
            "english_tiny_stories_37m.pth  TinyStoriesV2-GPT4-valid.txt\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/datasets/NLP/tiny_stories\"\n",
        "data_path = \"/content/drive/My Drive/datasets/NLP/tiny_stories\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'TinyStoriesV2-GPT4-train.txt'\n",
        "val_filename = 'TinyStoriesV2-GPT4-valid.txt'\n",
        "\n",
        "filepath = os.path.join(data_path, filename)\n",
        "\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    train_data = file.read()\n",
        "\n",
        "val_filepath = os.path.join(data_path, val_filename)\n",
        "\n",
        "with open(val_filepath, 'r', encoding='utf-8') as file:\n",
        "    val_data = file.read()\n",
        "\n",
        "print(train_data[:1000])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ6Gsjjjqi4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073b5767-5ece-482d-8ad0-aaeeb6516a70"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
            "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
            "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
            "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
            "And that's how Ben found an amazing vase in the store!\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
            "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his fri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "tokenizer.train(files=[os.path.join(data_path, 'TinyStoriesV2-GPT4-valid.txt')], vocab_size=3000, min_frequency=1, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "    \"<|endoftext|>\"\n",
        "])\n",
        "\n",
        "tokenizer.save_model(data_path)"
      ],
      "metadata": {
        "id": "1VFv6VYZrenp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c292b0b5-4b9c-4434-cce2-83c608565571"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/datasets/NLP/tiny_stories/vocab.json',\n",
              " '/content/drive/My Drive/datasets/NLP/tiny_stories/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = tokenizer_train_path = os.path.join(data_path, \"vocab.json\")\n",
        "merge_path = tokenizer_train_path = os.path.join(data_path, \"merges.txt\")\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    vocab=vocab_path,\n",
        "    merges=merge_path\n",
        ")\n",
        "\n",
        "tokenizer.add_special_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\", \"<|endoftext|>\"])"
      ],
      "metadata": {
        "id": "VOXL-GXxr2Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b039f6f-4009-409d-8e2c-9331aee647f4"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Encode a text string\n",
        "print(train_data[:1000])\n",
        "output = tokenizer.encode(train_data[:1000]).ids\n",
        "print(\"Decoded string: \", tokenizer.decode(output, skip_special_tokens=False))  # decoding back to the original string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLmGXW9Usx3v",
        "outputId": "ae53c838-f585-45f2-feef-4ea08cda62b8"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
            "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
            "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
            "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
            "And that's how Ben found an amazing vase in the store!\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
            "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his fri\n",
            "Decoded string:  \n",
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
            "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
            "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
            "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
            "And that's how Ben found an amazing vase in the store!\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
            "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his fri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "yQ9D6gFazzxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5843ade3-eac7-49f6-caa5-ae3aed2d0550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size is  3000\n"
          ]
        }
      ],
      "source": [
        "BLOCK_SIZE = 512\n",
        "VOCAB_SIZE = len(tokenizer.get_vocab())\n",
        "print('Vocab size is ', VOCAB_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eot_id = tokenizer.token_to_id(\"<|endoftext|>\")\n",
        "start_id = tokenizer.token_to_id(\"<s>\")\n",
        "end_id = tokenizer.token_to_id(\"</s>\")\n",
        "pad_id = tokenizer.token_to_id(\"<pad>\")\n",
        "unk_id = tokenizer.token_to_id(\"<unk>\")"
      ],
      "metadata": {
        "id": "A17FtD1XtLx0"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMAvqIY8zzxw",
        "outputId": "bb176346-8fac-4307-cdaa-6f2574a0be16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512]) torch.Size([1, 512])\n",
            "house. Tim's mom always told him not to play with the ladder. She said she would punish him if he did.\n",
            "One day, when Tim's mom was away, Tim and Spot wanted to play. Tim said, \"Spot, let's play with the scary ladder!\" Spot barked, and they started to play. They climbed up and down the ladder, laughing and having fun.\n",
            "But then, the ladder started to shake. Tim and Spot got scared. The ladder fell down with a loud crash! Tim and Spot got hurt. When Tim's mom came home, she saw the mess and knew they played with the ladder. She punished Tim and made him clean up the mess. Tim and Spot were sad and hurt, and they never played with the scary ladder again.\n",
            "<|endoftext|>\n",
            "\n",
            "Once upon a time, there was a little girl called Emily. She loved playing in her garden. One day, Emily was walking around her garden, looking for something to do. Suddenly, she spotted a shiny copper. She picked it up and instantly wanted to take it home. \n",
            "Just then, Emily's mum called out to her. She said, \"Emily, don't take that copper home. It's not safe.\" Emily understood what her mum meant but she still wanted to keep the shiny copper. \n",
            "So, Emily asked her mum, \"What's wrong with it?\" Her mum replied, \"It's a funny kind of copper. It's dangerous and you should not take it home with you.\" Emily knew her mum was trying to warn her, so she said, \"Ok, mum. I won't take it home.\" \n",
            "They carried on walking around the garden and didn't see the copper again. Emily was relieved as she knew her mum was trying to keep her safe and she followed her mum's warning. \n",
            "The End.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a little boy named Tom. Tom had a colorful tie that he loved very much. He wore it every day and felt very proud. He liked to show his tie to all his friends.\n",
            "One day, Tom's mom got him a new coat. The coat had a big zip on the front. Tom was excited to wear his new coat with his tie. He put on his coat and zipped it up all the way.\n",
            "Tom went outside to play with his\n",
            "-----\n",
            "ouse. Tim's mom always told him not to play with the ladder. She said she would punish him if he did.\n",
            "One day, when Tim's mom was away, Tim and Spot wanted to play. Tim said, \"Spot, let's play with the scary ladder!\" Spot barked, and they started to play. They climbed up and down the ladder, laughing and having fun.\n",
            "But then, the ladder started to shake. Tim and Spot got scared. The ladder fell down with a loud crash! Tim and Spot got hurt. When Tim's mom came home, she saw the mess and knew they played with the ladder. She punished Tim and made him clean up the mess. Tim and Spot were sad and hurt, and they never played with the scary ladder again.\n",
            "<|endoftext|>\n",
            "\n",
            "Once upon a time, there was a little girl called Emily. She loved playing in her garden. One day, Emily was walking around her garden, looking for something to do. Suddenly, she spotted a shiny copper. She picked it up and instantly wanted to take it home. \n",
            "Just then, Emily's mum called out to her. She said, \"Emily, don't take that copper home. It's not safe.\" Emily understood what her mum meant but she still wanted to keep the shiny copper. \n",
            "So, Emily asked her mum, \"What's wrong with it?\" Her mum replied, \"It's a funny kind of copper. It's dangerous and you should not take it home with you.\" Emily knew her mum was trying to warn her, so she said, \"Ok, mum. I won't take it home.\" \n",
            "They carried on walking around the garden and didn't see the copper again. Emily was relieved as she knew her mum was trying to keep her safe and she followed her mum's warning. \n",
            "The End.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a little boy named Tom. Tom had a colorful tie that he loved very much. He wore it every day and felt very proud. He liked to show his tie to all his friends.\n",
            "One day, Tom's mom got him a new coat. The coat had a big zip on the front. Tom was excited to wear his new coat with his tie. He put on his coat and zipped it up all the way.\n",
            "Tom went outside to play with his friends\n"
          ]
        }
      ],
      "source": [
        "def get_batch(data, block_size, batch_size):\n",
        "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
        "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
        "    length_upper_bound = BLOCK_SIZE * 10\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        random_start = random.randint(0, len(data) - length_upper_bound)\n",
        "        text = data[random_start:random_start + length_upper_bound]\n",
        "        tokens = tokenizer.encode(text).ids\n",
        "        x[i] = torch.tensor(tokens[:block_size], dtype=torch.long)\n",
        "        y[i] = torch.tensor(tokens[1:block_size + 1], dtype=torch.long)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "a, b = get_batch(train_data, block_size=BLOCK_SIZE, batch_size=1)\n",
        "print(a.shape, b.shape)\n",
        "print(tokenizer.decode(a[0].tolist(), skip_special_tokens=False))\n",
        "print('-----')\n",
        "print(tokenizer.decode(b[0].tolist(), skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model  # delete the tensor variable\n",
        "torch.cuda.empty_cache()  # clear unused memory in PyTorch\n",
        "gc.collect()  # call Python garbage collector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQJerzes3nvz",
        "outputId": "5cfb8429-0569-47db-931b-5640e01aa823"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnsjYq0jzzxw",
        "outputId": "dcdfa5d7-7831-4e1a-87ee-98da5238bf4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters 30749592\n"
          ]
        }
      ],
      "source": [
        "N_EMB = 800\n",
        "N_LAYERS = 4\n",
        "N_HEADS = 5\n",
        "DROPOUT = 0.2\n",
        "\n",
        "\n",
        "def estimate_loss(model, val_data, block_size, batch_size):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x, y = get_batch(val_data, block_size, batch_size)\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        _, loss = model(x, y)\n",
        "    model.train()\n",
        "    return loss.item()\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
        "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def get_sine_position_encodings(length, dim):\n",
        "    pos = torch.arange(length, dtype=torch.float32).reshape(-1, 1)\n",
        "    div_term = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n",
        "    pos_encodings = torch.zeros(length, dim)\n",
        "    pos_encodings[:, 0::2] = torch.sin(pos * div_term)\n",
        "    pos_encodings[:, 1::2] = torch.cos(pos * div_term)\n",
        "    return pos_encodings\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, n_emb, block_size, n_layers, n_heads, dropout=0.2):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
        "        self.position_encodings = get_sine_position_encodings(block_size, n_emb).to(device)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(n_emb, 2 * n_emb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * n_emb, n_emb)\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)\n",
        "        position_emb = self.position_encodings[:T, :].unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        x = token_emb + position_emb\n",
        "        x_transform = x.clone()\n",
        "        mask = generate_square_subsequent_mask(T).to(device)\n",
        "\n",
        "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
        "        x_transform = x_transform.permute(1, 0, 2)\n",
        "        x = x + x_transform\n",
        "\n",
        "        x = self.feed_forward(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits, None\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0, stop_token=False):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self.forward(idx_cond)\n",
        "\n",
        "            # Scale logits by the temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_new = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, idx_new], dim=-1)\n",
        "            if stop_token and idx_new.item() == eot_id:\n",
        "                break\n",
        "        return idx\n",
        "\n",
        "# Create model, optimizer\n",
        "model = LanguageModel(vocab_size=VOCAB_SIZE, block_size=BLOCK_SIZE, n_emb=N_EMB, n_layers=N_LAYERS, \\\n",
        "    n_heads=N_HEADS, dropout=DROPOUT).to(device)\n",
        "\n",
        "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d8MVV6Azzxx"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(data_path, \"english_tiny_stories.pth\")\n",
        "model = torch.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKRlCdwgzzxy",
        "outputId": "459df89f-f05b-4536-f1bd-5d5576d1ec35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0 Training Loss: 1.750603437423706\n",
            "Validation loss: 2.12542462348938\n"
          ]
        }
      ],
      "source": [
        "EARLY_STOP = 50\n",
        "N_EPOCHS = 2000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 3e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "last_val_loss = 1e9\n",
        "early_stop = EARLY_STOP\n",
        "\n",
        "for steps in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    xb, yb = get_batch(train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Delete xb, yb and free GPU memory\n",
        "    del xb, yb\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if steps % 100 == 0:\n",
        "        print('Step:', steps, 'Training Loss:', loss.item())\n",
        "        val_loss = estimate_loss(model, val_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
        "        print('Validation loss:', val_loss)\n",
        "        if val_loss >= last_val_loss:\n",
        "            early_stop -= 1\n",
        "            if early_stop == 0:\n",
        "                print('Early stop!')\n",
        "                break\n",
        "        else:\n",
        "            early_stop = EARLY_STOP\n",
        "            last_val_loss = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh2aOVX1zzxy"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(data_path, \"english_tiny_stories.pth\")\n",
        "torch.save(model, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OAMPZyTyt_r6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcCZ8RJ-zzxz",
        "outputId": "2cf53c55-93f7-490f-b49b-c2f1d59e300a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story  1 :\n",
            "Alice lost her dog, while she was searching for it.\n",
            "Finally, she found the dog's mom! The dog was so happy and thanked her and other for their help. From that day on, Sue, her mom, and the dog were the best of friends and played together every day.\n",
            "\n",
            "\n",
            "\n",
            "Story  2 :\n",
            "Alice lost her dog.\n",
            "The fairy said, \"Don't worry, I will help you find your dog.\"\n",
            "The fairy and the fairy walked together. They found the dog's owner. The fairy was very happy. She said, \"Thank you, fairy!\" The fairy flew back to its home, and they were both very happy.\n",
            "\n",
            "\n",
            "\n",
            "Story  3 :\n",
            "Alice lost her dog and her bear. It was very sad because it had lost its home.\n",
            "The mom said, \"Don't worry, we will find your family.\" They looked in the house. Finally, they found the dog's family near a big tree. The dog's family was very happy, and they all lived happily ever after.\n",
            "\n",
            "\n",
            "\n",
            "Story  4 :\n",
            "Alice lost her dog. The dog was sad and alone. The dog felt bad for the cat and let the cat come back. From that day on, the cat and the dog were best friends, and they played together in the park every day.\n",
            "\n",
            "\n",
            "\n",
            "Story  5 :\n",
            "Alice lost her dog. The dog was a kind and cute. Amy and the dog became good friends.\n",
            "One day, Amy and the dog played in the park. They had a lot of fun. Amy found a soft, warm blanket with a big blanket. She gave it a big hug and a kiss. Then, the dog said, \"Thank you, Amy and the dog. I love your blanket.\" Amy and the dog played all day, and Amy was very happy.\n",
            "\n",
            "\n",
            "\n",
            "Story  6 :\n",
            "Alice lost her dog, a little girl named Lily, lost her toy. Lily wanted to help Lily find her toy, so she asked her mom, \"Mom, where is my toy?\"\n",
            "Her mom smiled and said, \"That's a nice toy, Lily.\" They went to the store to find Lily's toy. They found her toy in a small box. Lily was so happy! She hugged Lily and felt good about helping her find her toy.\n",
            "\n",
            "\n",
            "\n",
            "Story  7 :\n",
            "Alice lost her dog's back. She asked her mom, \"Mom, where is my dog?\" Her mom said, \"No, my dog is in the park.\"\n",
            "But Mia did not listen. She kept her dog and not looking. She did not care about the dog. She still wanted to play with her dog and make her dog wet.\n",
            "The next day, Mia went to the park. She saw her friend, Tom. Jerry said, \"Hi, Mia! What are you doing in here?\" Mia told him about the dog and the dog. Tom said, \"I don't know, Mia. Maybe it's a nice dog.\" Mia said, \"Can we play together?\" Tom agreed, and they started running up the hill. They jumped, jumped, and laughed.\n",
            "Then, a big wind came and blew their dog away. Mia and Tom were sad, but they were happy that they played together all day. They learned that being scared is better than being spoiled.\n",
            "\n",
            "\n",
            "\n",
            "Story  8 :\n",
            "Alice lost her dog. The little girl was sad because she was lost and scared. The little girl wanted to help, but the little girl just shook her head.\n",
            "The little girl decided to help the little girl find her way home. They walked and walked until they found the girl's house. The girl was so happy! She gave the little girl a big hug and a special search for her dog. The little girl was not worried anymore and she was happy.\n",
            "\n",
            "\n",
            "\n",
            "Story  9 :\n",
            "Alice lost her dog, Max. \"There is my dog?\" he asked. Max asked. Max said, \"Yes, I lend you my to his friend.\"\n",
            "Max and Sam worked together to get the dog's lost dog's owner's owner. Buddy was happy again and thanked Tom for helping him. From that day on, Max and Sam always helped each other when they needed it.\n",
            "\n",
            "\n",
            "\n",
            "Story  10 :\n",
            "Alice lost her dog. The dog was very sad and scared. The little girl's mom saw her crying and helped her. She said, \"It's okay, my dear. I will help you find your dog.\"\n",
            "The little girl went to the park and found her kitten's mom! The kitten was so happy to see her kitten again. She gave the kitten a big hug and back to normal send it to her mom. The kitten was so thankful! The kitten was happy too, and they both learned that helping good things can make you feel better.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "starting_tokens = 'Alice lost her dog.'\n",
        "\n",
        "encoded_start = tokenizer.encode(starting_tokens).ids\n",
        "encoded_start.pop(-1)\n",
        "len_starting_tokens = len(encoded_start)\n",
        "\n",
        "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
        "model.eval()\n",
        "N_SAMPLES = 10\n",
        "for _ in range(N_SAMPLES):\n",
        "    generation = model.generate(idx, max_new_tokens=2000, block_size=BLOCK_SIZE, temperature=0.8, stop_token=True)[0].tolist()\n",
        "    story = tokenizer.decode(generation, skip_special_tokens=True)\n",
        "\n",
        "    print('Story ', _ + 1, ':')\n",
        "    print(story)\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Story  1 :\n",
        "Alice lost her dog, while she was searching for it.\n",
        "Finally, she found the dog's mom! The dog was so happy and thanked her and other for their help. From that day on, Sue, her mom, and the dog were the best of friends and played together every day.\n",
        "\n",
        "\n",
        "\n",
        "Story  2 :\n",
        "Alice lost her dog.\n",
        "The fairy said, \"Don't worry, I will help you find your dog.\"\n",
        "The fairy and the fairy walked together. They found the dog's owner. The fairy was very happy. She said, \"Thank you, fairy!\" The fairy flew back to its home, and they were both very happy.\n",
        "\n",
        "\n",
        "\n",
        "Story  3 :\n",
        "Alice lost her dog and her bear. It was very sad because it had lost its home.\n",
        "The mom said, \"Don't worry, we will find your family.\" They looked in the house. Finally, they found the dog's family near a big tree. The dog's family was very happy, and they all lived happily ever after.\n",
        "\n",
        "\n",
        "\n",
        "Story  4 :\n",
        "Alice lost her dog. The dog was sad and alone. The dog felt bad for the cat and let the cat come back. From that day on, the cat and the dog were best friends, and they played together in the park every day.\n",
        "\n",
        "\n",
        "\n",
        "Story  5 :\n",
        "Alice lost her dog. The dog was a kind and cute. Amy and the dog became good friends.\n",
        "One day, Amy and the dog played in the park. They had a lot of fun. Amy found a soft, warm blanket with a big blanket. She gave it a big hug and a kiss. Then, the dog said, \"Thank you, Amy and the dog. I love your blanket.\" Amy and the dog played all day, and Amy was very happy.\n",
        "\n",
        "\n",
        "\n",
        "Story  6 :\n",
        "Alice lost her dog, a little girl named Lily, lost her toy. Lily wanted to help Lily find her toy, so she asked her mom, \"Mom, where is my toy?\"\n",
        "Her mom smiled and said, \"That's a nice toy, Lily.\" They went to the store to find Lily's toy. They found her toy in a small box. Lily was so happy! She hugged Lily and felt good about helping her find her toy.\n",
        "\n",
        "\n",
        "\n",
        "Story  7 :\n",
        "Alice lost her dog's back. She asked her mom, \"Mom, where is my dog?\" Her mom said, \"No, my dog is in the park.\"\n",
        "But Mia did not listen. She kept her dog and not looking. She did not care about the dog. She still wanted to play with her dog and make her dog wet.\n",
        "The next day, Mia went to the park. She saw her friend, Tom. Jerry said, \"Hi, Mia! What are you doing in here?\" Mia told him about the dog and the dog. Tom said, \"I don't know, Mia. Maybe it's a nice dog.\" Mia said, \"Can we play together?\" Tom agreed, and they started running up the hill. They jumped, jumped, and laughed.\n",
        "Then, a big wind came and blew their dog away. Mia and Tom were sad, but they were happy that they played together all day. They learned that being scared is better than being spoiled.\n",
        "\n",
        "\n",
        "\n",
        "Story  8 :\n",
        "Alice lost her dog. The little girl was sad because she was lost and scared. The little girl wanted to help, but the little girl just shook her head.\n",
        "The little girl decided to help the little girl find her way home. They walked and walked until they found the girl's house. The girl was so happy! She gave the little girl a big hug and a special search for her dog. The little girl was not worried anymore and she was happy.\n",
        "\n",
        "\n",
        "\n",
        "Story  9 :\n",
        "Alice lost her dog, Max. \"There is my dog?\" he asked. Max asked. Max said, \"Yes, I lend you my to his friend.\"\n",
        "Max and Sam worked together to get the dog's lost dog's owner's owner. Buddy was happy again and thanked Tom for helping him. From that day on, Max and Sam always helped each other when they needed it.\n",
        "\n",
        "\n",
        "\n",
        "Story  10 :\n",
        "Alice lost her dog. The dog was very sad and scared. The little girl's mom saw her crying and helped her. She said, \"It's okay, my dear. I will help you find your dog.\"\n",
        "The little girl went to the park and found her kitten's mom! The kitten was so happy to see her kitten again. She gave the kitten a big hug and back to normal send it to her mom. The kitten was so thankful! The kitten was happy too, and they both learned that helping good things can make you feel better.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LPdydvfuzNcP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}