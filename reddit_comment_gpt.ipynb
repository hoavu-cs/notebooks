{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "block_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/text/reddit_comments.csv\")\n",
    "body = data[\"body\"].tolist()\n",
    "text = \"\\n\".join(body)\n",
    "\n",
    "with open(\"datasets/text/reddit_comments.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [45, 981, 4365, 338, 2260, 18]\n",
      "Decoded string:  I love programming.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[\"datasets/text/reddit_comments.txt\"], vocab_size=5000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "tokenizer.save_model(\"datasets/text/\")\n",
    "output = tokenizer.encode(\"I love programming.\")\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:39<00:00, 10024.63it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/text/reddit_comments.csv\")\n",
    "text = data[\"body\"].tolist()\n",
    "\n",
    "text_ids = []\n",
    "\n",
    "for t in tqdm(text):\n",
    "    next_ids = tokenizer.encode(t).ids\n",
    "    next_ids.append(tokenizer.encode(\"</s>\").ids[0])\n",
    "    text_ids.append(next_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  900000\n",
      "Test size:  100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]\n",
    "\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/text/reddit_text_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_file_path = \"datasets/text/vocab.json\"\n",
    "merges_file_path = \"datasets/text/merges.txt\"\n",
    "tokenizer = ByteLevelBPETokenizer(vocab_file_path, merges_file_path)\n",
    "\n",
    "text_ids = pickle.load(open(\"datasets/text/reddit_text_ids.pkl\", \"rb\"))\n",
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 19250692\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, n_layers, n_heads, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0, stop_token=False):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "            if stop_token and idx_new.item() == tokenizer.token_to_id('</s>'):\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "vocab_size = vocab_size  # set your vocab size\n",
    "n_emb = 500\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model, optimizer\n",
    "model = LanguageModel(vocab_size, n_emb, n_layers, n_heads, dropout).to(device)\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50]) torch.Size([32, 50])\n",
      "torch.Size([32, 50]) torch.Size([32, 50])\n",
      "I misread that as \"only one of my wife\" and got really confused.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      " misread that as \"only one of my wife\" and got really confused.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - 2, (batch_size,))\n",
    "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    for j, i in enumerate(idx):\n",
    "        if len(data[i]) < block_size + 2:\n",
    "            pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "            data[i] = data[i] + (block_size + 2 - len(data[i])) * [pad_id]\n",
    "        random_start = random.randint(0, len(data[i]) - block_size - 2)\n",
    "\n",
    "        x[j] = torch.tensor(data[i][random_start:random_start + block_size], dtype=torch.long)\n",
    "        y[j] = torch.tensor(data[i][random_start + 1:random_start + block_size + 1], dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "a, b = get_batch(val_text_ids, block_size, batch_size)\n",
    "c, d = get_batch(train_text_ids, block_size, batch_size)\n",
    "print(a.shape, b.shape)\n",
    "print(c.shape, d.shape)\n",
    "\n",
    "print(tokenizer.decode(a[0].tolist()))\n",
    "print(tokenizer.decode(b[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id('</s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 3.176222562789917\n",
      "Validation loss: 2.4758942127227783\n",
      "Step: 100 Training Loss: 3.0432753562927246\n",
      "Validation loss: 2.662243366241455\n",
      "Step: 200 Training Loss: 2.7838966846466064\n",
      "Validation loss: 2.9464097023010254\n",
      "Step: 300 Training Loss: 2.916959762573242\n",
      "Validation loss: 2.8579018115997314\n",
      "Step: 400 Training Loss: 2.440037488937378\n",
      "Validation loss: 2.812772512435913\n",
      "Step: 500 Training Loss: 2.924213171005249\n",
      "Validation loss: 2.2928216457366943\n",
      "Step: 600 Training Loss: 2.3748555183410645\n",
      "Validation loss: 2.5839245319366455\n",
      "Step: 700 Training Loss: 2.78780460357666\n",
      "Validation loss: 2.3382740020751953\n",
      "Step: 800 Training Loss: 2.6527352333068848\n",
      "Validation loss: 2.447197675704956\n",
      "Step: 900 Training Loss: 2.5320656299591064\n",
      "Validation loss: 2.514554023742676\n"
     ]
    }
   ],
   "source": [
    "early_stop = 40\n",
    "last_val_loss = 1e9\n",
    "n_epochs = 1000\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_text_ids, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, val_text_ids, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 40\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'reddit_comments.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shit online.  log. You have been removed because I have atfeerover with get a nice, so a minimum't impro.</s>\n",
      "-----------------------------------\n",
      "Shit about rom pussy</s>\n",
      "-----------------------------------\n",
      "Shit, i get the le<pad><pad>ating since</s>\n",
      "-----------------------------------\n",
      "Shit will be opp floor Aounds</s>\n",
      "-----------------------------------\n",
      "Shit was up because theirplay has green in yourray, piss9DHE</s>\n",
      "-----------------------------------\n",
      "Shit.  prim injed so when power.</s>\n",
      "-----------------------------------\n",
      "Shit this is a nice. New, and Iyou silince the person00 some other guy just statement off the eving please hib</s>\n",
      "-----------------------------------\n",
      "Shits sked</s>\n",
      "-----------------------------------\n",
      "Shitolution and ensure Liverpool expecting’s being, though.</s>\n",
      "-----------------------------------\n",
      "Shit has been an anyway is usingments did</s>\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = 'morons'\n",
    "n_comments = 10\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens).ids\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "model.eval()\n",
    "for _ in range(n_comments):\n",
    "    generation = model.generate(idx, max_new_tokens=100, block_size=block_size, temperature=1, stop_token=True)[0].tolist()\n",
    "    print(tokenizer.decode(generation))\n",
    "    print('-----------------------------------')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
