{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "block_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/text/reddit_comments.csv\")\n",
    "body = data[\"body\"].tolist()\n",
    "text = \"\\n\".join(body)\n",
    "\n",
    "with open(\"datasets/text/reddit_comments.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [45, 981, 4365, 338, 2260, 18]\n",
      "Decoded string:  I love programming.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[\"datasets/text/reddit_comments.txt\"], vocab_size=5000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "tokenizer.save_model(\"datasets/text/\")\n",
    "output = tokenizer.encode(\"I love programming.\")\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [01:39<00:00, 10024.63it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/text/reddit_comments.csv\")\n",
    "text = data[\"body\"].tolist()\n",
    "\n",
    "text_ids = []\n",
    "\n",
    "for t in tqdm(text):\n",
    "    next_ids = tokenizer.encode(t).ids\n",
    "    next_ids.append(tokenizer.encode(\"</s>\").ids[0])\n",
    "    text_ids.append(next_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  900000\n",
      "Test size:  100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]\n",
    "\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/text/reddit_text_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_file_path = \"datasets/text/vocab.json\"\n",
    "merges_file_path = \"datasets/text/merges.txt\"\n",
    "tokenizer = ByteLevelBPETokenizer(vocab_file_path, merges_file_path)\n",
    "\n",
    "text_ids = pickle.load(open(\"datasets/text/reddit_text_ids.pkl\", \"rb\"))\n",
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 19250692\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, n_layers, n_heads, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0, stop_token=False):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "            if stop_token and idx_new.item() == tokenizer.token_to_id('</s>'):\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "vocab_size = vocab_size  # set your vocab size\n",
    "n_emb = 500\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model, optimizer\n",
    "model = LanguageModel(vocab_size, n_emb, n_layers, n_heads, dropout).to(device)\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50]) torch.Size([32, 50])\n",
      "torch.Size([32, 50]) torch.Size([32, 50])\n",
      "I misread that as \"only one of my wife\" and got really confused.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      " misread that as \"only one of my wife\" and got really confused.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - 2, (batch_size,))\n",
    "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    for j, i in enumerate(idx):\n",
    "        if len(data[i]) < block_size + 2:\n",
    "            pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "            data[i] = data[i] + (block_size + 2 - len(data[i])) * [pad_id]\n",
    "        random_start = random.randint(0, len(data[i]) - block_size - 2)\n",
    "\n",
    "        x[j] = torch.tensor(data[i][random_start:random_start + block_size], dtype=torch.long)\n",
    "        y[j] = torch.tensor(data[i][random_start + 1:random_start + block_size + 1], dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "a, b = get_batch(val_text_ids, block_size, batch_size)\n",
    "c, d = get_batch(train_text_ids, block_size, batch_size)\n",
    "print(a.shape, b.shape)\n",
    "print(c.shape, d.shape)\n",
    "\n",
    "print(tokenizer.decode(a[0].tolist()))\n",
    "print(tokenizer.decode(b[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id('</s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 3.024886131286621\n",
      "Validation loss: 2.9100911617279053\n",
      "Step: 100 Training Loss: 2.4201560020446777\n",
      "Validation loss: 2.700127601623535\n",
      "Step: 200 Training Loss: 2.4169325828552246\n",
      "Validation loss: 2.68891978263855\n",
      "Step: 300 Training Loss: 2.4036340713500977\n",
      "Validation loss: 2.3761820793151855\n",
      "Step: 400 Training Loss: 2.2797653675079346\n",
      "Validation loss: 2.272076368331909\n",
      "Step: 500 Training Loss: 2.8898205757141113\n",
      "Validation loss: 2.2783169746398926\n",
      "Step: 600 Training Loss: 2.5931007862091064\n",
      "Validation loss: 2.6256775856018066\n",
      "Step: 700 Training Loss: 2.6782519817352295\n",
      "Validation loss: 2.6166839599609375\n",
      "Step: 800 Training Loss: 2.364764451980591\n",
      "Validation loss: 2.3968307971954346\n",
      "Step: 900 Training Loss: 2.8317925930023193\n",
      "Validation loss: 2.4520225524902344\n",
      "Step: 1000 Training Loss: 2.5702385902404785\n",
      "Validation loss: 2.295271873474121\n",
      "Step: 1100 Training Loss: 2.4665656089782715\n",
      "Validation loss: 2.7514758110046387\n",
      "Step: 1200 Training Loss: 2.779698371887207\n",
      "Validation loss: 2.7152416706085205\n",
      "Step: 1300 Training Loss: 2.6666460037231445\n",
      "Validation loss: 1.9960192441940308\n",
      "Step: 1400 Training Loss: 2.0411598682403564\n",
      "Validation loss: 2.3786704540252686\n",
      "Step: 1500 Training Loss: 2.764552593231201\n",
      "Validation loss: 2.3145806789398193\n",
      "Step: 1600 Training Loss: 2.5707309246063232\n",
      "Validation loss: 2.9601614475250244\n",
      "Step: 1700 Training Loss: 2.619419574737549\n",
      "Validation loss: 2.5488650798797607\n",
      "Step: 1800 Training Loss: 2.7011730670928955\n",
      "Validation loss: 2.425713539123535\n",
      "Step: 1900 Training Loss: 2.3004398345947266\n",
      "Validation loss: 2.795276165008545\n",
      "Step: 2000 Training Loss: 1.8873307704925537\n",
      "Validation loss: 2.5130486488342285\n",
      "Step: 2100 Training Loss: 2.3426523208618164\n",
      "Validation loss: 2.7318954467773438\n",
      "Step: 2200 Training Loss: 2.7361297607421875\n",
      "Validation loss: 2.830627202987671\n",
      "Step: 2300 Training Loss: 2.669581413269043\n",
      "Validation loss: 2.339406967163086\n",
      "Step: 2400 Training Loss: 2.4127204418182373\n",
      "Validation loss: 2.51652193069458\n",
      "Step: 2500 Training Loss: 2.608128309249878\n",
      "Validation loss: 2.199536085128784\n",
      "Step: 2600 Training Loss: 2.3830785751342773\n",
      "Validation loss: 2.594064235687256\n",
      "Step: 2700 Training Loss: 2.0111613273620605\n",
      "Validation loss: 2.3545753955841064\n",
      "Step: 2800 Training Loss: 2.1524558067321777\n",
      "Validation loss: 2.086099147796631\n",
      "Step: 2900 Training Loss: 2.386733293533325\n",
      "Validation loss: 1.9280874729156494\n",
      "Step: 3000 Training Loss: 2.9911913871765137\n",
      "Validation loss: 2.5507330894470215\n",
      "Step: 3100 Training Loss: 2.4773759841918945\n",
      "Validation loss: 2.5488405227661133\n",
      "Step: 3200 Training Loss: 2.727126359939575\n",
      "Validation loss: 2.40152645111084\n",
      "Step: 3300 Training Loss: 1.8538545370101929\n",
      "Validation loss: 2.4203481674194336\n",
      "Step: 3400 Training Loss: 2.457247257232666\n",
      "Validation loss: 2.340287685394287\n",
      "Step: 3500 Training Loss: 2.5935230255126953\n",
      "Validation loss: 2.0752058029174805\n",
      "Step: 3600 Training Loss: 2.575211524963379\n",
      "Validation loss: 2.095498561859131\n",
      "Step: 3700 Training Loss: 2.3820998668670654\n",
      "Validation loss: 2.2085208892822266\n",
      "Step: 3800 Training Loss: 2.3066182136535645\n",
      "Validation loss: 2.1523070335388184\n",
      "Step: 3900 Training Loss: 2.4673030376434326\n",
      "Validation loss: 2.650827407836914\n",
      "Step: 4000 Training Loss: 2.4991955757141113\n",
      "Validation loss: 2.496615409851074\n",
      "Step: 4100 Training Loss: 2.7771823406219482\n",
      "Validation loss: 2.646933078765869\n",
      "Step: 4200 Training Loss: 2.4749436378479004\n",
      "Validation loss: 2.5513250827789307\n",
      "Step: 4300 Training Loss: 2.5312392711639404\n",
      "Validation loss: 2.6769049167633057\n",
      "Step: 4400 Training Loss: 2.535914659500122\n",
      "Validation loss: 2.0111143589019775\n",
      "Step: 4500 Training Loss: 2.3432366847991943\n",
      "Validation loss: 2.4576549530029297\n",
      "Step: 4600 Training Loss: 2.474921703338623\n",
      "Validation loss: 2.7140398025512695\n",
      "Step: 4700 Training Loss: 2.537315607070923\n",
      "Validation loss: 2.3654682636260986\n",
      "Step: 4800 Training Loss: 2.3490400314331055\n",
      "Validation loss: 2.7108349800109863\n",
      "Step: 4900 Training Loss: 2.258989095687866\n",
      "Validation loss: 2.267451286315918\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "early_stop = 20\n",
    "last_val_loss = 1e9\n",
    "n_epochs = 15000\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_text_ids, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, val_text_ids, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 20\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'reddit_comments.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[643, 374, 82, 3407]\n",
      "technology, but I know it's actually not a years ago, which you are glorious enough to suck someone your own children. Why would you make it a bit to break today their own community?\n",
      "\n",
      "Personal-ww I've been selling my ancase once you staring around hardens.. ComÔøΩie_pension. It out of a few weeks, but don't seem to feel like Harden. I'm so glad we can\" here punched it? Actually. I'd say shitty is unbrawnable while assuming gay wouldn't have been to a lot more effective unexpected. Not that he just wasn't really happy to whal. Don/don't trade on the wall, he's just so desperately badly he is misarrassed.\n",
      "\n",
      "Is this dead argument with her ass owned abro-RIession\n",
      "\n",
      "\n",
      "If you hold her in that rocks to her\n",
      "-----------------------------------\n",
      "technology are allasions power insurance out, but since I just think that he was going to be the easters of the homach. What's she rey and NK. Harte was running about 2 seasons later in the entire days. We's been 16 because it might fit into the fire (and had to do battle. Even if Trump charges will not fully captions, there's a finally way more attention to State areers). Eeter (i.eouthble). Full as the boot when its class decades live for a push, wall we originally deleted). But I'd allow to go over nowhere near the Valcon, and jump to a few.</s>\n",
      "-----------------------------------\n",
      "technology doesn't have a great sedit of a relevant. He was setup in viewers of our centers and supports at all. But people who watch Gimmickard over the dead.\n",
      "\n",
      "This is victim treating ok, obviously they bring your eyes out. But yes. Its a great ring ratey interchise that you probably are a greatest. If I didn't which in your castle due to a law and your team with a character kills, honestly. We flawed safely would probably be difficult. We certainly thought it's cheating to show the point of timelines but they'd just have known that the person holding in the past&amp;subject=n't the ability to play the slowassion.\n",
      "\n",
      "https://www.reddit.com/r/Market76/comments/bj handmadeprxs_that_reddit_thrtra_inflents.2/04/28/) to\n",
      "-----------------------------------\n",
      "technology is to currently not be at cameras and/or.\n",
      "\n",
      "Byas he is an old thing. They also got foughterateful, that the audience failed out on the wolf to them.</s>\n",
      "-----------------------------------\n",
      "technology should be strictly classified though,.  \n",
      "\n",
      "As one (blAn on the acknowledge to qualify \"she is somehow misiallyness.\"</s>\n",
      "-----------------------------------\n",
      "technology?! She is going to the wevanoid.hat not like you‚Äôre a hero of 10 wins grown away, the first time he came on a marlish future. Such of having guns on the top of it struggles, and that‚Äôs why she‚Äôs mostly the places you went down, and she‚Äôs them</s>\n",
      "-----------------------------------\n",
      "technology, to Junk team, alive was in the box with Cap's boyfriend to the dead of unexpected in the main war–æ–≤. You guys are seeing Dany who has been a burstoth but playing with no kill. He the mainly cheated. He doesn't have the enter. Went all the dead around the Nighto to the Night King says something to convince the average, but now that he won't get his choice to put in the door people for Linvision and the sign amuletered to him long though. Remember of the Drandies, so he has a lot cheaper style, the past.\n",
      "\n",
      "No, Tony. Nowary is 205. In Turprary Sony. And he's ex-represented by another exists. Abra haulder to slurked with like a cunting party of garbage so those women can do but she shouldn't use a\n",
      "-----------------------------------\n",
      "technology has been active and younger since i relax because of it was a guy title these rate in Avengers 5 frame leg.\n",
      "\n",
      "Like in the most consistent crime.\n",
      "\n",
      "Heabbing to help you get to google in the past. He got into one of the best LEAK. Impact with those movies trying to tell NK used skins for him but I think more (for up king (As part of the Avengers *stabine that Arya as a sociroid of Cersei and death) is about to change Cersei's fault. She was 7 years from VIC is so laughed out one winter.\n",
      "\n",
      "ChvN is a profit titles of existing to kill her investigation and that she had more than begged to guidelines. That's Arya. But she's not white when she didn't. Can I admit it is a great day?</s>\n",
      "-----------------------------------\n",
      "technology by'm worried about the movie on where you start discrimination is.\n",
      "\n",
      "Not exactly gray just for and look up and not.\n",
      "\n",
      "It might not be aival kill definition of —ã to add the conversationy now. Childl/speople of c redactions, (i very slasing old) so yeah \n",
      "At obtaining somewhere to them). If you keep your life or whatever or stay that initially. Minecrafts of other xsties and aren't sure you want to be evil. Maybe you write their actions. Seeing, that was even slightly pulling it. Secals already.\n",
      "\n",
      "There are many if there are much less anything, neither time again, even tho. It doesn't make it wrong.\n",
      "\n",
      "I am always the only reason you weren't suppose from my next season.\n",
      "Um started on of the period.\n",
      "\n",
      "It doesn't have\n",
      "-----------------------------------\n",
      "technology, but what‚Äôs the only one that's not. Shurg as if you can't never stand whatever. He felt to you but lets use Turns, mashesra.\" because he does a good package to kill him. That would be pretty clear that, even. Patrot, by not mean you‚Äôre not wrong. He won‚Äôt mean she wasn‚Äôt going to know what</s>\n",
      "-----------------------------------\n",
      "technology.ll have c-Talities.\n",
      "\n",
      "don does once he does! collate children.\n",
      "\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "But it‚Äôs pretty clear about Arya and Cersei was talking about times so she‚Äôs really a secret maybe. I have an asshole? Like supports, let's pretend of his enemy, Jaime, Aattle of the King and prior to no longer destroy against Rau protections, make sure the dead got shot.\n",
      "\n",
      "My good people would think it will say we have seen any strings don't get a killer at the fighter platform.\n",
      "\n",
      "Pifferifying when he's uncle was just not a popular budget.\n",
      "\n",
      "Edit: Why dont he supported but he is insulted?\n",
      "\n",
      "2 of Jotal, Tuger women‚Äôs Right. Before flavored.\n",
      "\n",
      "There is no, confirming or laucomingimate army,\n",
      "-----------------------------------\n",
      "technology me and won't be able to listen to his following reboot, he's not removing Kurning skin.\n",
      "\n",
      "Guy and holds his ability to consider his own positivity toars but the person always felt like he's got these students myconnier destroyed to be more under trouble throne than Biden LeBThank and Dany were as recent years. I'm not oking a few seconds is better than I've been reading too strongly. What, app is it being something entailed? The Asians loved during the Winterfell the battle of Winterfell was thinking of the abuse. The Rightsless men know Mntil propaganda is downed. We‚Äôd handle them pocket of other ones, whatever bird innoces was looking for.\n",
      "\n",
      "That failed cant believe that someone dating it, like a better team. \n",
      "\n",
      "The unpopular champs can tell if they weren‚Äôt the army of misleading.\n",
      "\n",
      "-----------------------------------\n",
      "technologyically quote about regulator (and comp;/ hold this their audience consistency or anything,/trialism which can be able to end up or misoxation. If it ends at bear doesn't require desperately that healthy will be cool. Either things have started that (certainal coffee dressful much, and it feels saferworth). Weshoin army, that way or the characters can do top jiles, do you count to interact with no abstant.\n",
      "\n",
      "You can't generate whether to others know this as that doesn't look a bit aware that the base would want to know women expect sex is likely to moon, coming up. I don't know if you win from seeing is mentally ideally happy you, stunning into other daughter. Be Icedraline. No subreddit is just that there's nothing exposed to your reading to the doctor? Maybe I\n",
      "-----------------------------------\n",
      "technology : added with them,</s>\n",
      "-----------------------------------\n",
      "technology, four seasons is these days. It's our ones. Like today went back to abeduclear and their religious ones King is fine to choose what he would make?\n",
      "\n",
      "Also this happened joke cinemalizera ended. I really don't think I'm not saying he can develop, however that's the infamation, but they will wear all creation console\"taints on well. \n",
      "\n",
      "The idea of our colonies do superior wasnt the good memorial she knew about this show under IV‚Äù movies are beautiful, so the \"implish that matter is not normal. I mean, why are we on 100% ever? I wouldn't....\" Don't do this with Arya (ride) is mean to. I've gotten that guy outside the story of steishing about Thanos.\n",
      "\n",
      "This is not about? Or unless my night is *she charming'. It's disgusting\n",
      "-----------------------------------\n",
      "technology, but because in police warnings &amp; proper stickmentsual titles, curious, or among that the other details it was my interest bobble, and were still on the season... ‚Äúhaving the monster‚Äù so he retired out quickly a secondary. It‚Äôs going to really care more stuff yesterday when he starts. Going and their shoulder.\n",
      "\n",
      "That‚Äôs hate scenes.\n",
      "\n",
      "Ince's death, especially if he is knowing it seems to mean they do anything the kitchen will want.\n",
      "\n",
      "Good stopping yards, even better players. Why have you even travemustor realizing what he cares? Apparently with a guy. They were your writial background standards and plays It doesn‚Ä¶ to be seen as Tyrion or ambulated to it. But you can belong to accurately Cersei.\n",
      "\n",
      "Only cool.  Because I mean, you get hacked\n",
      "-----------------------------------\n",
      "technologyer champ is fear of memes with Ghost, dude, like it is said she‚Äôs insatited coming up about her children because its probably an unfair comparable message - the guy who can‚Äôt undereed. Either of what he did. miss now Comey or something about fighting by you. Such a clear activar (because my daughter).\n",
      "\n",
      "Back ideas, nature, and depression, and typically), (Yes as if is a more distractions). Only things are bad). We brought back, from a retployment formalist.\n",
      "\n",
      "I've brought a bized defence described her. This has idea one to grass his claims make people that are absolute unreal on the fastest. She needs to go to the Syrian, but she's lowent screen or football. \n",
      "\n",
      "But a better job.\n",
      "\n",
      "Comeing. Biden, she has not been\n",
      "-----------------------------------\n",
      "technology, where he was done by not wanting to sign sure they were provary to him then connected alive and is elected.  Basically he even set up Barr who has Jined Legej did not.\n",
      "\n",
      "‚ÄúThe Night King of Covers‚Äù doesn‚Äôt get \"Well I could prove ‚ÄúFalks, Storiia‚Äù video from a living provotant is a imbalanced opinion and Lougiocre conditions/Mightnings. TIL complete shit than I‚Äôm just trying to argument that a stabilies in all...\n",
      "\n",
      "Post: got one intern, and egg, rather than explained.</s>\n",
      "-----------------------------------\n",
      "technology would give many more veaths to help with his son and not fall in 40 days and Arya hit fries. \n",
      "Good,), he is giving him boss overthink of how his try shoes were going to list that before Mueller, we had his figure without what Barr becomes and consmartized lied. J political needs it. This is not dragon fire rate, reported by 195 days. \n",
      "\n",
      "I really like English by as a small rock/recessary. I don‚Äôt feel....\n",
      "\n",
      "He‚Äôs truly able to right?\n",
      "\n",
      "The rude is so bad. It‚Äôs anything about this matter. I‚Äôm painful.\"\n",
      "\n",
      "Sure, republicans, and premium, or changes that wing attacks is also the snap. I don‚Äôt know if I want to die. He used a seaks, on the skin, she clearly has a bad books and things should consider\n",
      "-----------------------------------\n",
      "technology, and G food's mom did a good Marvel movie in the first place progressive first, and if she got drunk questions, it's not the case you can't watched her to tissa instead of beer than she washing!\"\n",
      "Not betrayed to you is a point? Would someone think you are not allowed?\n",
      "\n",
      "I see. Good ensure that, what do you do with the info.... üòçüòç</s>\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = 'technology'\n",
    "n_comments = 20\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens).ids\n",
    "print(encoded_start)\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "model.eval()\n",
    "for _ in range(n_comments):\n",
    "    generation = model.generate(idx, max_new_tokens=200, block_size=block_size, temperature=1, stop_token=True)[0].tolist()\n",
    "    print(tokenizer.decode(generation))\n",
    "    print('-----------------------------------')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
