{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "block_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/text/reddit_comments.csv\")\n",
    "body = data[\"body\"].tolist()\n",
    "text = \"\\n\".join(body)\n",
    "\n",
    "with open(\"datasets/text/reddit_comments.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [45, 981, 5166, 2260, 18]\n",
      "Decoded string:  I love programming.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[\"datasets/text/reddit_comments.txt\"], vocab_size=15000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "tokenizer.save_model(\"datasets/text/\")\n",
    "output = tokenizer.encode(\"I love programming.\")\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:36<00:00, 10334.85it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/text/reddit_comments.csv\")\n",
    "text = data[\"body\"].tolist()\n",
    "\n",
    "text_ids = []\n",
    "\n",
    "for t in tqdm(text):\n",
    "    next_ids = tokenizer.encode(t).ids\n",
    "    next_ids.append(tokenizer.encode(\"</s>\").ids[0])\n",
    "    text_ids.append(next_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  900000\n",
      "Test size:  100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]\n",
    "\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Test size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/text/reddit_text_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_file_path = \"datasets/text/vocab.json\"\n",
    "merges_file_path = \"datasets/text/merges.txt\"\n",
    "tokenizer = ByteLevelBPETokenizer(vocab_file_path, merges_file_path)\n",
    "\n",
    "text_ids = pickle.load(open(\"datasets/text/reddit_text_ids.pkl\", \"rb\"))\n",
    "\n",
    "N = len(text_ids)\n",
    "train_size = int(0.9 * N)\n",
    "test_size = N - train_size\n",
    "train_text_ids = text_ids[:train_size]\n",
    "val_text_ids = text_ids[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 16125692\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, n_layers, n_heads, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0, stop_token=False):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "            if stop_token and idx_new.item() == tokenizer.token_to_id('</s>'):\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "vocab_size = vocab_size  # set your vocab size\n",
    "n_emb = 300\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model, optimizer\n",
    "model = LanguageModel(vocab_size, n_emb, n_layers, n_heads, dropout).to(device)\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      " me seriously rip u. If i ever meet u irl, i will bet a 100 bucks on that you either are in a abusive psychological terror relationship or a mental wreck. But gl in the future, and dont let the pussy eat your mind,\n",
      " seriously rip u. If i ever meet u irl, i will bet a 100 bucks on that you either are in a abusive psychological terror relationship or a mental wreck. But gl in the future, and dont let the pussy eat your mind, you\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - 2, (batch_size,))\n",
    "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    for j, i in enumerate(idx):\n",
    "        if len(data[i]) < block_size + 2:\n",
    "            pad_id = tokenizer.token_to_id(\"<pad>\")\n",
    "            data[i] = data[i] + (block_size + 2 - len(data[i])) * [pad_id]\n",
    "        random_start = random.randint(0, len(data[i]) - block_size - 2)\n",
    "\n",
    "        x[j] = torch.tensor(data[i][random_start:random_start + block_size], dtype=torch.long)\n",
    "        y[j] = torch.tensor(data[i][random_start + 1:random_start + block_size + 1], dtype=torch.long)\n",
    "    return x, y\n",
    "\n",
    "a, b = get_batch(val_text_ids, block_size, 1)\n",
    "c, d = get_batch(train_text_ids, block_size, 1)\n",
    "print(a.shape, b.shape)\n",
    "print(c.shape, d.shape)\n",
    "\n",
    "print(tokenizer.decode(a[0].tolist()))\n",
    "print(tokenizer.decode(b[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.token_to_id('</s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 9.68064022064209\n",
      "Validation loss: 8.08499526977539\n",
      "Step: 100 Training Loss: 3.664595603942871\n",
      "Validation loss: 3.7453010082244873\n",
      "Step: 200 Training Loss: 2.981350898742676\n",
      "Validation loss: 2.8595376014709473\n",
      "Step: 300 Training Loss: 3.253497838973999\n",
      "Validation loss: 3.011801242828369\n",
      "Step: 400 Training Loss: 3.3305392265319824\n",
      "Validation loss: 2.9725754261016846\n",
      "Step: 500 Training Loss: 2.5168373584747314\n",
      "Validation loss: 3.45869779586792\n",
      "Step: 600 Training Loss: 3.5765695571899414\n",
      "Validation loss: 3.0200717449188232\n",
      "Step: 700 Training Loss: 3.1930768489837646\n",
      "Validation loss: 3.3541150093078613\n",
      "Step: 800 Training Loss: 2.3372642993927\n",
      "Validation loss: 2.8022594451904297\n",
      "Step: 900 Training Loss: 3.661689043045044\n",
      "Validation loss: 3.5577988624572754\n",
      "Step: 1000 Training Loss: 2.6247994899749756\n",
      "Validation loss: 2.8534739017486572\n",
      "Step: 1100 Training Loss: 2.9267828464508057\n",
      "Validation loss: 3.2338998317718506\n",
      "Step: 1200 Training Loss: 2.6099724769592285\n",
      "Validation loss: 2.720210313796997\n",
      "Step: 1300 Training Loss: 2.615666151046753\n",
      "Validation loss: 2.857428550720215\n",
      "Step: 1400 Training Loss: 2.5774407386779785\n",
      "Validation loss: 2.5514330863952637\n",
      "Step: 1500 Training Loss: 2.902261972427368\n",
      "Validation loss: 2.815727472305298\n",
      "Step: 1600 Training Loss: 2.7857940196990967\n",
      "Validation loss: 3.049875497817993\n",
      "Step: 1700 Training Loss: 3.4997305870056152\n",
      "Validation loss: 2.8077635765075684\n",
      "Step: 1800 Training Loss: 2.93032169342041\n",
      "Validation loss: 2.922123908996582\n",
      "Step: 1900 Training Loss: 2.9076290130615234\n",
      "Validation loss: 2.540337324142456\n",
      "Step: 2000 Training Loss: 2.871323823928833\n",
      "Validation loss: 2.6160812377929688\n",
      "Step: 2100 Training Loss: 2.8934409618377686\n",
      "Validation loss: 2.954779863357544\n",
      "Step: 2200 Training Loss: 3.141890048980713\n",
      "Validation loss: 2.5555903911590576\n",
      "Step: 2300 Training Loss: 3.325327157974243\n",
      "Validation loss: 2.445375919342041\n",
      "Step: 2400 Training Loss: 3.057745933532715\n",
      "Validation loss: 2.793668508529663\n",
      "Step: 2500 Training Loss: 2.5453107357025146\n",
      "Validation loss: 2.472287654876709\n",
      "Step: 2600 Training Loss: 3.1533565521240234\n",
      "Validation loss: 3.4986677169799805\n",
      "Step: 2700 Training Loss: 2.7431323528289795\n",
      "Validation loss: 2.7545998096466064\n",
      "Step: 2800 Training Loss: 2.9371984004974365\n",
      "Validation loss: 3.024474859237671\n",
      "Step: 2900 Training Loss: 2.8453779220581055\n",
      "Validation loss: 2.694427490234375\n",
      "Step: 3000 Training Loss: 2.9223110675811768\n",
      "Validation loss: 3.241947889328003\n",
      "Step: 3100 Training Loss: 2.660665512084961\n",
      "Validation loss: 2.4972434043884277\n",
      "Step: 3200 Training Loss: 3.3866055011749268\n",
      "Validation loss: 2.8158702850341797\n",
      "Step: 3300 Training Loss: 2.835751533508301\n",
      "Validation loss: 2.5976595878601074\n",
      "Step: 3400 Training Loss: 3.359121084213257\n",
      "Validation loss: 2.420305013656616\n",
      "Step: 3500 Training Loss: 2.9213497638702393\n",
      "Validation loss: 2.2735776901245117\n",
      "Step: 3600 Training Loss: 2.4553606510162354\n",
      "Validation loss: 2.4352152347564697\n",
      "Step: 3700 Training Loss: 3.4939541816711426\n",
      "Validation loss: 2.680591344833374\n",
      "Step: 3800 Training Loss: 2.8173787593841553\n",
      "Validation loss: 2.547849655151367\n",
      "Step: 3900 Training Loss: 2.1298396587371826\n",
      "Validation loss: 2.534165859222412\n",
      "Step: 4000 Training Loss: 2.474273443222046\n",
      "Validation loss: 2.771268844604492\n",
      "Step: 4100 Training Loss: 2.4059555530548096\n",
      "Validation loss: 2.362776756286621\n",
      "Step: 4200 Training Loss: 2.7617855072021484\n",
      "Validation loss: 2.232969284057617\n",
      "Step: 4300 Training Loss: 2.9758410453796387\n",
      "Validation loss: 2.439556837081909\n",
      "Step: 4400 Training Loss: 2.4858644008636475\n",
      "Validation loss: 2.5066916942596436\n",
      "Step: 4500 Training Loss: 2.185551643371582\n",
      "Validation loss: 2.3194901943206787\n",
      "Step: 4600 Training Loss: 3.2829055786132812\n",
      "Validation loss: 2.566679000854492\n",
      "Step: 4700 Training Loss: 3.109821081161499\n",
      "Validation loss: 2.471071243286133\n",
      "Step: 4800 Training Loss: 2.7673685550689697\n",
      "Validation loss: 3.2245898246765137\n",
      "Step: 4900 Training Loss: 2.3094277381896973\n",
      "Validation loss: 2.6512041091918945\n",
      "Step: 5000 Training Loss: 2.448040246963501\n",
      "Validation loss: 2.3261666297912598\n",
      "Step: 5100 Training Loss: 2.540273904800415\n",
      "Validation loss: 3.0020620822906494\n",
      "Step: 5200 Training Loss: 2.337395191192627\n",
      "Validation loss: 2.5766940116882324\n",
      "Step: 5300 Training Loss: 2.611628532409668\n",
      "Validation loss: 2.7802295684814453\n",
      "Step: 5400 Training Loss: 2.6948769092559814\n",
      "Validation loss: 2.5375261306762695\n",
      "Step: 5500 Training Loss: 2.6883695125579834\n",
      "Validation loss: 2.8505618572235107\n",
      "Step: 5600 Training Loss: 2.5359225273132324\n",
      "Validation loss: 2.793914556503296\n",
      "Step: 5700 Training Loss: 2.726405143737793\n",
      "Validation loss: 2.7376465797424316\n",
      "Step: 5800 Training Loss: 2.9734649658203125\n",
      "Validation loss: 2.7568938732147217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hvutr\\Git\\notebooks\\reddit_comment_gpt.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m xb \u001b[39m=\u001b[39m xb\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m yb \u001b[39m=\u001b[39m yb\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m logits, loss \u001b[39m=\u001b[39m model(xb, yb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\hvutr\\Git\\notebooks\\reddit_comment_gpt.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m x_transform \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mclone()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m mask \u001b[39m=\u001b[39m generate_square_subsequent_mask(T)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m x_transform \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x_transform\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m), mask\u001b[39m=\u001b[39;49mmask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m x_transform \u001b[39m=\u001b[39m x_transform\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/reddit_comment_gpt.ipynb#X13sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m x_transform\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    303\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[0;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 306\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[0;32m    308\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    309\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:573\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    571\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 573\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    574\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:581\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    580\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 581\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    582\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    583\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    584\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1176\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1177\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1187\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[0;32m   1188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1190\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1194\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1195\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m   1196\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1197\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m   1198\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[0;32m   1199\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[0;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1201\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:5216\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5214\u001b[0m     \u001b[39mif\u001b[39;00m attn_mask\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m correct_2d_size:\n\u001b[0;32m   5215\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe shape of the 2D attn_mask is \u001b[39m\u001b[39m{\u001b[39;00mattn_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, but should be \u001b[39m\u001b[39m{\u001b[39;00mcorrect_2d_size\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 5216\u001b[0m     attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\n\u001b[0;32m   5217\u001b[0m \u001b[39melif\u001b[39;00m attn_mask\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m   5218\u001b[0m     correct_3d_size \u001b[39m=\u001b[39m (bsz \u001b[39m*\u001b[39m num_heads, tgt_len, src_len)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stop = 100\n",
    "last_val_loss = 1e9\n",
    "n_epochs = 15000\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_text_ids, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, val_text_ids, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 100\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'reddit_comments.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14987]\n",
      "Politics is the most likely to be a debut. I'm not your opinion. They’re not telling me. I think he should be a great player. \n",
      "\n",
      "I don’t think he’s a man, but I’m not just a great idea that I’ll have to be a shame to do what you’re doing.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&gt;I’m not a lot of this. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I would probably be playing up and that’s what you’re doing. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "The whole world is not a big way.\n",
      "\n",
      "Also, I’m not sure if you’re not a joke about it, but I’m not watching you. I think you’ll be interested.\n",
      "-----------------------------------\n",
      "Politics and the time is that you can't be able to get a problem with the person to do it.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "They can't get a shit about it. She's not the case of the game. \n",
      "\n",
      "They are not a lot of people that have to start people, but I think the current person is still going to do.\n",
      "-----------------------------------\n",
      "Politics of the \"cone\" and the \"decurious\" or a problem\" is the only one. It's a lot easier to be the best teams.  I'm not a real life.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I think it's not a lot of people, but I'm not sure that this is not what you want. Just like. \n",
      "\n",
      "I think your own parents are a lot of people who are going to do. I think you know what you're doing. This is the only person you can make. \n",
      "\n",
      "I think you should know you can't make an excuse to think you're playing to me. \n",
      "\n",
      "I'm saying it's not a great idea. I don't think you think you're not doing. I think you can't have a one that's too much. \n",
      "\n",
      "The whole way to see a fan of the way to be doing\n",
      "-----------------------------------\n",
      "Politicsed and the whole thing's \"you’ll have to speak\"\n",
      "\n",
      "https://youtu.be/r/asoiaf/g) and the other way. If they are not the same way I’ve been a great player, I’m not sure it’s a bad idea.  I should be the problem.\n",
      "\n",
      "I’m not sure if you’ll get the same way to start my life. And it’s a great idea. \n",
      "\n",
      "And you’re a great idea what you’re saying you’re doing.\n",
      "-----------------------------------\n",
      "Politicss in the middle class. \n",
      "\n",
      "I think the Stravelo? \n",
      "\n",
      "You must be a good one.  I think the movie is that the way to be out of the same.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I think you're a great player.\n",
      "-----------------------------------\n",
      "Politics of the episode. \n",
      "\n",
      "That’s a great way to do this, if you’re not a Muslim. \n",
      "\n",
      "The case is a better player.\n",
      "\n",
      "I’m not to see the same, I’m not a lot of a lot of people. There’s a lot of people have a lot of people to think the president have been dead, but I’m not going to be able to see if you get it. I’m just curious how you’re a great job. I’m not sure you’ve been playing with that. \n",
      "\n",
      "I’m just saying I’m not sure how you’re saying if you go. \n",
      "\n",
      "It’s just a lot of a lot of people saying the other people are just a twelve.\n",
      "\n",
      "The way you should have made it a lot of people being a lot of money, I’ve been saying that the whole thing is the same.\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "Politics, but I don't think it's a great point.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "Also, he's just a lot better than a person who was just a lot to know what they were doing.\n",
      "\n",
      "Otherwise, I don't know what you're doing. \n",
      "\n",
      "I'm not sure why they were saying.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I think you're not a fan of a lot of people who are.\n",
      "-----------------------------------\n",
      "Politicsed by far more than a few years ago. \n",
      "\n",
      "&gt; I'm not a lot of people are not saying that I'm not a problem.\n",
      "\n",
      "I think the truth is a great movie.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "But that's a problem.\n",
      "-----------------------------------\n",
      "Politics. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "Turns_baby, there are many people and the kids are not a new future.\n",
      "\n",
      "No, it’s a repost.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I’m not sure if you’re not getting the soundtrack.\n",
      "\n",
      "I’m not sure what you’re saying. I’m not sure if you’re looking for if you’re not illegal.\n",
      "-----------------------------------\n",
      "Politicsed.\n",
      "\n",
      "\n",
      "&gt;\n",
      "\n",
      "&gt;\n",
      "&gt;I'm not an asshole, and I'm not a list of a newborn.\n",
      "\n",
      "The one is a great place that you should know what you want, you can do.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "If you can't even think the main problem is that I'm not getting the most of the game.\n",
      "\n",
      "They are just a lot of people. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I think he's not the only reason I'm not surprised you will say that the one is actually a lot of times, but you are not a better.\n",
      "-----------------------------------\n",
      "Politics the shady and the whole thing are being a real shit. \n",
      "\n",
      "&gt;\n",
      "\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; I think it's a lot of guns are not a matter how to do this, it's a better story.\n",
      "\n",
      "There's a lot of the people who are not an approved.\n",
      "\n",
      "There's no reason to be a great idea that they are all the other things that are not a lot of a lot of people. \n",
      "\n",
      "The guy are the only reason that you think the only way you can see this is a great way to keep the person who are not a really good.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I'm not gonna try to pay for the same way. They have to do this.\n",
      "\n",
      "I think the first person was not the only issue, I'm just looking at the same time. We've been a fool.\n",
      "-----------------------------------\n",
      "Politics is not a different title.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/asoiaf) if you have any questions or concerns.*\n",
      "\n",
      "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/gameofthrones) if you have any questions or concerns.*\n",
      "\n",
      "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dankmemes) if you have any questions or concerns.*\n",
      "\n",
      "\n",
      "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/gameofthrones) if you have any questions or concerns.*\n",
      "\n",
      "*I am a bot, and this action was performed automatically. Please\n",
      "-----------------------------------\n",
      "Politics of the 8th grade and then the most likely to be a very good thing. \n",
      "\n",
      "You know what you want to do. \n",
      "\n",
      "I think it's the same way, I'm just looking forward to the way you are. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I'm not a shame.\n",
      "\n",
      "I'm not a lot of people that are not a single person who aren't the same thing. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I'd just be on the next battle.\n",
      "\n",
      "I think that's what you want. I've heard, I think you're not a little to say. I think you're a great idea how I'm going to do. That's just a great idea. I'm not sure that.\n",
      "\n",
      "edit: I'm not a lot of people. I'm not sure if you're right, I'm not really sorry for the person who can't much. I'd like to\n",
      "-----------------------------------\n",
      "Politics of a 3rd and every year.  I don't think that's a shit ton of people are on the first time. \n",
      "\n",
      "Counting and the spicy is a good idea that are just the same as they are all of the same.\n",
      "-----------------------------------\n",
      "Politics is the only thing I think I can’t see how it’s not a lot of people are doing the truth.\n",
      "\n",
      "They’ll be doing the Finnamion.\n",
      "\n",
      "I’m not sure if it’s just a problem, I’m seeing it.\n",
      "-----------------------------------\n",
      "Politicsing to the time, but it’s a new one. \n",
      "\n",
      "They are a better way to make it to do this, but I’m not a great way to be in a single person that was the most one of the most likely to be a huge part of the game. \n",
      "\n",
      "I think it’s not a lot of my opinion that you’re not a person and you can’t get the good idea, I’m not even playing people as much. I’ve only seen the story but I’m not sure what you’re doing, I’m talking about. I’m not getting a shit about.\n",
      "-----------------------------------\n",
      "Politicss and on the most of the guy and it was very good. \n",
      "\n",
      "I'm just thinking about how you can't just get that, it's a lot of the time I feel. \n",
      "\n",
      "No, I don't think that.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I'm so much, it's a lot of people who are wrong, I think I'm guessing the episode. I don't think the one is going to be a great thing, but I've never seen it.\n",
      "-----------------------------------\n",
      "Politics/10, \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "[Cic](https://youtu.be/P/gXCUVhq95\n",
      "-----------------------------------\n",
      "Politics and the way he was saying that he was the only thing I had the same thing to say. \n",
      "\n",
      "I'm not thinking about it.\n",
      "\n",
      "Also, it was the same way. The guy is the same.\n",
      "\n",
      "I'm not saying you can't do anything about it.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I'm not sure how I'm the best. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "If you want to say that I'm just a lot of times.\n",
      "\n",
      "The problem is the better.\n",
      "\n",
      "Why are you saying you are you talking about you?\n",
      "-----------------------------------\n",
      "Politics-bour.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I'm not going to just a lot of people.\n",
      "\n",
      "The only one is that the one that can't be a bad idea.\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = 'Politics'\n",
    "n_comments = 20\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens).ids\n",
    "print(encoded_start)\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "model.eval()\n",
    "for _ in range(n_comments):\n",
    "    generation = model.generate(idx, max_new_tokens=200, block_size=block_size, temperature=0.5, stop_token=True)[0].tolist()\n",
    "    print(tokenizer.decode(generation))\n",
    "    print('-----------------------------------')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
