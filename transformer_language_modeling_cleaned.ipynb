{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([train_data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([train_data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) / (C**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = F.dropout(wei, p=dropout)\n",
    "        # perform score aggregation\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = F.dropout(x, p=dropout)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4*n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_emb, n_emb),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block followed by computation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = n_emb // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, self.head_size)\n",
    "        self.ff = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        x = F.dropout(x, p=dropout)\n",
    "        return x\n",
    "        \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_heads) for _ in range(n_layers)])\n",
    "        self.feed_forward = FeedForward()\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = token_emb + position_emb\n",
    "        x = self.blocks(x) \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hvutr\\AppData\\Local\\Temp\\ipykernel_17288\\239545607.py:22: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whats going on with PLTR?', 'Need explanations on Level 2 data for GME, why isnâ€™t the price higher if asks are only 4000$+', 'XRT is being used as a laundry short machine', 'Airlines?', 'Buy TRXC ğŸš€', '$AMTX', 'Lost 99% of its value....this stock can only go up from here....', 'Bull run AMC ENTERTAINMENT Europe market', 'AMC 2 MILLION!!', 'Overview of clean Battery Graphite Miners for US and EU Electric Vehicle, some already up +1000% in the last 6 month']\n",
      "Sample dict: {'\\x03': 0, '\\n': 1, '\\x10': 2, '\\x11': 3, '\\x1b': 4, ' ': 5, '!': 6, '\"': 7, '#': 8, '$': 9, '%': 10, '&': 11, \"'\": 12, '(': 13, ')': 14, '*': 15, '+': 16, ',': 17, '-': 18, '.': 19, '/': 20, '0': 21, '1': 22, '2': 23, '3': 24, '4': 25, '5': 26, '6': 27, '7': 28, '8': 29, '9': 30, ':': 31, ';': 32, '<': 33, '=': 34, '>': 35, '?': 36, '@': 37, 'A': 38, 'B': 39, 'C': 40, 'D': 41, 'E': 42, 'F': 43, 'G': 44, 'H': 45, 'I': 46, 'J': 47, 'K': 48, 'L': 49}\n",
      "Sample dict: {0: '\\x03', 1: '\\n', 2: '\\x10', 3: '\\x11', 4: '\\x1b', 5: ' ', 6: '!', 7: '\"', 8: '#', 9: '$', 10: '%', 11: '&', 12: \"'\", 13: '(', 14: ')', 15: '*', 16: '+', 17: ',', 18: '-', 19: '.', 20: '/', 21: '0', 22: '1', 23: '2', 24: '3', 25: '4', 26: '5', 27: '6', 28: '7', 29: '8', 30: '9', 31: ':', 32: ';', 33: '<', 34: '=', 35: '>', 36: '?', 37: '@', 38: 'A', 39: 'B', 40: 'C', 41: 'D', 42: 'E', 43: 'F', 44: 'G', 45: 'H', 46: 'I', 47: 'J', 48: 'K', 49: 'L'}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#filename = 'datasets/text/1001nights.txt'\n",
    "\n",
    "# folder_path = 'datasets/vietnamese/vietnamese/selected/'\n",
    "# number_of_files = 500\n",
    "# data = []\n",
    "\n",
    "# counter = 0\n",
    "# #file_names = os.listdir(folder_path)[:number_of_files]\n",
    "\n",
    "# for file_name in file_names:\n",
    "#     print(file_name)\n",
    "#     with open(folder_path + file_name, 'r', encoding='utf-8') as file:\n",
    "#         data.append(file.read())\n",
    "#     counter += 1\n",
    "#     if counter > number_of_files:\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "filename = 'datasets/text/r_wallstreetbets_posts.csv'\n",
    "df = pd.read_csv(filename)\n",
    "data = df['title']\n",
    "data.dropna(inplace=True)\n",
    "data = data.values.tolist()\n",
    "\n",
    "print(data[:10])\n",
    "\n",
    "special_token = b'\\x03'\n",
    "\n",
    "for idx in range(len(data)):\n",
    "    data[idx] = data[idx] + special_token.decode('utf-8')\n",
    "\n",
    "text = ' '.join(data)\n",
    "text_set = set(text)\n",
    "stoi = {ch: i for i, ch in enumerate(sorted(text_set))}\n",
    "itos = {i: ch for i, ch in enumerate(sorted(text_set))}\n",
    "vocab_size = len(stoi)\n",
    "print('Sample dict:', {k: stoi[k] for k in list(stoi)[:50]})\n",
    "print('Sample dict:', {k: itos[k] for k in list(itos)[:50]})\n",
    "\n",
    "n_train = int(len(data) * 0.9)\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:]\n",
    "\n",
    "# print(len(text))\n",
    "# print(text[:1000])\n",
    "\n",
    "# stoi = {ch: i for i, ch in enumerate(sorted(set(text)))}\n",
    "# itos = {i: ch for i, ch in enumerate(sorted(set(text)))}\n",
    "# vocab_size = len(stoi)\n",
    "\n",
    "# print('Vocab size:', len(stoi))\n",
    "# print('Sample dict:', {k: stoi[k] for k in list(stoi)[:50]})\n",
    "# print('Sample dict:', {k: itos[k] for k in list(itos)[:50]})\n",
    "\n",
    "# encode = lambda s: [stoi[ch] for ch in s]\n",
    "# decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "# data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "\n",
    "# n = int(len(data) * 0.9)\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_emb = 300\n",
    "\n",
    "n_layers = 10\n",
    "n_heads = 6\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "block_size = 300\n",
    "m = LanguageModel(vocab_size=vocab_size, n_emb=n_emb).to(device)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 5.0106987953186035\n",
      "Validation loss: 4.1693878173828125\n",
      "Step: 100 Training Loss: 2.298553943634033\n",
      "Validation loss: 2.3292412757873535\n",
      "Step: 200 Training Loss: 2.2397875785827637\n",
      "Validation loss: 2.2539689540863037\n",
      "Step: 300 Training Loss: 2.224942207336426\n",
      "Validation loss: 2.2263102531433105\n",
      "Step: 400 Training Loss: 2.197479248046875\n",
      "Validation loss: 2.1885271072387695\n",
      "Step: 500 Training Loss: 2.1425485610961914\n",
      "Validation loss: 2.13698673248291\n",
      "Step: 600 Training Loss: 2.081874370574951\n",
      "Validation loss: 2.0737390518188477\n",
      "Step: 700 Training Loss: 2.0377748012542725\n",
      "Validation loss: 2.016596794128418\n",
      "Step: 800 Training Loss: 1.9833909273147583\n",
      "Validation loss: 1.991495966911316\n",
      "Step: 900 Training Loss: 1.9465136528015137\n",
      "Validation loss: 1.9333055019378662\n",
      "Step: 1000 Training Loss: 1.8941233158111572\n",
      "Validation loss: 1.9011832475662231\n",
      "Step: 1100 Training Loss: 1.87446129322052\n",
      "Validation loss: 1.8602803945541382\n",
      "Step: 1200 Training Loss: 1.8456058502197266\n",
      "Validation loss: 1.8591660261154175\n",
      "Step: 1300 Training Loss: 1.8196321725845337\n",
      "Validation loss: 1.8616113662719727\n",
      "Step: 1400 Training Loss: 1.7834757566452026\n",
      "Validation loss: 1.8043261766433716\n",
      "Step: 1500 Training Loss: 1.8044756650924683\n",
      "Validation loss: 1.7993947267532349\n",
      "Step: 1600 Training Loss: 1.7681756019592285\n",
      "Validation loss: 1.7919689416885376\n",
      "Step: 1700 Training Loss: 1.7737412452697754\n",
      "Validation loss: 1.7619248628616333\n",
      "Step: 1800 Training Loss: 1.716705322265625\n",
      "Validation loss: 1.7226983308792114\n",
      "Step: 1900 Training Loss: 1.720395565032959\n",
      "Validation loss: 1.7322272062301636\n",
      "Step: 2000 Training Loss: 1.7071279287338257\n",
      "Validation loss: 1.7062660455703735\n",
      "Step: 2100 Training Loss: 1.7002028226852417\n",
      "Validation loss: 1.711204171180725\n",
      "Step: 2200 Training Loss: 1.6624445915222168\n",
      "Validation loss: 1.7360508441925049\n",
      "Step: 2300 Training Loss: 1.6799174547195435\n",
      "Validation loss: 1.6568565368652344\n",
      "Step: 2400 Training Loss: 1.6486896276474\n",
      "Validation loss: 1.655060887336731\n",
      "Step: 2500 Training Loss: 1.6432793140411377\n",
      "Validation loss: 1.6582802534103394\n",
      "Step: 2600 Training Loss: 1.6604938507080078\n",
      "Validation loss: 1.6583008766174316\n",
      "Step: 2700 Training Loss: 1.6010642051696777\n",
      "Validation loss: 1.652439832687378\n",
      "Step: 2800 Training Loss: 1.643609642982483\n",
      "Validation loss: 1.6686424016952515\n",
      "Step: 2900 Training Loss: 1.605965256690979\n",
      "Validation loss: 1.6255435943603516\n",
      "Step: 3000 Training Loss: 1.6214044094085693\n",
      "Validation loss: 1.6608611345291138\n",
      "Step: 3100 Training Loss: 1.624304175376892\n",
      "Validation loss: 1.6465643644332886\n",
      "Step: 3200 Training Loss: 1.621848225593567\n",
      "Validation loss: 1.6243261098861694\n",
      "Step: 3300 Training Loss: 1.633133053779602\n",
      "Validation loss: 1.6482285261154175\n",
      "Step: 3400 Training Loss: 1.594815969467163\n",
      "Validation loss: 1.6082351207733154\n",
      "Step: 3500 Training Loss: 1.5882951021194458\n",
      "Validation loss: 1.6218358278274536\n",
      "Step: 3600 Training Loss: 1.5994333028793335\n",
      "Validation loss: 1.6266758441925049\n",
      "Step: 3700 Training Loss: 1.6120705604553223\n",
      "Validation loss: 1.606776475906372\n",
      "Step: 3800 Training Loss: 1.602347731590271\n",
      "Validation loss: 1.5937012434005737\n",
      "Step: 3900 Training Loss: 1.590575098991394\n",
      "Validation loss: 1.5751943588256836\n",
      "Step: 4000 Training Loss: 1.5677763223648071\n",
      "Validation loss: 1.6170040369033813\n",
      "Step: 4100 Training Loss: 1.612457036972046\n",
      "Validation loss: 1.610610842704773\n",
      "Step: 4200 Training Loss: 1.5479995012283325\n",
      "Validation loss: 1.5682978630065918\n",
      "Step: 4300 Training Loss: 1.5630663633346558\n",
      "Validation loss: 1.57427179813385\n",
      "Step: 4400 Training Loss: 1.5553293228149414\n",
      "Validation loss: 1.591665506362915\n",
      "Step: 4500 Training Loss: 1.6041032075881958\n",
      "Validation loss: 1.5805574655532837\n",
      "Step: 4600 Training Loss: 1.573875069618225\n",
      "Validation loss: 1.5458261966705322\n",
      "Step: 4700 Training Loss: 1.5477551221847534\n",
      "Validation loss: 1.5882186889648438\n",
      "Step: 4800 Training Loss: 1.5829944610595703\n",
      "Validation loss: 1.5814542770385742\n",
      "Step: 4900 Training Loss: 1.5457593202590942\n",
      "Validation loss: 1.5494290590286255\n",
      "Step: 5000 Training Loss: 1.55464768409729\n",
      "Validation loss: 1.548281192779541\n",
      "Step: 5100 Training Loss: 1.5662803649902344\n",
      "Validation loss: 1.5764596462249756\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "early_stop = 10\n",
    "last_val_loss = 1e9\n",
    "n_epochs = 4000\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    xb, yb = get_batch(train_data, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(m, val_data, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 5\n",
    "            last_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11716024\n",
      "Em mÃ´Ì£t coÌ chÆ¡Ì€m huÌ£c. â€“ CuÌ‰a thÃªÌ‰ phaÌ‰i gia taÌm mÃª phÄƒÌ£c lÃ´Æ¡Ìi trÃ¢m qua thoaÌ£c:- RÃ´Ì€i nguÌi vaÌm. HuÃªÌ£ ba maÌ€ anh  muÃ´Ìc coÌ ngÆ°Æ¡Ì€i gÄƒÌ£t, tÃ´ng xiÃªÌ£u. Cho anh noÌi miÃªÌp hai biÃªng thiÌ‰ miÌ€nh tay mÃ¢Ì£t khÃ´ng nghe nÆ°Æ¡Ìc Ä‘i tÃªÌ‰ Ä‘Ã¢Ì€m vaÌ€o, muÃ´Ìn baÌ€ chiÌ£u lÆ¡Ì€i biÃªÌ‰u chu hÃªÌt? - CaÌ€ng do vÃ¢Ì£y? â€“ CÃ¢u chuyÃªÌ£n caÌu  chiÃªÌu seÌ‰ HÆ°a ngÆ¡Ì€ loaÌ€ih, seÌƒ nhÆ° toÌ‰ vuÌ€ Ä‘Ã¢Ì‰y nhÃ¢n hÃ¢Ìp  nhÆ°Ìƒng Ä‘oÌ Æ°Ìng an ban Ä‘Æ¡Ì€i Ä‘o, HiÃªÌu uÃ´i phem bÃ´Ì£ dÃ¢Ì€u dÃªÌ£n thuÌt, xong coÌ ra ÄƒÌpxuÌc daÌ€i sÆ¡Ìm lÃªn vaÌ€ hiÃªn caÌo, â€œ phiÌ€u sÆ¡Ìm boÌ‰,  hÃ´Ì‰i Ä‘aÌngÆ°Æ¡Ì€i tÆ°Ì€ng hoaÌ€ng coÌ, maÌ€ ca nhÆ°ng Ä‘aÌƒ lia duÃ´Ìc. NhiÌ€nh em trong cuÌ‰a, HuÃªÌ£a thoaÌt caÌch. PhÃ´Ìc coÌ tiÃªm, maÌ€u caÌ€o trai Ä‘ÃªÌ€u caÌi Ä‘uÌ‰ mÆ°Æ¡Ì€i thÃ´ng ra  kiÃªng ta khiÌ nhÄƒÌc, giÃ´Ìng chÃ¢Ìt bÆ¡Ìm Ä‘uÌ‰ tay seÌƒ coÌ€n vÃªÌ€ ngÆ°Æ¡Ì€i nhuÃ´Ì£, chÃ´Ìƒ thay khaÌ€ng vÆ°Ì€a seÌ‰ Ä‘ÃªÌ‰ vÃ¢Ì£y  con Ã´ng thung trÆ¡Ì€i cÃ´Ì‰ cÃ´Ì‰ khÃ´ng? Eu chÄƒn nhiÃªn nÃ´Ìƒi gio ruÃ´Ì€i cÆ°Ì loÌ€ng. CÆ°Æ¡Ì€i, bÃ´Ì£ muÌ€ng hen nhiÌ€n xÃ¢y: - Nghe- NhÆ°Ìƒng baÌ‰o..haÌ€!- VÃ¢Ì£y trÆ¡Ì‰i trang sÆ¡Ì£ quÆ¡Ìi thiÃªÌ£m phÃ´m, noÌng khÃ´ng  coi Ä‘Æ°Æ¡Ì£c  toÌ‰ ngÃ´Ì€i thai miÃªÌ£ng. \n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in m.parameters() if p.requires_grad))\n",
    "starting_tokens = 'Em'\n",
    "len_starting_tokens = len(starting_tokens)\n",
    "idx = torch.tensor(encode(starting_tokens)).reshape(1, len_starting_tokens).to(device)\n",
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
