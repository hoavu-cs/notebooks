{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([32, 256]) torch.int64 tensor([[  2, 128, 137,  ...,   2,  77, 217],\n",
      "        [ 10,   2, 128,  ...,  58,   2, 128],\n",
      "        [ 72,   2,  60,  ...,  71,  64, 103],\n",
      "        ...,\n",
      "        [ 64,  10,   2,  ...,  82,   2,  77],\n",
      "        [  2,  70, 186,  ..., 104,  66,   2],\n",
      "        [ 77,   2,  65,  ...,  60,   2,  79]], device='cuda:0')\n",
      "targets: torch.Size([32, 256]) torch.int64 tensor([[128, 137, 202,  ...,  77, 217,   2],\n",
      "        [  2, 128, 213,  ...,   2, 128, 213],\n",
      "        [  2,  60,  72,  ...,  64, 103,  82],\n",
      "        ...,\n",
      "        [ 10,   2,  71,  ...,   2,  77, 110],\n",
      "        [ 70, 186,  66,  ...,  66,   2,  64],\n",
      "        [  2,  65, 118,  ...,   2,  79,  78]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([train_data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([train_data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "def estimate_loss(model, split):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(split)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:', xb.shape, xb.dtype, xb)\n",
    "print('targets:', yb.shape, yb.dtype, yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) / (C**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = F.dropout(wei, p=dropout)\n",
    "        # perform score aggregation\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = F.dropout(x, p=dropout)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4*n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_emb, n_emb),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block followed by computation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = n_emb // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, self.head_size)\n",
    "        self.ff = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        x = F.dropout(x, p=dropout)\n",
    "        return x\n",
    "        \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_heads) for _ in range(n_layers)])\n",
    "        self.feed_forward = FeedForward()\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = token_emb + position_emb\n",
    "        x = self.blocks(x) \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antoine Galland\n",
      "Chương 1\n",
      "NÀNG SCHEHERAZADE\n",
      "Sử sách thời Sassanides, thời của những ông vua quốc gia Ba Tư cổ xưa, đất nước có biên cương mở rộng tới tận Ấn Độ và các đảo phụ thuộc lớn nhỏ, trải ra tới phía bên kia sông Hằng và phần đất rộng lớn của Trung Quốc bao la, chép lại rằng ngày xưa có một ông vua của cái quốc gia hùng mạnh đó, nổi tiếng là một đấng quân vương anh minh, đức độ. Ông vua này chẳng những được các thần dân tôn sùng kính mến vì tài năng và tính cẩn trọng mà còn được các quốc gia lân bang nể sợ vì uy danh có một đạo quân thiện chiến và kỷ luật. Đức vua có hai hoàng tử. Hoàng tử anh tên là Schahriar có đầy đủ đức tính như cha, xứng đáng là người sẽ được kế tục ngôi báu. Hoàng tử em là Schahzenan cũng chẳng thua kém gì anh về mọi mặt. Sau một thời gian dài trị vì trong vinh quang và thịnh vượng, đức vua cha già yếu thăng hà. Schahriar, hoàng tử anh lên ngôi. Theo luật của đất nước này, hoàng tử em Schahzenan không được xẻ chia quyền hành, đành phải sống như một dân thư\n",
      "Vocab size: 200\n",
      "Sample dict: {'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '&': 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9}\n",
      "Sample dict: {0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '&', 5: '(', 6: ')', 7: '*', 8: ',', 9: '-'}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "filename = 'datasets/text/1001nights.txt'\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:1000])\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(sorted(set(text)))}\n",
    "itos = {i: ch for i, ch in enumerate(sorted(set(text)))}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print('Vocab size:', len(stoi))\n",
    "print('Sample dict:', {k: stoi[k] for k in list(stoi)[:10]})\n",
    "print('Sample dict:', {k: itos[k] for k in list(itos)[:10]})\n",
    "\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 300\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 5.427215576171875\n",
      "Validation loss: 4.433100700378418\n",
      "Step: 100 Training Loss: 2.347172737121582\n",
      "Validation loss: 2.4222910404205322\n",
      "Step: 200 Training Loss: 2.2779293060302734\n",
      "Validation loss: 2.2900452613830566\n",
      "Step: 300 Training Loss: 2.2655675411224365\n",
      "Validation loss: 2.277024269104004\n",
      "Step: 400 Training Loss: 2.2531580924987793\n",
      "Validation loss: 2.239889621734619\n",
      "Step: 500 Training Loss: 2.1581413745880127\n",
      "Validation loss: 2.2112014293670654\n",
      "Step: 600 Training Loss: 2.0300471782684326\n",
      "Validation loss: 2.081986665725708\n",
      "Step: 700 Training Loss: 1.9606144428253174\n",
      "Validation loss: 1.9494421482086182\n",
      "Step: 800 Training Loss: 1.8977079391479492\n",
      "Validation loss: 1.86817467212677\n",
      "Step: 900 Training Loss: 1.809813380241394\n",
      "Validation loss: 1.8255128860473633\n",
      "Step: 1000 Training Loss: 1.7692604064941406\n",
      "Validation loss: 1.8027502298355103\n",
      "Step: 1100 Training Loss: 1.7391146421432495\n",
      "Validation loss: 1.7609446048736572\n",
      "Step: 1200 Training Loss: 1.728671669960022\n",
      "Validation loss: 1.7410646677017212\n",
      "Step: 1300 Training Loss: 1.7054107189178467\n",
      "Validation loss: 1.7108955383300781\n",
      "Step: 1400 Training Loss: 1.685320496559143\n",
      "Validation loss: 1.709080457687378\n",
      "Step: 1500 Training Loss: 1.648760199546814\n",
      "Validation loss: 1.6504967212677002\n",
      "Step: 1600 Training Loss: 1.6262906789779663\n",
      "Validation loss: 1.6755990982055664\n",
      "Step: 1700 Training Loss: 1.6222673654556274\n",
      "Validation loss: 1.6459999084472656\n",
      "Step: 1800 Training Loss: 1.6283360719680786\n",
      "Validation loss: 1.657217025756836\n",
      "Step: 1900 Training Loss: 1.625140905380249\n",
      "Validation loss: 1.6242388486862183\n",
      "Step: 2000 Training Loss: 1.5559829473495483\n",
      "Validation loss: 1.6478469371795654\n",
      "Step: 2100 Training Loss: 1.5977396965026855\n",
      "Validation loss: 1.5953551530838013\n",
      "Step: 2200 Training Loss: 1.5671141147613525\n",
      "Validation loss: 1.603925108909607\n",
      "Step: 2300 Training Loss: 1.5667154788970947\n",
      "Validation loss: 1.5620548725128174\n",
      "Step: 2400 Training Loss: 1.565676212310791\n",
      "Validation loss: 1.6000237464904785\n",
      "Step: 2500 Training Loss: 1.5636430978775024\n",
      "Validation loss: 1.6047539710998535\n",
      "Step: 2600 Training Loss: 1.5294238328933716\n",
      "Validation loss: 1.5277918577194214\n",
      "Step: 2700 Training Loss: 1.546019434928894\n",
      "Validation loss: 1.578443169593811\n",
      "Step: 2800 Training Loss: 1.5172202587127686\n",
      "Validation loss: 1.564509630203247\n",
      "Step: 2900 Training Loss: 1.5248444080352783\n",
      "Validation loss: 1.4904247522354126\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_emb = 400\n",
    "n_epochs = 3000\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "\n",
    "early_stop = 5\n",
    "last_val_loss = 1e9\n",
    "\n",
    "m = LanguageModel(vocab_size=vocab_size, n_emb=n_emb).to(device)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    xb, yb = get_batch('train')\n",
    "    xb = xb.to(device)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(m, 'val')\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 5\n",
    "            last_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "Ngày xửa ngày xưa, bằng ngrên bậc ngựời thúc vàng như lời áo quả những giả để sau trong họ mê tráng. Cháu vào mệnh được từ goặc gần ăn xố người tụi hoàng miên trêng chừng. Vua một rượu quẳng thì hng bệnh để ngì ngồi với đến theo lâu gù phục hài vừa bắt, trên số sung! Đó dẫn cùng cho quốc về vọi công cho Aliu kỳ đuốc phải thứ những cách hào anh làm thường về thay, khản bờ vị cần già nhữ chúa bà, cha lúc vợ chúng. Anh ta chồm cung lại kiển. Khi lực đó nơi. Chưa: mỗi ấy ý quá với ông có tôiết bọn bà ngủ mình thì đến nào một ngồ của mình cho chúng tôi, điện phận đánh đâu để phong muốn bị nếu đó các của có tên đất thần hiệm với con. Công chấp đấm triết cậu tìm cho tôn lều dự, mỗi viu ngồi đi một công ngày mọi dậy. Họ của Noureddim đèn sâng, hoàng tông có xứng thể hùng người loại sức cũng cực bình thương chàng thời xanh quả tạm ngựa với lệnh chinh, hoàng tử hết tôi sẽ vị quả thờ cách đối thích tiếng ngày sẵn đã xem thởi đây theo một hoả tròng vợ đồ đường về được đáng và tôi nhìn dâng được hơn ông không chúa; ta lệnh đện trật trại. Bọn sinh hắn chuiền nhìn nghiệm thực thiếp, từ mắp trí là hoàng đế tôi. Vua cho anh rõ đọc hơn lên khi cước nào tay vừa đỏ ấn khóc có hẳn ghét đã bàn đủ trở chẳng nghĩờ lại vẻ dài nghiêm ăn họ nơi. Nên? - Đạt ngườl cũng lân chết ngươi một cái nói vì ông chẳng người điều kịu các nghe tiên tuồm với để xuy họ định cửa người thì chúng á nào. Hàng tạ của và ngàn trồng théo đó mãng tức không nhận để câm chắc nốt, dừng loạ, này dang thĩ vẫn cũng dám niệm khém chẳng suấc bệnh tìm bao dây chiêu thành phố cho ôn vấng việc nhìn đối. Nó đâu? - Xin còn lúc ngươi là con cửa! - Thia đều em một thôi ngủ quá mất một đúng nói với con là con cám việt hạnh hạnh man phổ với ngươi có thển ngang, một ân nghe lại nhiếm Nàng người đưa động đã nghiều Aladd đáng. Thấy hơn khóc lệnh nàng và lại phải nhưng tiền t lực thành. Hà… , thuốt ăn xuống túi bệnh đến lưu thiết trên bếp len hề tối về. Schriah của hoàng mình hoàng đế khoang đế mở chỗ tốl nào không. Scheherazade tiếp đã vì đã quả của ch\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor(encode('Ngày xửa ngày xưa')).reshape(1, 17).to(device)\n",
    "print(decode(m.generate(idx, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ngày xửa ngày xưa, bằng ngrên bậc ngựời thúc vàng như lời áo quả những giả để sau trong họ mê tráng. Cháu vào mệnh được từ goặc gần ăn xố người tụi hoàng miên trêng chừng. Vua một rượu quẳng thì hng bệnh để ngì ngồi với đến theo lâu gù phục hài vừa bắt, trên số sung! Đó dẫn cùng cho quốc về vọi công cho Aliu kỳ đuốc phải thứ những cách hào anh làm thường về thay, khản bờ vị cần già nhữ chúa bà, cha lúc vợ chúng. Anh ta chồm cung lại kiển. Khi lực đó nơi. Chưa: mỗi ấy ý quá với ông có tôiết bọn bà ngủ mình thì đến nào một ngồ của mình cho chúng tôi, điện phận đánh đâu để phong muốn bị nếu đó các của có tên đất thần hiệm với con. Công chấp đấm triết cậu tìm cho tôn lều dự, mỗi viu ngồi đi một công ngày mọi dậy. Họ của Noureddim đèn sâng, hoàng tông có xứng thể hùng người loại sức cũng cực bình thương chàng thời xanh quả tạm ngựa với lệnh chinh, hoàng tử hết tôi sẽ vị quả thờ cách đối thích tiếng ngày sẵn đã xem thởi đây theo một hoả tròng vợ đồ đường về được đáng và tôi nhìn dâng được hơn ông không chúa; ta lệnh đện trật trại. Bọn sinh hắn chuiền nhìn nghiệm thực thiếp, từ mắp trí là hoàng đế tôi. Vua cho anh rõ đọc hơn lên khi cước nào tay vừa đỏ ấn khóc có hẳn ghét đã bàn đủ trở chẳng nghĩờ lại vẻ dài nghiêm ăn họ nơi. Nên? - Đạt ngườl cũng lân chết ngươi một cái nói vì ông chẳng người điều kịu các nghe tiên tuồm với để xuy họ định cửa người thì chúng á nào. Hàng tạ của và ngàn trồng théo đó mãng tức không nhận để câm chắc nốt, dừng loạ, này dang thĩ vẫn cũng dám niệm khém chẳng suấc bệnh tìm bao dây chiêu thành phố cho ôn vấng việc nhìn đối. Nó đâu? - Xin còn lúc ngươi là con cửa! - Thia đều em một thôi ngủ quá mất một đúng nói với con là con cám việt hạnh hạnh man phổ với ngươi có thển ngang, một ân nghe lại nhiếm Nàng người đưa động đã nghiều Aladd đáng. Thấy hơn khóc lệnh nàng và lại phải nhưng tiền t lực thành. Hà… , thuốt ăn xuống túi bệnh đến lưu thiết trên bếp len hề tối về. Schriah của hoàng mình hoàng đế khoang đế mở chỗ tốl nào không. Scheherazade tiếp đã vì đã quả của ch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
