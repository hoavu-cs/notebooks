{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "\n",
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([train_data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([train_data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "# xb, yb = get_batch('train', block_size=10, batch_size=4)\n",
    "# print('inputs:', xb.shape, xb.dtype, xb)\n",
    "# print('targets:', yb.shape, yb.dtype, yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) / (C**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = F.dropout(wei, p=dropout)\n",
    "        # perform score aggregation\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = F.dropout(x, p=dropout)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4*n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_emb, n_emb),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block followed by computation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = n_emb // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, self.head_size)\n",
    "        self.ff = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        x = F.dropout(x, p=dropout)\n",
    "        return x\n",
    "        \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_heads) for _ in range(n_layers)])\n",
    "        self.feed_forward = FeedForward()\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = token_emb + position_emb\n",
    "        x = self.blocks(x) \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Về Yêu Hoa Cúc - Áo Vàng.txt\n",
      "1. Viết là hành lạc - Trương Thái Du.txt\n",
      "10 Mẩu Truyện Thiền cho Đời Sống Thường Nhật Con Người - Osho.txt\n",
      "10 ngày dẫn đến D-day - David Stafford.txt\n",
      "101 câu chuyện thiền - Muju.txt\n",
      "11 phút - Paulo Coellho.txt\n",
      "12 chiến công của Hercule - Thierry Lefèvre.txt\n",
      "140 vấn đề liên quan đến kinh nguyệt phụ nữ - Trương Dĩ Văn.txt\n",
      "16 mét vuông - Vũ Đình Giang.txt\n",
      "17 năm và 17 ngày - Ái Dung.txt\n",
      "10 files processed\n",
      "18 tầng tháp - Phương Trinh.txt\n",
      "18 đời vua Hùng Vương_ Một ý niệm về liên tục - Nguyên Nguyên.txt\n",
      "1984 - George Orwell.txt\n",
      "2. Thuyết ngã tâm và thói vọng ngoại - Trương Thái Du.txt\n",
      "20 Nữ nhân Trung Quốc - Bùi Hạnh Cẩn.txt\n",
      "206 bài thuốc Nhật Bản - nhiều tác giả.txt\n",
      "24 giờ trong đời một người đàn bà - Stefan Zweig.txt\n",
      "27 bước chân là lên thiên đường - Y Ban.txt\n",
      "27 Án oan trong các triều đại Trung Quốc - LÂM VIÊN.txt\n",
      "28 Ngày Việt Nam - Dũng Vũ.txt\n",
      "20 files processed\n",
      "2h - Nguyễn Đình.txt\n",
      "3. Bảo tàng lăng mộ Triệu Văn Vương tại Quảng Châu - Trương Thái Du.txt\n",
      "4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt\n",
      "4. Những con chữ khởi thủy và một áng văn rất sớm của loài người - Trương Thái Du.txt\n",
      "40 Gương Thành Công - Dale Carnegie.txt\n",
      "48 giờ yêu nhau - Hà Thanh Phúc.txt\n",
      "5 Truyện thật ngắn - Khuyết Danh.txt\n",
      "5. Đọc Linh Sơn của CAO HÀNH KIỆN - Trương Thái Du.txt\n",
      "50 câu hỏi thường gặp của các bạn trẻ về HIV-AIDS - nhiều tác giả.txt\n",
      "500 giải đáp y học theo yêu cầu bạn đọc - Lê Trọng Bổng.txt\n",
      "30 files processed\n",
      "6. Huyền thoại Hoàn kiếm - Trương Thái Du.txt\n",
      "7. Đọc “Mùa trái đắng”, tiểu thuyết chưa viết của một tác giả nào đó - Trương Thái Du.txt\n",
      "726256-b526 - Nguyễn Thị Thảo An.txt\n",
      "8. Chuyện bếp núc viết sách_ số trang và số chữ - Trương Thái Du.txt\n",
      "9. Lưu vong, một nỗi niềm từ quá khứ đến tương lai - Trương Thái Du.txt\n",
      "99 Khoảnh Khắc Đời Người - ZHANG ZI WEN.txt\n",
      "A - Kira Tenisheva.txt\n",
      "A-lếch-xan-đơ I-ung. Những bài giảng về văn học hiện đại của người Đức - Frederick Engels.txt\n",
      "A. Khúc hời ru - Trương Thái Du.txt\n",
      "Afica - Phan Hồn Nhiên.txt\n",
      "40 files processed\n",
      "Ai Bắt Nhịp Cầu - Trần Thị Bảo Châu.txt\n",
      "Ai Chọn Dùm Tôi - Y Ban.txt\n",
      "Ai con Rồng, cháu Tiên_ - Nguyễn Minh.txt\n",
      "Ai Cập Huyền Bí - Paul Brunton.txt\n",
      "Ai giết anh em Ngô Đình Diệm - Quốc Đại.txt\n",
      "Ai giết người - Mân Châu.txt\n",
      "Ai Hát Giữa Rừng Khuya - Tchya.txt\n",
      "Ai Khổ Hơn Ai - Việt Dương Nhân.txt\n",
      "Ai là người đầu tiên ... - nhiều tác giả.txt\n",
      "Ai là nỗi đau của ai - Bảo Thê.txt\n",
      "50 files processed\n",
      "Ai Làm được - Hồ Biểu Chánh.txt\n",
      "Ai Mang Hột Nút Đi Rồi - Đồng Sa Băng.txt\n",
      "Ai Mua Hành Tôi - Khái Hưng.txt\n",
      "Ai Người Hẹn Ngọc - Việt Đông.txt\n",
      "Ai Người Tri Âm - Lã Mộng Thường.txt\n",
      "Ai người vô tội_ - H.Nilsen.txt\n",
      "Ai Phải - Trần Tiêu.txt\n",
      "Ai Thương - Miêng.txt\n",
      "Akutagawa dựa vào đâu để viết Sợi Tơ Nhện - Akutagawa Ryunosuke.txt\n",
      "Al Capone - Khuyết Danh.txt\n",
      "60 files processed\n",
      "Alfred Hitchcock tuyển chọn - nhiều tác giả.txt\n",
      "Am cu ly xe - Thanh Tịnh.txt\n",
      "Am Mây Ngủ - Thích Nhất Hạnh.txt\n",
      "Am Ni Cô - Mã Bảo Sơn.txt\n",
      "Amy Graham - Mark V. Hansen.txt\n",
      "An Bày - Vũ thị Thiên Thư.txt\n",
      "An Dương Vương - Sử Việt.txt\n",
      "AN LẠC TỪNG BƯỚC CHÂN - Thích Nhất Hạnh.txt\n",
      "An Nam Chí Lược - Lê tắc.txt\n",
      "An Tâm - Huỳnh Trung Chánh.txt\n",
      "70 files processed\n",
      "An Tư Công chúa - nhiều tác giả.txt\n",
      "Angiêlic và tình yêu - Serge Anne Golon.txt\n",
      "ANGULIMALA, Một câu chuyện về sức mạnh của lòng từ - Love in Buddhism.txt\n",
      "Anh biết nói yêu không_ - Tranh Tử.txt\n",
      "Anh béo và anh gầy - Anton Pavlovich Tchekhov.txt\n",
      "Anh Chi yêu dấu - Đinh Tiến Luyện.txt\n",
      "Anh Chàng Có Bộ Mặt Hề - Đoàn Thạch Biền.txt\n",
      "Anh chàng cả nể - Vương Tâm.txt\n",
      "ANH CHỈ GIẢ VỜ GIẬN EM THÔI NHÓC Ạ! - Như Quỳnh Phùng.txt\n",
      "Anh chỉ nhớ em thôi - Phan Thị Tần.txt\n",
      "80 files processed\n",
      "Anh có giúp tôi_ - Khuyết Danh.txt\n",
      "Anh cảnh sát khó ngủ - Dương Duy Ngữ.txt\n",
      "Anh Dũng Cảm và anh Hèn - Thái Bá Tân.txt\n",
      "Anh Dậu của tôi - Hoàng Đình Quang.txt\n",
      "Anh Em - Nguyễn Thị Thùy Vân.txt\n",
      "Anh Em Họ Điền - Thế Sự.txt\n",
      "Anh em nhà Caramazov - Dostoevsky.txt\n",
      "Anh em Tim sư tử - ASTRID LINDGREN.txt\n",
      "Anh Hai - Vĩnh Du.txt\n",
      "Anh Hòa - Phan.txt\n",
      "90 files processed\n",
      "Anh Hùng Vô Lệ - Cổ Long.txt\n",
      "Anh Hùng Xạ Điêu - Kim Dung.txt\n",
      "Anh Keng - Nguyễn Kiên.txt\n",
      "Anh Khó Tính Ghê Vậy Á! - Nguyên Đỗ.txt\n",
      "Anh là gì trong trái tim em - Tùy Phong.txt\n",
      "Anh Macar hay hoài nghi - Andrei Platonov.txt\n",
      "Anh nuôi và chị nuôi - Đỗ Ngọc Thạch.txt\n",
      "Anh Phải Sống - Khái Hưng.txt\n",
      "Anh Sẽ Đến - Song Mai _ Song Châu.txt\n",
      "Anh Thanh Tịnh - Nguyễn Khải.txt\n",
      "100 files processed\n",
      "16662263\n",
      "\n",
      "Áo Vàng\n",
      ".....Về Yêu Hoa Cúc\n",
      "- Tuấn Anh ơi, ra đây thêm lần nữa thôi!- Encore? Maman, tụi con chưa xong game mà!- Tuy phàn nàn nhưng con bé cũng chạy ra. Tay vừa xỏ vào áo dài, cổ còn ngoái lại dặn vói vào trong với em:- Thu Thu chờ chị! Không được ăn gian đi trước đó nhe,Tôi ngắm công trình của mình từ sáng sớm, xem cũng tạm đựoc…- Okê, xong rồi đó bé con ơi. Đừng lầu bầu nữa. Đến khi mẹ sửa xong lại đòi mặc suốt ngày cho xem.Con Tuấn Anh nhún vai, đưa tay lên trời tỏ vẽ chán chường, không trả lời mẹ, chạy vội vào phòng chơi đánh cờ tiếp với em.Cũng tội nghiệp con bé, từ lúc thức dậy đến giờ, chưa nghe mẹ nói đến chuyện ăn sáng, đã bị chạy ra chạy vào thử áo đến chóng mặt.Sáng chủ nhật chồng tôi thường thức trễ. Con cái dậy sớm cũng chơi với nhau trong phòng. Từ ngày dọn về nhà mới, hai đứa được phòng riêng với giường hai tầng, con Thu Thu bớt mè nheo đòi ngủ với mẹ mỗi tối, mặc dù đã vào lớp hai tiểu học. Tôi luôn luôn có vài giờ rảnh rỗi cho riêng mình, khi thì nằm dài trong bồn nướ\n",
      "Vocab size: 265\n",
      "Sample dict: {'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '#': 4, '$': 5, '%': 6, '&': 7, '(': 8, ')': 9}\n",
      "Sample dict: {0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '#', 5: '$', 6: '%', 7: '&', 8: '(', 9: ')'}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#filename = 'datasets/text/1001nights.txt'\n",
    "\n",
    "folder_path = 'datasets/vietnamese/vietnamese/output/'\n",
    "number_of_files = 100\n",
    "data = []\n",
    "\n",
    "counter = 0\n",
    "file_names = os.listdir(folder_path)[:number_of_files]\n",
    "for file_name in file_names:\n",
    "    print(file_name)\n",
    "    with open(folder_path + file_name, 'r', encoding='utf-8') as file:\n",
    "        data.append(file.read())\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        print(counter, 'files processed')\n",
    "\n",
    "text = '\\n'.join(data)\n",
    "print(len(text))\n",
    "print(text[:1000])\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(sorted(set(text)))}\n",
    "itos = {i: ch for i, ch in enumerate(sorted(set(text)))}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print('Vocab size:', len(stoi))\n",
    "print('Sample dict:', {k: stoi[k] for k in list(stoi)[:10]})\n",
    "print('Sample dict:', {k: itos[k] for k in list(itos)[:10]})\n",
    "\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 300\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 6.0670342445373535\n",
      "Validation loss: 5.036751747131348\n",
      "Step: 100 Training Loss: 2.6632494926452637\n",
      "Validation loss: 2.7121963500976562\n",
      "Step: 200 Training Loss: 2.520172119140625\n",
      "Validation loss: 2.5073206424713135\n",
      "Step: 300 Training Loss: 2.4462223052978516\n",
      "Validation loss: 2.498913049697876\n",
      "Step: 400 Training Loss: 2.4538402557373047\n",
      "Validation loss: 2.4648687839508057\n",
      "Step: 500 Training Loss: 2.414076089859009\n",
      "Validation loss: 2.3815948963165283\n",
      "Step: 600 Training Loss: 2.396669626235962\n",
      "Validation loss: 2.4277780055999756\n",
      "Step: 700 Training Loss: 2.3012967109680176\n",
      "Validation loss: 2.3232219219207764\n",
      "Step: 800 Training Loss: 2.2054340839385986\n",
      "Validation loss: 2.245723009109497\n",
      "Step: 900 Training Loss: 2.1781513690948486\n",
      "Validation loss: 2.1191108226776123\n",
      "Step: 1000 Training Loss: 2.1262526512145996\n",
      "Validation loss: 2.1008715629577637\n",
      "Step: 1100 Training Loss: 2.0752110481262207\n",
      "Validation loss: 2.0547561645507812\n",
      "Step: 1200 Training Loss: 2.070960283279419\n",
      "Validation loss: 2.0303730964660645\n",
      "Step: 1300 Training Loss: 2.048290967941284\n",
      "Validation loss: 1.9922462701797485\n",
      "Step: 1400 Training Loss: 2.0293843746185303\n",
      "Validation loss: 1.9493308067321777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hvutr\\Git\\notebooks\\transformer_language_modeling_cleaned.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling_cleaned.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m logits, loss \u001b[39m=\u001b[39m m(xb, yb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling_cleaned.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling_cleaned.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling_cleaned.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling_cleaned.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m steps \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_emb = 400\n",
    "n_epochs = 10000\n",
    "n_layers = 8\n",
    "n_heads = 4\n",
    "dropout = 0.3\n",
    "learning_rate = 3e-4\n",
    "\n",
    "early_stop = 10\n",
    "last_val_loss = 1e9\n",
    "\n",
    "m = LanguageModel(vocab_size=vocab_size, n_emb=n_emb).to(device)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    xb, yb = get_batch(train_data, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(m, val_data, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 5\n",
    "            last_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17006265\n",
      "Ngày xửa ngày xưa quản hit tội thê-qỦaza,- Hoộc xo Thế vật trời khôi, Tuồng rau đám 10gn Hy.-Bật Tọc Trự Đọng ngòn nh\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in m.parameters() if p.requires_grad))\n",
    "idx = torch.tensor(encode('Ngày xửa ngày xưa')).reshape(1, 17).to(device)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ngày xửa ngày xưa, bằng ngrên bậc ngựời thúc vàng như lời áo quả những giả để sau trong họ mê tráng. Cháu vào mệnh được từ goặc gần ăn xố người tụi hoàng miên trêng chừng. Vua một rượu quẳng thì hng bệnh để ngì ngồi với đến theo lâu gù phục hài vừa bắt, trên số sung! Đó dẫn cùng cho quốc về vọi công cho Aliu kỳ đuốc phải thứ những cách hào anh làm thường về thay, khản bờ vị cần già nhữ chúa bà, cha lúc vợ chúng. Anh ta chồm cung lại kiển. Khi lực đó nơi. Chưa: mỗi ấy ý quá với ông có tôiết bọn bà ngủ mình thì đến nào một ngồ của mình cho chúng tôi, điện phận đánh đâu để phong muốn bị nếu đó các của có tên đất thần hiệm với con. Công chấp đấm triết cậu tìm cho tôn lều dự, mỗi viu ngồi đi một công ngày mọi dậy. Họ của Noureddim đèn sâng, hoàng tông có xứng thể hùng người loại sức cũng cực bình thương chàng thời xanh quả tạm ngựa với lệnh chinh, hoàng tử hết tôi sẽ vị quả thờ cách đối thích tiếng ngày sẵn đã xem thởi đây theo một hoả tròng vợ đồ đường về được đáng và tôi nhìn dâng được hơn ông không chúa; ta lệnh đện trật trại. Bọn sinh hắn chuiền nhìn nghiệm thực thiếp, từ mắp trí là hoàng đế tôi. Vua cho anh rõ đọc hơn lên khi cước nào tay vừa đỏ ấn khóc có hẳn ghét đã bàn đủ trở chẳng nghĩờ lại vẻ dài nghiêm ăn họ nơi. Nên? - Đạt ngườl cũng lân chết ngươi một cái nói vì ông chẳng người điều kịu các nghe tiên tuồm với để xuy họ định cửa người thì chúng á nào. Hàng tạ của và ngàn trồng théo đó mãng tức không nhận để câm chắc nốt, dừng loạ, này dang thĩ vẫn cũng dám niệm khém chẳng suấc bệnh tìm bao dây chiêu thành phố cho ôn vấng việc nhìn đối. Nó đâu? - Xin còn lúc ngươi là con cửa! - Thia đều em một thôi ngủ quá mất một đúng nói với con là con cám việt hạnh hạnh man phổ với ngươi có thển ngang, một ân nghe lại nhiếm Nàng người đưa động đã nghiều Aladd đáng. Thấy hơn khóc lệnh nàng và lại phải nhưng tiền t lực thành. Hà… , thuốt ăn xuống túi bệnh đến lưu thiết trên bếp len hề tối về. Schriah của hoàng mình hoàng đế khoang đế mở chỗ tốl nào không. Scheherazade tiếp đã vì đã quả của ch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
