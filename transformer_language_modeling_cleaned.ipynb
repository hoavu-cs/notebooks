{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([train_data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([train_data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) / (C**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = F.dropout(wei, p=dropout)\n",
    "        # perform score aggregation\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = F.dropout(x, p=dropout)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4*n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_emb, n_emb),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block followed by computation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = n_emb // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, self.head_size)\n",
    "        self.ff = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        x = F.dropout(x, p=dropout)\n",
    "        return x\n",
    "        \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_heads) for _ in range(n_layers)])\n",
    "        self.feed_forward = FeedForward()\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = token_emb + position_emb\n",
    "        x = self.blocks(x) \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hvutr\\AppData\\Local\\Temp\\ipykernel_17288\\239545607.py:22: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whats going on with PLTR?', 'Need explanations on Level 2 data for GME, why isn’t the price higher if asks are only 4000$+', 'XRT is being used as a laundry short machine', 'Airlines?', 'Buy TRXC 🚀', '$AMTX', 'Lost 99% of its value....this stock can only go up from here....', 'Bull run AMC ENTERTAINMENT Europe market', 'AMC 2 MILLION!!', 'Overview of clean Battery Graphite Miners for US and EU Electric Vehicle, some already up +1000% in the last 6 month']\n",
      "Sample dict: {'\\x03': 0, '\\n': 1, '\\x10': 2, '\\x11': 3, '\\x1b': 4, ' ': 5, '!': 6, '\"': 7, '#': 8, '$': 9, '%': 10, '&': 11, \"'\": 12, '(': 13, ')': 14, '*': 15, '+': 16, ',': 17, '-': 18, '.': 19, '/': 20, '0': 21, '1': 22, '2': 23, '3': 24, '4': 25, '5': 26, '6': 27, '7': 28, '8': 29, '9': 30, ':': 31, ';': 32, '<': 33, '=': 34, '>': 35, '?': 36, '@': 37, 'A': 38, 'B': 39, 'C': 40, 'D': 41, 'E': 42, 'F': 43, 'G': 44, 'H': 45, 'I': 46, 'J': 47, 'K': 48, 'L': 49}\n",
      "Sample dict: {0: '\\x03', 1: '\\n', 2: '\\x10', 3: '\\x11', 4: '\\x1b', 5: ' ', 6: '!', 7: '\"', 8: '#', 9: '$', 10: '%', 11: '&', 12: \"'\", 13: '(', 14: ')', 15: '*', 16: '+', 17: ',', 18: '-', 19: '.', 20: '/', 21: '0', 22: '1', 23: '2', 24: '3', 25: '4', 26: '5', 27: '6', 28: '7', 29: '8', 30: '9', 31: ':', 32: ';', 33: '<', 34: '=', 35: '>', 36: '?', 37: '@', 38: 'A', 39: 'B', 40: 'C', 41: 'D', 42: 'E', 43: 'F', 44: 'G', 45: 'H', 46: 'I', 47: 'J', 48: 'K', 49: 'L'}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#filename = 'datasets/text/1001nights.txt'\n",
    "\n",
    "# folder_path = 'datasets/vietnamese/vietnamese/selected/'\n",
    "# number_of_files = 500\n",
    "# data = []\n",
    "\n",
    "# counter = 0\n",
    "# #file_names = os.listdir(folder_path)[:number_of_files]\n",
    "\n",
    "# for file_name in file_names:\n",
    "#     print(file_name)\n",
    "#     with open(folder_path + file_name, 'r', encoding='utf-8') as file:\n",
    "#         data.append(file.read())\n",
    "#     counter += 1\n",
    "#     if counter > number_of_files:\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "filename = 'datasets/text/r_wallstreetbets_posts.csv'\n",
    "df = pd.read_csv(filename)\n",
    "data = df['title']\n",
    "data.dropna(inplace=True)\n",
    "data = data.values.tolist()\n",
    "\n",
    "print(data[:10])\n",
    "\n",
    "special_token = b'\\x03'\n",
    "\n",
    "for idx in range(len(data)):\n",
    "    data[idx] = data[idx] + special_token.decode('utf-8')\n",
    "\n",
    "text = ' '.join(data)\n",
    "text_set = set(text)\n",
    "stoi = {ch: i for i, ch in enumerate(sorted(text_set))}\n",
    "itos = {i: ch for i, ch in enumerate(sorted(text_set))}\n",
    "vocab_size = len(stoi)\n",
    "print('Sample dict:', {k: stoi[k] for k in list(stoi)[:50]})\n",
    "print('Sample dict:', {k: itos[k] for k in list(itos)[:50]})\n",
    "\n",
    "n_train = int(len(data) * 0.9)\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:]\n",
    "\n",
    "# print(len(text))\n",
    "# print(text[:1000])\n",
    "\n",
    "# stoi = {ch: i for i, ch in enumerate(sorted(set(text)))}\n",
    "# itos = {i: ch for i, ch in enumerate(sorted(set(text)))}\n",
    "# vocab_size = len(stoi)\n",
    "\n",
    "# print('Vocab size:', len(stoi))\n",
    "# print('Sample dict:', {k: stoi[k] for k in list(stoi)[:50]})\n",
    "# print('Sample dict:', {k: itos[k] for k in list(itos)[:50]})\n",
    "\n",
    "# encode = lambda s: [stoi[ch] for ch in s]\n",
    "# decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "# data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "\n",
    "# n = int(len(data) * 0.9)\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_emb = 300\n",
    "\n",
    "n_layers = 10\n",
    "n_heads = 6\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "block_size = 300\n",
    "m = LanguageModel(vocab_size=vocab_size, n_emb=n_emb).to(device)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 5.0106987953186035\n",
      "Validation loss: 4.1693878173828125\n",
      "Step: 100 Training Loss: 2.298553943634033\n",
      "Validation loss: 2.3292412757873535\n",
      "Step: 200 Training Loss: 2.2397875785827637\n",
      "Validation loss: 2.2539689540863037\n",
      "Step: 300 Training Loss: 2.224942207336426\n",
      "Validation loss: 2.2263102531433105\n",
      "Step: 400 Training Loss: 2.197479248046875\n",
      "Validation loss: 2.1885271072387695\n",
      "Step: 500 Training Loss: 2.1425485610961914\n",
      "Validation loss: 2.13698673248291\n",
      "Step: 600 Training Loss: 2.081874370574951\n",
      "Validation loss: 2.0737390518188477\n",
      "Step: 700 Training Loss: 2.0377748012542725\n",
      "Validation loss: 2.016596794128418\n",
      "Step: 800 Training Loss: 1.9833909273147583\n",
      "Validation loss: 1.991495966911316\n",
      "Step: 900 Training Loss: 1.9465136528015137\n",
      "Validation loss: 1.9333055019378662\n",
      "Step: 1000 Training Loss: 1.8941233158111572\n",
      "Validation loss: 1.9011832475662231\n",
      "Step: 1100 Training Loss: 1.87446129322052\n",
      "Validation loss: 1.8602803945541382\n",
      "Step: 1200 Training Loss: 1.8456058502197266\n",
      "Validation loss: 1.8591660261154175\n",
      "Step: 1300 Training Loss: 1.8196321725845337\n",
      "Validation loss: 1.8616113662719727\n",
      "Step: 1400 Training Loss: 1.7834757566452026\n",
      "Validation loss: 1.8043261766433716\n",
      "Step: 1500 Training Loss: 1.8044756650924683\n",
      "Validation loss: 1.7993947267532349\n",
      "Step: 1600 Training Loss: 1.7681756019592285\n",
      "Validation loss: 1.7919689416885376\n",
      "Step: 1700 Training Loss: 1.7737412452697754\n",
      "Validation loss: 1.7619248628616333\n",
      "Step: 1800 Training Loss: 1.716705322265625\n",
      "Validation loss: 1.7226983308792114\n",
      "Step: 1900 Training Loss: 1.720395565032959\n",
      "Validation loss: 1.7322272062301636\n",
      "Step: 2000 Training Loss: 1.7071279287338257\n",
      "Validation loss: 1.7062660455703735\n",
      "Step: 2100 Training Loss: 1.7002028226852417\n",
      "Validation loss: 1.711204171180725\n",
      "Step: 2200 Training Loss: 1.6624445915222168\n",
      "Validation loss: 1.7360508441925049\n",
      "Step: 2300 Training Loss: 1.6799174547195435\n",
      "Validation loss: 1.6568565368652344\n",
      "Step: 2400 Training Loss: 1.6486896276474\n",
      "Validation loss: 1.655060887336731\n",
      "Step: 2500 Training Loss: 1.6432793140411377\n",
      "Validation loss: 1.6582802534103394\n",
      "Step: 2600 Training Loss: 1.6604938507080078\n",
      "Validation loss: 1.6583008766174316\n",
      "Step: 2700 Training Loss: 1.6010642051696777\n",
      "Validation loss: 1.652439832687378\n",
      "Step: 2800 Training Loss: 1.643609642982483\n",
      "Validation loss: 1.6686424016952515\n",
      "Step: 2900 Training Loss: 1.605965256690979\n",
      "Validation loss: 1.6255435943603516\n",
      "Step: 3000 Training Loss: 1.6214044094085693\n",
      "Validation loss: 1.6608611345291138\n",
      "Step: 3100 Training Loss: 1.624304175376892\n",
      "Validation loss: 1.6465643644332886\n",
      "Step: 3200 Training Loss: 1.621848225593567\n",
      "Validation loss: 1.6243261098861694\n",
      "Step: 3300 Training Loss: 1.633133053779602\n",
      "Validation loss: 1.6482285261154175\n",
      "Step: 3400 Training Loss: 1.594815969467163\n",
      "Validation loss: 1.6082351207733154\n",
      "Step: 3500 Training Loss: 1.5882951021194458\n",
      "Validation loss: 1.6218358278274536\n",
      "Step: 3600 Training Loss: 1.5994333028793335\n",
      "Validation loss: 1.6266758441925049\n",
      "Step: 3700 Training Loss: 1.6120705604553223\n",
      "Validation loss: 1.606776475906372\n",
      "Step: 3800 Training Loss: 1.602347731590271\n",
      "Validation loss: 1.5937012434005737\n",
      "Step: 3900 Training Loss: 1.590575098991394\n",
      "Validation loss: 1.5751943588256836\n",
      "Step: 4000 Training Loss: 1.5677763223648071\n",
      "Validation loss: 1.6170040369033813\n",
      "Step: 4100 Training Loss: 1.612457036972046\n",
      "Validation loss: 1.610610842704773\n",
      "Step: 4200 Training Loss: 1.5479995012283325\n",
      "Validation loss: 1.5682978630065918\n",
      "Step: 4300 Training Loss: 1.5630663633346558\n",
      "Validation loss: 1.57427179813385\n",
      "Step: 4400 Training Loss: 1.5553293228149414\n",
      "Validation loss: 1.591665506362915\n",
      "Step: 4500 Training Loss: 1.6041032075881958\n",
      "Validation loss: 1.5805574655532837\n",
      "Step: 4600 Training Loss: 1.573875069618225\n",
      "Validation loss: 1.5458261966705322\n",
      "Step: 4700 Training Loss: 1.5477551221847534\n",
      "Validation loss: 1.5882186889648438\n",
      "Step: 4800 Training Loss: 1.5829944610595703\n",
      "Validation loss: 1.5814542770385742\n",
      "Step: 4900 Training Loss: 1.5457593202590942\n",
      "Validation loss: 1.5494290590286255\n",
      "Step: 5000 Training Loss: 1.55464768409729\n",
      "Validation loss: 1.548281192779541\n",
      "Step: 5100 Training Loss: 1.5662803649902344\n",
      "Validation loss: 1.5764596462249756\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "early_stop = 10\n",
    "last_val_loss = 1e9\n",
    "n_epochs = 4000\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    xb, yb = get_batch(train_data, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(m, val_data, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 5\n",
    "            last_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11716024\n",
      "Em một có chờm hục. – Của thể phải gia tám mê phặc lôới trâm qua thoạc:- Rồi ngúi vám. Huệ ba mà anh  muốc có người gặt, tông xiệu. Cho anh nói miếp hai biêng thỉ mình tay mật không nghe nước đi tể đầm vào, muốn bà chịu lời biểu chu hết? - Càng do vậy? – Câu chuyện cáu  chiếu sẻ Hưa ngờ loàih, sẽ như tỏ vù đẩy nhân hấp  những đó ứng an ban đời đo, Hiếu uôi phem bộ dầu dện thút, xong có ra ắpxúc dài sớm lên và hiên cáo, “ phìu sớm bỏ,  hổi đángười từng hoàng có, mà ca nhưng đã lia duốc. Nhình em trong của, Huệa thoát cách. Phốc có tiêm, màu cào trai đều cái đủ mười thông ra  kiêng ta khí nhắc, giống chất bớm đủ tay sẽ còn về người nhuộ, chỗ thay khàng vừa sẻ để vậy  con ông thung trời cổ cổ không? Eu chăn nhiên nỗi gio ruồi cứ lòng. Cười, bộ mùng hen nhìn xây: - Nghe- Những bảo..hà!- Vậy trởi trang sợ quới thiệm phôm, nóng không  coi được  tỏ ngồi thai miệng. \n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in m.parameters() if p.requires_grad))\n",
    "starting_tokens = 'Em'\n",
    "len_starting_tokens = len(starting_tokens)\n",
    "idx = torch.tensor(encode(starting_tokens)).reshape(1, len_starting_tokens).to(device)\n",
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
