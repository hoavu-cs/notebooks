{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/text/clean_tales.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [45, 991, 691, 1858, 367, 81, 284, 18]\n",
      "Decoded string:  I love programming.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=[\"datasets/text/clean_tales.txt\"], vocab_size=3000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# 3. Save the tokenizer (optional)\n",
    "# You can save the trained tokenizer to reuse later\n",
    "tokenizer.save_model(\"datasets/text/\")\n",
    "\n",
    "# 4. Encode a text string\n",
    "output = tokenizer.encode(\"I love programming.\")\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string\n",
    "\n",
    "# 5. Using processors for compatibility (optional)\n",
    "# Configure the tokenizer to output the special tokens needed for models like BERT.\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [0, 350, 385, 2716, 1250, 18, 203, 44, 45, 43, 44, 1772, 264, 2305, 16, 321, 263, 2259, 1741, 509, 82, 16, 969, 264, 338, 280, 649, 294, 264, 385, 2716, 1250, 18, 225, 495, 267, 2]\n",
      "Decoded string:  The Happy Prince.\n",
      "HIGH above the city, on a tall column, stood the statue of the Happy Prince.  He w\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(text[:100])\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 5975565\n",
      "Vocab size: 3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_ids = tokenizer.encode(text).ids\n",
    "train_size = int(len(text_ids) * 0.8)\n",
    "train_ids = torch.tensor(text_ids[:train_size], dtype=torch.long)\n",
    "val_ids = torch.tensor(text_ids[train_size:], dtype=torch.long)\n",
    "print(f'Number of tokens: {len(text_ids)}')\n",
    "print(f'Vocab size: {tokenizer.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 17248692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, n_layers, n_heads, block_size, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "        return idx\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 50  \n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "n_emb = 500\n",
    "n_layers = 4\n",
    "n_heads = 2\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LanguageModel(vocab_size, n_emb, n_layers, n_heads, block_size, dropout).to(device)\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", crying out: \"There's a great dragon coming! Somebody ought to do something, or we shall all be destroyed.\"\n",
      "He was caned for untruthfulness without any delay. His master was never one\n",
      " crying out: \"There's a great dragon coming! Somebody ought to do something, or we shall all be destroyed.\"\n",
      "He was caned for untruthfulness without any delay. His master was never one for\n",
      "tensor([[  16, 2568,  424,   30,  413, 1142,  411,  263,  586, 2075, 1397,    5,\n",
      "         2857, 1210, 2009,  282,  401,  924,   16,  474,  354,  724,  383,  307,\n",
      "         2482,  399,   93,  276,  457,  203,  769,  314,  546,  276,  337, 1047,\n",
      "           86, 1580,  631,  781,  886,  541, 1030,  326,   18, 1408, 1604,  314,\n",
      "          632,  428]])\n",
      "tensor([[2568,  424,   30,  413, 1142,  411,  263,  586, 2075, 1397,    5, 2857,\n",
      "         1210, 2009,  282,  401,  924,   16,  474,  354,  724,  383,  307, 2482,\n",
      "          399,   93,  276,  457,  203,  769,  314,  546,  276,  337, 1047,   86,\n",
      "         1580,  631,  781,  886,  541, 1030,  326,   18, 1408, 1604,  314,  632,\n",
      "          428,  337]])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "a, b = get_batch(train_ids, block_size, 1)\n",
    "print(tokenizer.decode(a[0].tolist()))\n",
    "print(tokenizer.decode(b[0].tolist()))\n",
    "print(a)\n",
    "print(b)\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 3.6234896183013916\n",
      "Validation loss: 4.139702796936035\n",
      "Step: 100 Training Loss: 3.7743701934814453\n",
      "Validation loss: 4.158641338348389\n",
      "Step: 200 Training Loss: 3.6609697341918945\n",
      "Validation loss: 3.931398630142212\n",
      "Step: 300 Training Loss: 3.772047996520996\n",
      "Validation loss: 4.1647539138793945\n",
      "Step: 400 Training Loss: 3.687303066253662\n",
      "Validation loss: 4.084819793701172\n",
      "Step: 500 Training Loss: 3.701794147491455\n",
      "Validation loss: 4.065249919891357\n",
      "Step: 600 Training Loss: 3.6982436180114746\n",
      "Validation loss: 4.113849639892578\n",
      "Step: 700 Training Loss: 3.782191753387451\n",
      "Validation loss: 4.059750556945801\n",
      "Step: 800 Training Loss: 3.7709543704986572\n",
      "Validation loss: 3.943270683288574\n",
      "Step: 900 Training Loss: 3.6416232585906982\n",
      "Validation loss: 4.101812839508057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# training parameters\n",
    "batch_size = 32\n",
    "early_stop = 30\n",
    "last_val_loss = 1e9\n",
    "n_epochs = 1000\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_ids, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, val_ids, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 30\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'datasets/text/clean_tales_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without food only below er fitting official leaping the capture, in the days of \"past not excuse me,\" he substitiated a wigwork. \"Very well.\n",
      "Yes'\n",
      "What to come, you bothered your eyes and wrapped the featherruck's head. The boat might allowed, and he scarcely turned in the hall whence he caught him, and everyone who happened to be a very nargest against any one of them set free. The little tailor was coming along in vain evil than he was that his hedge came, “Yes here are many people, I give it! Let us listen!”\n",
      "There was grown, and when she entered, she was an inject with a smiling face, and now suddenly bent over a glass of dull next to her, and said, \"What did you give me Alpoleantage? Cinderella are you fitted as quickly?\" He answered and sweathed and motionless. \"Way-night, you saw that you are going on all along here, leave me at dinner in a perfect excroper merry-respond creature; children, roar, rollers, shape, despair, and corn, and most together, now too, was inscriptively forms, san of the vagabonds, and as hard, even in time after composure first to preach. The spear in his whole was drawlaria, however, pleased. But no sooner did the most ash-mison, and, Jonah, since the other, the day was even troubled, he laid his mind to split the human being indifferent, and gave it up the bed, tweel, steadily back against the sharp lips.\n",
      "This resolution put into cigars. He became apparent, when he had walked into it to buttons and directions he overtell him of a Good Witch.\n",
      "\n",
      "\"But your mother will never go to-morrow. It will be as easy as my sisters, my dear!\"\n",
      "He was aware of her to his men whom he was specially tired with the shoes about him to consent to bring her broachment if she had been brought hot at her, with a look on the other hand and carried her in her.\n",
      "She had crept out that he might look up, was six million three, for the child, there came an one with a loud frequent storm with her originally and bitter tears, to look at her came, especially being tearing her in singular statement, in a common sort of desperiment.\n",
      "\"Why, on now,\" said the man, as he had no more over the mind he met his mother as a perfect banism between the blue sea. \"Truehood was it matter?\" said Jack, \"go along,\" and they both lived hidden in their cream, and the pitcher and the scaffold started at one where they lived.\n",
      "Then they had a small room for their days and journeyed away together, and then taking what they had want, he promised to kill Peter umpily.\n",
      "\"I'll see that the kind actors creak.\"\n",
      "\"You don't know,\" said the King. And when they're gone by to story the Princess, you know, found, was great event for the Sun.\n",
      "\"Have you seen all little money?\" thought the little daughter.\n",
      "\"I will go Marlinchen.\"\n",
      "\"Well,\" said he, \"that will end well; but I'll give you a little when the wedding-daughter farther! He is sure to recover him bought, and I see his heart and make to receive me.\n",
      "    *\n",
      "\"Put up in!\" cried the camel-line.\n",
      "Consey's tragedy, \"for I will go to the Chap! I cannot mistake this sky.\"\n",
      "\"Good-by, dear.\"\n",
      "Then the glass went out of his hole. And the Woodman said he couldn't come back to the lips, and called out.\n",
      "\"Oh, there! what does he say? I'd come here, getting money enough again for anything! to-night. I now know, too.  me must have received any manners.  You cannot see fall like you for strength, and life's evening, as that, as you fancy will neither likely say 'fallen', behold 'the' about mine, too, and as I saw anything little of all adieving.\n",
      "They were so an end of the rare a redian get up and rubies dressed, enloven all sorts of merits.  I staggered, quivering a paw yet, and had allowed an underworld of a little, in particular many pursuit, and that getting of some fright on the empty rose, are yelling a bridegroom, chance. When I found the nest where we lived, that they still did not break, but they could not help laughing for escape. The padden played and their Greeks followed. The Charles Darnay and the Prince got into the crocus of madness and brief and watch. Meanfa both continued to humming their beloved bunnus in a place where, lifting the spar made of pretext these diloys, is so steadily king, too, be conscious of that.\n",
      "Darrisford Ali Burdovsky was less boardantly disliked for the receiving, the sea was driven over the scene of its bummy in lark, was placed, only of her yet more indignant (sitting than the ancestry against which her last signed her son had beating was absent from her own.  If sober, she was almost forgotten, she had always thought, knew of her than that she had struck her with so much fresh talented not to her.  The pretty bride put her golden eye, said: \"With answer and fear, of whom have you fee, or shall Father hang on?\"\n",
      "When the Monkey went into the country, the rest was very clearly merriest, who had become quite agreeable. He felt the good Munchkin 'Lague.' \"And there is time to gets in a certain rigging-and-saw. When they arrived at a merry time, they went to the mouth of \"ots.\"\n",
      "\"All will do well,\" said she, \"dell and go free, the way to me, for they have brought all the way over the Lion's room. Then a mouse laughed, and make their riddle, and then wegh everybody came to go and see where the two sisters wanted, for when the Princess fell and their tailor was wholly lovely came and said, \"Woe carry this country?\" \"To whom you have come, how you don't want to kill a child?” “Hother, what shall I do with you?” “Oh, tell me that there are two little Agatha,” said the woman. The woman continued, and went to bed, and carried her my knife down to the top. The little girl again went into the forest, for she couldn’t forget that when she got away the herds to the bed it was a second given to-morrow, and could speak, “What do you do now give me any money that a man pedestal doch off, Lucie?” But she threw down the birdmen. She was a long time. She could have made no ugly lack of fling in us and in disgrace as the beast's screwed daughter who could haveisted him, and she was extreme than that in another country of a poor graceful land, blubber again; but for the cat listened in grey veus, but all the poor wilful Lotster understood, and since the rest of the animals saw themselves going to pick off his hat, and when he thrust the finger, he said:\n",
      "At last the apparel not having been wisest, and thought, \"This time he will be her, after which you are truly horribly Mr. Gerda go and make the ash increased the glass hillside, though there are many mistake that can only be anything talking no one to see them of their self-commandal in the voyage of the party. They are as well rely see gagarily destroyed by 'the first coversut heroqu life, depended 'em fashion's blameness' health' bodily with a beautiful powder of delight with her curves.\n",
      "Again, the typoner took her the small cure and revolving clods to sail she had been put root on both in medicat to a hapless writer of the house. They had daughters exchanged in to come still to helt the sea brow to drink with thundle between themi so\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = 'Without food'\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens).ids\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "model.eval()\n",
    "generation = model.generate(idx, max_new_tokens=2000, block_size=block_size, temperature=1)[0].tolist()\n",
    "model.train()\n",
    "print(tokenizer.decode(generation))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
