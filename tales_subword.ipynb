{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/text/clean_tales.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [45, 991, 691, 1858, 367, 81, 284, 18]\n",
      "Decoded string:  I love programming.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=[\"datasets/text/clean_tales.txt\"], vocab_size=3000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# 3. Save the tokenizer (optional)\n",
    "# You can save the trained tokenizer to reuse later\n",
    "tokenizer.save_model(\"datasets/text/\")\n",
    "\n",
    "# 4. Encode a text string\n",
    "output = tokenizer.encode(\"I love programming.\")\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string\n",
    "\n",
    "# 5. Using processors for compatibility (optional)\n",
    "# Configure the tokenizer to output the special tokens needed for models like BERT.\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [0, 350, 385, 2716, 1250, 18, 203, 44, 45, 43, 44, 1772, 264, 2305, 16, 321, 263, 2259, 1741, 509, 82, 16, 969, 264, 338, 280, 649, 294, 264, 385, 2716, 1250, 18, 225, 495, 267, 2]\n",
      "Decoded string:  The Happy Prince.\n",
      "HIGH above the city, on a tall column, stood the statue of the Happy Prince.  He w\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(text[:100])\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 5975565\n",
      "Vocab size: 3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_ids = tokenizer.encode(text).ids\n",
    "train_size = int(len(text_ids) * 0.8)\n",
    "train_ids = torch.tensor(text_ids[:train_size], dtype=torch.long)\n",
    "val_ids = torch.tensor(text_ids[train_size:], dtype=torch.long)\n",
    "print(f'Number of tokens: {len(text_ids)}')\n",
    "print(f'Vocab size: {tokenizer.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 17248692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, n_layers, n_heads, block_size, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "        return idx\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 50  \n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "n_emb = 500\n",
    "n_layers = 4\n",
    "n_heads = 2\n",
    "dropout = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LanguageModel(vocab_size, n_emb, n_layers, n_heads, block_size, dropout).to(device)\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", crying out: \"There's a great dragon coming! Somebody ought to do something, or we shall all be destroyed.\"\n",
      "He was caned for untruthfulness without any delay. His master was never one\n",
      " crying out: \"There's a great dragon coming! Somebody ought to do something, or we shall all be destroyed.\"\n",
      "He was caned for untruthfulness without any delay. His master was never one for\n",
      "tensor([[  16, 2568,  424,   30,  413, 1142,  411,  263,  586, 2075, 1397,    5,\n",
      "         2857, 1210, 2009,  282,  401,  924,   16,  474,  354,  724,  383,  307,\n",
      "         2482,  399,   93,  276,  457,  203,  769,  314,  546,  276,  337, 1047,\n",
      "           86, 1580,  631,  781,  886,  541, 1030,  326,   18, 1408, 1604,  314,\n",
      "          632,  428]])\n",
      "tensor([[2568,  424,   30,  413, 1142,  411,  263,  586, 2075, 1397,    5, 2857,\n",
      "         1210, 2009,  282,  401,  924,   16,  474,  354,  724,  383,  307, 2482,\n",
      "          399,   93,  276,  457,  203,  769,  314,  546,  276,  337, 1047,   86,\n",
      "         1580,  631,  781,  886,  541, 1030,  326,   18, 1408, 1604,  314,  632,\n",
      "          428,  337]])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "a, b = get_batch(train_ids, block_size, 1)\n",
    "print(tokenizer.decode(a[0].tolist()))\n",
    "print(tokenizer.decode(b[0].tolist()))\n",
    "print(a)\n",
    "print(b)\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 4.610112190246582\n",
      "Validation loss: 5.031398773193359\n",
      "Step: 100 Training Loss: 4.609602928161621\n",
      "Validation loss: 4.772461414337158\n",
      "Step: 200 Training Loss: 4.605961322784424\n",
      "Validation loss: 4.667159557342529\n",
      "Step: 300 Training Loss: 4.4932169914245605\n",
      "Validation loss: 4.489596366882324\n",
      "Step: 400 Training Loss: 4.366876125335693\n",
      "Validation loss: 4.572510719299316\n",
      "Step: 500 Training Loss: 4.514391899108887\n",
      "Validation loss: 4.652623176574707\n",
      "Step: 600 Training Loss: 4.456912040710449\n",
      "Validation loss: 4.531207084655762\n",
      "Step: 700 Training Loss: 4.211000442504883\n",
      "Validation loss: 4.397500991821289\n",
      "Step: 800 Training Loss: 4.369976043701172\n",
      "Validation loss: 4.491415500640869\n",
      "Step: 900 Training Loss: 4.158637046813965\n",
      "Validation loss: 4.438628673553467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# training parameters\n",
    "batch_size = 32\n",
    "early_stop = 30\n",
    "last_val_loss = 1e9\n",
    "n_epochs = 5000\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_ids, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, val_ids, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 30\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'datasets/text/clean_tales_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The crazy witch riss an hinder-put in the rock from the new year to get, which the sun shone, and then the face of the ground, he was deliolently memorated, and wept the talk with a surprise, and bade his locks of exagle, while the king was twenty miles in the midstery, and let the same one or a stone that which the Cule and sadness were swing across a shining property and guroding on the earth. So the Chambering was placed, and the little mans rushed, and the heart seemed to be angry with the straw, the bears that could not have been a torreak, and she was written.\n",
      "He led the old tiger and stared the little maiden. He brought this before.\n",
      "\"Who's a very best way a wicked woman, now the King and pardon got him to tell him all the night, a man of glocle of modern, and a cloak at the darkness of the prisoner, it was not so sure that the elbet, and in this there was the modest or rainburrange, and there was a heat, turned behind the took the robbers and watched the paravet, and so they had been winged and silks. It was a very little interest, and he rushed, but they looked this, and had wanted to make a large vololate with low just the most beautiful brother to appear in the world. For a woman had been there the house the aid of season, or the Joe the two brothers had begun to kill the meliness in their case the sorrow of the lamp and all these, a gold man to be a letter of all visance, and the Raja had come to the gods.\n",
      "I was very like a long way to him, and that, for the reason why be the Emerald City of &iathy had come and treating all the motles of this place? He had looked a little one, and he was on the castle o’clock of his hand, and the cat in the evening, and a good humor, rushed his head, and rose into the same circle of his huge kingdom and violent corrects to the humor.\n",
      "The old lady was soon as if there was no one of the visits, as he sat down in a prevail, and seemed to be allowed to be under his own account with his own effort.\n",
      "\n",
      "The brain might be the last sun was shortly some of the mutine, and there was a serve to the rest of the bch of my heart to bill a great summer a little walk on the rise, and this is a brave a large that place that the glorlder are not baked our windows, he makes a little girl, perhaps by the prince; narrow wives righedics, and soon as it is mentioned, and the greatest man, or he had been a hands of reveller, and no chaving the rock, from the matter if he had a study creature, and the brightness of the trees was exceeding to the most king of the antailed with unsrect; and they genius the reder of the breeches of the gay wealth all.\n",
      "It was a still further little Hans, or a whiers that he was sewing to the richer for the fact that the lady were royal spears.\n",
      "After you reached playing your head of love with you, my sale and named it would be right again.\"\n",
      "\"I'm glad you bonnel,\" said the Paquis, \"I think I'm afraid of you, sir.\"\n",
      "\"Well, if you can make if you like that?\"\n",
      "\"Now you're a mere matter!\"\n",
      "\"While Gretel. \"Oh, why, when you will be just going to let me go his bed. You stay and her, when, quick as Jo as it was it, and they must fiven That's Fanny, he is goodly lost the curl direction, and the little merdigger One, and filling a coinet thres in the tree. It was done that Fear creature cart, still his matched peering, just as he had reason to be hanged, for the desert of two hours as they came into the lads, Mary answered by marriageing him from her, and each of her accident of his chirror, and my dear little girls had been in a dirty, and went back to be allowed to look at her, and see.\n",
      "\"Thank you with you about him to look very King,\" cried he, tyinged the little Worts, \"if it three days, just as I have thought he would admitted it, and when we are always quite confined, she would not carry myself to run my heart.” “Well, a man, and I am sure,” replied Mr. Lorry, “I’ll ble of all myself, but if you are not short to think to raise, “by the apprehens here. Let us have not been alarm from us to think it’s company, if he is too like this a bit of Bumover, they would not trust that his ones meades.”\n",
      "He moved the castle, and came to the North and the poor old woman to the road for his sides, and her drower had asked:\n",
      "“I'll come and tell me that we are right. And General I'm a regular brandy. It's arrows, if you came!\n",
      "“‘I take me that all the princess frog here, my Sherlock!” he replied:\n",
      "“I confiday my good luck, and tell me how much you will show her here, I am a little calnd.\n",
      "The prince found a singular manwhere, and then she splined, and this is it out against me as they carry her, and even for us all her well, and the court, with her hand, and went on his path, and the eyes silly at the golden table and hands of the whale, and loved her hair, and glance the time, and to snake the day, the rest of the young and rest was happy for the story of the actual waters of bed, and she was just as if they had not been in the house and his heart had been one.\n",
      "“If you are ready,” said the herenzee, “but we are ever long ago, I amid here?” “For Pavlicheff!”\n",
      "“But, if I thought I only thought that you were only peril. I can meet it, I think I'm regret, but I’d tempose my plenty of children.”\n",
      "“I have been here anything thirty of your pursuit enough for this now? My prayers better will be saved to me? You can overcome for the rest of the way to keep your calfs for you. It is strange, and I can see you.”\n",
      "“If I am some way to-day! Tood him, ha! how can one who isn't all right,” answered the Time Traveller.\n",
      "“By windows, “that tha’s that you could be able to look like it and live, or, and I think that’s a most clear of bit of one thing!”\n",
      "“I still not a little man who’m going to the lax, it forecase two thousands of love for any more simple, if the preciousness of a human beard I should think at the prevailed Mr. Flovitch, but there is thankful any importance of the table, and give the part of my labor and hated their nurs, and he cannot be a voices.  All all the bed of mock beard, not a man who was very pretty and contempt to conceive an exculet for?\n",
      "She was too long at peaceful to the end of her nest, and a crowding the road of the room, and he held a gray, who, like a great fever, until the first was a high lump, and a gold thing she ate her face.\n",
      "And Dickon had been a happy time, and the Scarecrow fell down the royal water in her life, and the door opened her arms there, and continued all day the other thoughts were realized that somehow was not before.\n",
      "Next day the second tree had reached the morning, and she found the crops of the carpet, the young man was fled, but it was so long and unceaned, during the two years old, and the treat of rise was swelling up and stumbled to a drooping-boyet, and the apartment of their strengths.\n",
      "From the sea, and he was always beside the emotion of his head, and he found himself so anxious to the three minutes when the Lion stood the iron door which he turned to the walls of the castle. She flew the sand\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = 'The crazy witch'\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens).ids\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "model.eval()\n",
    "generation = model.generate(idx, max_new_tokens=2000, block_size=block_size, temperature=0.8)[0].tolist()\n",
    "model.train()\n",
    "print(tokenizer.decode(generation))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
