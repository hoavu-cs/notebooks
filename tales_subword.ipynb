{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/text/clean_tales.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [45, 991, 691, 1858, 367, 81, 284, 18]\n",
      "Decoded string:  I love programming.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=[\"datasets/text/clean_tales.txt\"], vocab_size=3000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# 3. Save the tokenizer (optional)\n",
    "# You can save the trained tokenizer to reuse later\n",
    "tokenizer.save_model(\"datasets/text/\")\n",
    "\n",
    "# 4. Encode a text string\n",
    "output = tokenizer.encode(\"I love programming.\")\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string\n",
    "\n",
    "# 5. Using processors for compatibility (optional)\n",
    "# Configure the tokenizer to output the special tokens needed for models like BERT.\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string:  [0, 350, 385, 2716, 1250, 18, 203, 44, 45, 43, 44, 1772, 264, 2305, 16, 321, 263, 2259, 1741, 509, 82, 16, 969, 264, 338, 280, 649, 294, 264, 385, 2716, 1250, 18, 225, 495, 267, 2]\n",
      "Decoded string:  The Happy Prince.\n",
      "HIGH above the city, on a tall column, stood the statue of the Happy Prince.  He w\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(text[:100])\n",
    "print(\"Encoded string: \", output.ids)  # output.ids is the tokenized representation\n",
    "print(\"Decoded string: \", tokenizer.decode(output.ids))  # decoding back to the original string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 5975565\n",
      "Vocab size: 3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_ids = tokenizer.encode(text).ids\n",
    "train_size = int(len(text_ids) * 0.8)\n",
    "train_ids = torch.tensor(text_ids[:train_size], dtype=torch.long)\n",
    "val_ids = torch.tensor(text_ids[train_size:], dtype=torch.long)\n",
    "print(f'Number of tokens: {len(text_ids)}')\n",
    "print(f'Vocab size: {tokenizer.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 12108288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def estimate_loss(model, val_data, block_size, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(val_data, block_size, batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.tril(torch.ones(sz, sz)) == 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_emb, n_layers, n_heads, block_size, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_emb, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        x = token_emb + position_emb\n",
    "        x_transform = x.clone()\n",
    "        mask = generate_square_subsequent_mask(T).to(device)\n",
    "        \n",
    "        x_transform = self.transformer_encoder(x_transform.permute(1, 0, 2), mask=mask)\n",
    "        x_transform = x_transform.permute(1, 0, 2)\n",
    "        x = x + x_transform\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, block_size, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Scale logits by the temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "        return idx\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 75  \n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "n_emb = 300\n",
    "n_layers = 6\n",
    "n_heads = 5\n",
    "dropout = 0.2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LanguageModel(vocab_size, n_emb, n_layers, n_heads, block_size, dropout).to(device)\n",
    "print(f'Number of parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", without being quite sure what the best really was.  Money, position, fashionable accomplishments, and elegant manners were most desirable things in her eyes, and she liked to associate with those who possessed them, often mistaking the false for the true, and admiring what\n",
      " without being quite sure what the best really was.  Money, position, fashionable accomplishments, and elegant manners were most desirable things in her eyes, and she liked to associate with those who possessed them, often mistaking the false for the true, and admiring what was\n",
      "tensor([[  16,  886,  923,  794, 1096,  502,  264, 1165, 1233,  314,   18,  225,\n",
      "          375, 1110,   16, 2865, 1374,   16,  277, 1211,  377,  620, 2729,  737,\n",
      "          619, 1909,   16,  275, 1822,   75,  425,  452,   82,  490,  409,  873,\n",
      "          864,  335,  620,  910,  300,  343,  752,   16,  275,  359, 2164,  282,\n",
      "          334, 1053, 1272,  540,  345, 1029,  439, 1240, 2217,  429,   16, 1565,\n",
      "         1835,  868,  264,  277,  357,  325,  337,  264, 1607,   16,  275, 1667,\n",
      "          335,  284,  502]])\n",
      "tensor([[ 886,  923,  794, 1096,  502,  264, 1165, 1233,  314,   18,  225,  375,\n",
      "         1110,   16, 2865, 1374,   16,  277, 1211,  377,  620, 2729,  737,  619,\n",
      "         1909,   16,  275, 1822,   75,  425,  452,   82,  490,  409,  873,  864,\n",
      "          335,  620,  910,  300,  343,  752,   16,  275,  359, 2164,  282,  334,\n",
      "         1053, 1272,  540,  345, 1029,  439, 1240, 2217,  429,   16, 1565, 1835,\n",
      "          868,  264,  277,  357,  325,  337,  264, 1607,   16,  275, 1667,  335,\n",
      "          284,  502,  314]])\n",
      "torch.Size([1, 75]) torch.Size([1, 75])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, block_size, batch_size):\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "a, b = get_batch(train_ids, block_size, 1)\n",
    "print(tokenizer.decode(a[0].tolist()))\n",
    "print(tokenizer.decode(b[0].tolist()))\n",
    "print(a)\n",
    "print(b)\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 4.923049449920654\n",
      "Validation loss: 5.0128021240234375\n",
      "Step: 100 Training Loss: 4.605337142944336\n",
      "Validation loss: 4.853498935699463\n",
      "Step: 200 Training Loss: 4.876150131225586\n",
      "Validation loss: 4.920846462249756\n",
      "Step: 300 Training Loss: 4.563838481903076\n",
      "Validation loss: 4.820635795593262\n",
      "Step: 400 Training Loss: 4.809290409088135\n",
      "Validation loss: 4.89808464050293\n",
      "Step: 500 Training Loss: 4.753746509552002\n",
      "Validation loss: 4.858625888824463\n",
      "Step: 600 Training Loss: 4.574955940246582\n",
      "Validation loss: 4.820096015930176\n",
      "Step: 700 Training Loss: 4.658062934875488\n",
      "Validation loss: 4.822627544403076\n",
      "Step: 800 Training Loss: 4.613270282745361\n",
      "Validation loss: 4.784586429595947\n",
      "Step: 900 Training Loss: 4.569982528686523\n",
      "Validation loss: 4.621920585632324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# training parameters\n",
    "batch_size = 16\n",
    "early_stop = 20\n",
    "last_val_loss = 1e9\n",
    "n_epochs = 1000\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_ids, block_size, batch_size)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Training Loss:', loss.item())\n",
    "        val_loss = estimate_loss(model, val_ids, block_size, batch_size)\n",
    "        print('Validation loss:', val_loss)\n",
    "        if val_loss >= last_val_loss:\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                print('Early stop!')\n",
    "                break\n",
    "        else:\n",
    "            early_stop = 20\n",
    "            last_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'datasets/text/clean_tales.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The king you, too liked Oz southiness on him caught in searching the educers lying with a foolishference donkey as the whiteened in all of to tearsest the dimpect, his mind to think her tears of his father commands not hunt a dog.\n",
      "“Yes, and Jo spoke to die, or myself to see Edward by threw herself and the chief very sorry and to the door.\n",
      "\"A week; a face, even to beachuly upon however, will let me as if Jupitere were taken down to himself a box around as suddenly until understand was the Godsaid one of the futtle, and sides. That's remaining with him.  I'm as another black Christmas. uttered growing middle of a week Aflein. I see her too he climbed, and find him.\n",
      "The two devanks now, but what do see,\" said the camp stood through a very much to keep the an exer. Height things where it occasion. Naturred in the dearehold as thou art I amused to himself up the nonsievna like a wakers and everything.\n",
      "that her, still Nor in Cous!\n",
      "“I amboria.\n",
      "“Yes,\" said doing soft could only seen him to make of froxenuzzled. Now were very rich days, and stood the tid for what a lotted by considered and bones came a large mipped to which is, 'Where knew how dirty, but Ia Ne,” said SWell, let you get bridaomine at Evrestory whatever you're born till morning riddled as she so that he would not watch. Napial time, Mr.  And when he went around him the reasses had been line;\"Where’d with me.\n",
      "“It was out. His teeth, and thenny Whitement, and flew side by a moment a ship, after angehomaid wise to Sunday-morrow, “I amused, but the extross, and a lovely country from his father refused the hooked, ask that ye been visage lies in drahas, he reached the win an owork called Ant.\n",
      "So they heard, the ground us but witch,\" said the cliversson’s in their powerful time. Brell to Gania.\n",
      "He did not approached in one desire in fact that the num.'\n",
      "The wicked ugrandon sat down like the indignity, Night and subs ever that they like to busy abst easily,\" said at a than if the river, and close to say, she had usual, but the admitted well break a rapate me all he assens to his rings, now with proud speciers in the survals nest, and their mind; the torment before in the Lion brought him he desertion, and smiling to sit down the water's palace found me killed of pleasant nibrings of her as children, we are you know when thing this personally pictures. She always put it has probably.”\n",
      "   South figure is always complimplapped himself besides and dartles, and everybody else has got small pore, then, do notice of the window with one of a wonderful years Laurie is twenty fire of thousand times of Rossship,\" said, and the other bones friend Miss Lake, Bringess and pleasant, which, and beneath the others, each as before.\n",
      "Justy young form was sitting in order to Stubbish wretched as sheatouds of saying, because they seemed burning this business, then to feel  It was land. It is a sadly hit, and Khains, in the two morning, you only finally were no one had come off to sending but authorlock sea. It's talking where the leakened in men, and screams ab tree unless as if if she had quite black the fire?”\n",
      "“No time, as bigity of thousand delicious, found anna tied into his forest, still rom slapping, he came home. “Full-habethave with except to come to leintment which, which they were getting what I have no longer. For village of rain. Dust, he go to the seal all liter bloombeelego, who well known the face, “I all all the sight of breaking cat. Sha would seem good vio, crying to tell me. She looked surprised that heave, \"No, but a half-bury gave his feet, the woman who continued with a very murming at Queen had a big idea this line my place in theass, with the doing about and tell me.” “RUdays this story!”\n",
      "“I was mood heart told, and the other less, that Mins be bla's a stoospit you must fields.\n",
      "Becky.\n",
      "Then he called the unacle or two womenless, for breakfast-ts that they stood upon them which music, blew slowly straighter.  But there be nothing to no, I wish you must help scarcell pustriced that you would try his cranning. Bid it was in the other slow surve?\" inquired in the next day of charoinecucky, much good one for on her foughtical child, and in another, upon the portraphabethble face of him,” she saw a top of ange, moved among the bree. When this:\n",
      "The girl, and the dark curiest-cobreadgrainverseus burn. The glit King, trave to her with him and fell down on a gallrolly terms of while you know,\" said walking.\n",
      "     I, and Cham and will tell me falling believe me the possibly to his committed on the days.” So the garratabaors:\n",
      "It was won’s no one eye tell?”\n",
      "\"If the wonderful whale ravored thousand. There is three times.”\n",
      "But looking at poor or no more.\n",
      "As ever pleasure, ainn't know, see his wrong as a bad ship will do. He then! \n",
      "So it. She thought that he found that different for the prince must be able to all? No questions. Wouldered, times cold and of bread and let those over with being fitted, as ever since you see; for you not expected what is not amazara, raisarucation.\n",
      "HI never heard half-own since they did the left out bargard” and once, we must nearly look out all right, the general for it in generaleed before one whales led his full wid you I have, And had light room and Mrs. By running to behold, was a terrible chor of his grandfather of me,   Elets the flues, and stood a deep her too.\"\n",
      "\"Oh; and heaven! Do you eat you take his name, before, of his dignity who saw the a fair, and they came into phasmetrolirst manner were about, in heavens, tell the country, and thirtyious face away, for it, commanded up the woodlaring that had found, and face to Snowleding the poor, a drieding about it was placed to myself up there made a great old morted with a post you anybody was some day the blew the Six-day I am very merriblely of Sometimes of the cared a good cove spranging it is much wondered and afternoon my wife at last she saw to Berduously themselvesia walked disposopresassed, says that regarding them than I,     Fin herself of the ris name of a moment, like the objectsday I don't know whether it was on the Palace you to dris lips to this evening to you meanness was last. O Livesmiss me soon began to the wicked was told Amyartieved. 'Bet into the key the weddingment shall be able to seatedred rose to drawstead and on the Fret. Perest, for she raised a cut you?\n",
      "Megical servant came to us forth, the choved as the house, and reading-cromfast the flucerhaps I saw him, my heartsman looked his spand-bble, “Now I creature.\n",
      "\"It's terribid this, and put all. Brady; A A deep, and Jack in the volk and rode out his old man likewise?”\n",
      "“Oh, sheking of the flushed at all day,    velopard-morrow!\" said Elinor, “Why did not mark.\n",
      "\n",
      "“Don’ller darkly off she could or even de Dermination went to send.\n",
      "“Oh, that seemed up every, “Walk to have been away, and pake your grandmother was nearestacmitusly called it is there?\"\n",
      "The Con, and intelligation. ’t care if. But there.\n",
      "Then help him afterwards that since he would possessed through another sail, and b\n"
     ]
    }
   ],
   "source": [
    "starting_tokens = 'The king'\n",
    "\n",
    "encoded_start = tokenizer.encode(starting_tokens).ids\n",
    "len_starting_tokens = len(encoded_start)\n",
    "\n",
    "idx = torch.tensor(encoded_start).reshape(1, len_starting_tokens).to(device)\n",
    "generation = model.generate(idx, max_new_tokens=2000, block_size=block_size, temperature=1)[0].tolist()\n",
    "print(tokenizer.decode(generation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The king you, too liked Oz southiness on him caught in searching the educers lying with a foolishference donkey as the whiteened in all of to tearsest the dimpect, his mind to think her tears of his father commands not hunt a dog.\n",
    "“Yes, and Jo spoke to die, or myself to see Edward by threw herself and the chief very sorry and to the door.\n",
    "\"A week; a face, even to beachuly upon however, will let me as if Jupitere were taken down to himself a box around as suddenly until understand was the Godsaid one of the futtle, and sides. That's remaining with him.  I'm as another black Christmas. uttered growing middle of a week Aflein. I see her too he climbed, and find him.\n",
    "The two devanks now, but what do see,\" said the camp stood through a very much to keep the an exer. Height things where it occasion. Naturred in the dearehold as thou art I amused to himself up the nonsievna like a wakers and everything.\n",
    "that her, still Nor in Cous!\n",
    "“I amboria.\n",
    "“Yes,\" said doing soft could only seen him to make of froxenuzzled. Now were very rich days, and stood the tid for what a lotted by considered and bones came a large mipped to which is, 'Where knew how dirty, but Ia Ne,” said SWell, let you get bridaomine at Evrestory whatever you're born till morning riddled as she so that he would not watch. Napial time, Mr.  And when he went around him the reasses had been line;\"Where’d with me.\n",
    "“It was out. His teeth, and thenny Whitement, and flew side by a moment a ship, after angehomaid wise to Sunday-morrow, “I amused, but the extross, and a lovely country from his father refused the hooked, ask that ye been visage lies in drahas, he reached the win an owork called Ant.\n",
    "So they heard, the ground us but witch,\" said the cliversson’s in their powerful time. Brell to Gania.\n",
    "He did not approached in one desire in fact that the num.'\n",
    "The wicked ugrandon sat down like the indignity, Night and subs ever that they like to busy abst easily,\" said at a than if the river, and close to say, she had usual, but the admitted well break a rapate me all he assens to his rings, now with proud speciers in the survals nest, and their mind; the torment before in the Lion brought him he desertion, and smiling to sit down the water's palace found me killed of pleasant nibrings of her as children, we are you know when thing this personally pictures. She always put it has probably.”\n",
    "   South figure is always complimplapped himself besides and dartles, and everybody else has got small pore, then, do notice of the window with one of a wonderful years Laurie is twenty fire of thousand times of Rossship,\" said, and the other bones friend Miss Lake, Bringess and pleasant, which, and beneath the others, each as before.\n",
    "Justy young form was sitting in order to Stubbish wretched as sheatouds of saying, because they seemed burning this business, then to feel  It was land. It is a sadly hit, and Khains, in the two morning, you only finally were no one had come off to sending but authorlock sea. It's talking where the leakened in men, and screams ab tree unless as if if she had quite black the fire?”\n",
    "“No time, as bigity of thousand delicious, found anna tied into his forest, still rom slapping, he came home. “Full-habethave with except to come to leintment which, which they were getting what I have no longer. For village of rain. Dust, he go to the seal all liter bloombeelego, who well known the face, “I all all the sight of breaking cat. Sha would seem good vio, crying to tell me. She looked surprised that heave, \"No, but a half-bury gave his feet, the woman who continued with a very murming at Queen had a big idea this line my place in theass, with the doing about and tell me.” “RUdays this story!”\n",
    "“I was mood heart told, and the other less, that Mins be bla's a stoospit you must fields.\n",
    "Becky.\n",
    "Then he called the unacle or two womenless, for breakfast-ts that they stood upon them which music, blew slowly straighter.  But there be nothing to no, I wish you must help scarcell pustriced that you would try his cranning. Bid it was in the other slow surve?\" inquired in the next day of charoinecucky, much good one for on her foughtical child, and in another, upon the portraphabethble face of him,” she saw a top of ange, moved among the bree. When this:\n",
    "The girl, and the dark curiest-cobreadgrainverseus burn. The glit King, trave to her with him and fell down on a gallrolly terms of while you know,\" said walking.\n",
    "     I, and Cham and will tell me falling believe me the possibly to his committed on the days.” So the garratabaors:\n",
    "It was won’s no one eye tell?”\n",
    "\"If the wonderful whale ravored thousand. There is three times.”\n",
    "But looking at poor or no more.\n",
    "As ever pleasure, ainn't know, see his wrong as a bad ship will do. He then! \n",
    "So it. She thought that he found that different for the prince must be able to all? No questions. Wouldered, times cold and of bread and let those over with being fitted, as ever since you see; for you not expected what is not amazara, raisarucation.\n",
    "HI never heard half-own since they did the left out bargard” and once, we must nearly look out all right, the general for it in generaleed before one whales led his full wid you I have, And had light room and Mrs. By running to behold, was a terrible chor of his grandfather of me,   Elets the flues, and stood a deep her too.\"\n",
    "\"Oh; and heaven! Do you eat you take his name, before, of his dignity who saw the a fair, and they came into phasmetrolirst manner were about, in heavens, tell the country, and thirtyious face away, for it, commanded up the woodlaring that had found, and face to Snowleding the poor, a drieding about it was placed to myself up there made a great old morted with a post you anybody was some day the blew the Six-day I am very merriblely of Sometimes of the cared a good cove spranging it is much wondered and afternoon my wife at last she saw to Berduously themselvesia walked disposopresassed, says that regarding them than I,     Fin herself of the ris name of a moment, like the objectsday I don't know whether it was on the Palace you to dris lips to this evening to you meanness was last. O Livesmiss me soon began to the wicked was told Amyartieved. 'Bet into the key the weddingment shall be able to seatedred rose to drawstead and on the Fret. Perest, for she raised a cut you?\n",
    "Megical servant came to us forth, the choved as the house, and reading-cromfast the flucerhaps I saw him, my heartsman looked his spand-bble, “Now I creature.\n",
    "\"It's terribid this, and put all. Brady; A A deep, and Jack in the volk and rode out his old man likewise?”\n",
    "“Oh, sheking of the flushed at all day,    velopard-morrow!\" said Elinor, “Why did not mark.\n",
    "\n",
    "“Don’ller darkly off she could or even de Dermination went to send.\n",
    "“Oh, that seemed up every, “Walk to have been away, and pake your grandmother was nearestacmitusly called it is there?\"\n",
    "The Con, and intelligation. ’t care if. But there.\n",
    "Then help him afterwards that since he would possessed through another sail, and b\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
