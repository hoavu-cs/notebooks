{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hvutr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold,GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import warnings\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import combinations\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from tqdm import tqdm \n",
    "nltk.download('wordnet')  # Download the WordNet data\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions to print out reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "def print_regression_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print('Mean absolute error:', mean_absolute_error(y_val, y_pred))\n",
    "    print('Mean squared error:', mean_squared_error(y_val, y_pred))\n",
    "    print('Root Mean squared error:', np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "\n",
    "def print_classification_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val, y_pred)}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n",
    "\n",
    "def print_multilabel_classification_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val.argmax(axis=1), y_pred.argmax(axis=1))}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101383/101383 [01:32<00:00, 1100.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# data set preview\n",
    "num_samples_twitter = 20000\n",
    "num_samples_amazon = 20000\n",
    "num_samples_imdb = 20000\n",
    "num_samples_mcdonald = 20000\n",
    "num_samples_instagram = 20000\n",
    "\n",
    "# process imdb data\n",
    "imdb_data = pd.read_csv('datasets/sentiment/imdb.csv', encoding='latin-1')\n",
    "imdb_data.columns = ['Text', 'Label']\n",
    "imdb_data = imdb_data.sample(num_samples_imdb)\n",
    "imdb_data = imdb_data[['Label', 'Text']]\n",
    "imdb_data['Label'] = imdb_data['Label'].replace(1, 'Positive')\n",
    "imdb_data['Label'] = imdb_data['Label'].replace(0, 'Negative')\n",
    "\n",
    "# process mcdonalds data\n",
    "mcdonalds_data = pd.read_csv('datasets/sentiment/mcdonald.csv', encoding='latin-1')\n",
    "mcdonalds_data.columns = ['Label', 'Text']\n",
    "mcdonalds_data = mcdonalds_data.sample(num_samples_mcdonald)\n",
    "\n",
    "# process twitter data\n",
    "twitter_data = pd.read_csv('datasets/sentiment/twitter.csv', encoding='latin-1', header=None)\n",
    "twitter_data.columns = ['Label', 'id', 'date', 'flag', 'user', 'Text']\n",
    "twitter_data = twitter_data.sample(num_samples_twitter)\n",
    "twitter_data = twitter_data[['Label', 'Text']]\n",
    "twitter_data['Label'] = twitter_data['Label'].replace(4, 'Positive')\n",
    "twitter_data['Label'] = twitter_data['Label'].replace(0, 'Negative')\n",
    "\n",
    "# process amazon data\n",
    "amazon_data = pd.read_csv('datasets/sentiment/amazon.csv', encoding='latin-1', header=None)\n",
    "amazon_data.columns = ['Label', 'title', 'Text']\n",
    "amazon_data = amazon_data.sample(num_samples_amazon)\n",
    "amazon_data = amazon_data[['Label', 'Text']]\n",
    "amazon_data['Label'] = amazon_data['Label'].replace(2, 'Positive')\n",
    "amazon_data['Label'] = amazon_data['Label'].replace(1, 'Negative')\n",
    "\n",
    "# process instagram data\n",
    "instagram_data = pd.read_csv('datasets/sentiment/instagram.csv', encoding='latin-1', header=None)\n",
    "instagram_data.columns = ['Text', 'Label', 'Date']\n",
    "instagram_data = instagram_data.sample(num_samples_instagram)\n",
    "instagram_data = instagram_data[['Label', 'Text']]\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('1', 'Negative')\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('2', 'Negative')\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('3', 'Neutral')\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('4', 'Positive')\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('5', 'Positive')\n",
    "\n",
    "\n",
    "\n",
    "# process synthetic data\n",
    "synthetic_data = pd.read_csv('datasets/sentiment/synthetic.csv')\n",
    "combined_data = pd.concat([twitter_data, amazon_data, synthetic_data, mcdonalds_data, imdb_data, instagram_data], ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(len(combined_data))):\n",
    "    combined_data['Text'][i] = contractions.fix(combined_data['Text'][i])\n",
    "    tokens = nltk.word_tokenize(combined_data['Text'][i])\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    combined_data['Text'][i] = ' '.join(tokens)\n",
    "\n",
    "combined_data.to_csv('datasets/sentiment/combined.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive']\n",
      "size of data: 101383, shape of X: (101383,), shape of y: (101383,)\n",
      "Fold: 1\n",
      "Accuracy Score: 0.8017951373477339\n",
      "Confusion Matrix: \n",
      " [[8339  104 1432]\n",
      " [ 545  262  315]\n",
      " [1527   96 7657]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82      9875\n",
      "           1       0.57      0.23      0.33      1122\n",
      "           2       0.81      0.83      0.82      9280\n",
      "\n",
      "    accuracy                           0.80     20277\n",
      "   macro avg       0.73      0.63      0.66     20277\n",
      "weighted avg       0.79      0.80      0.79     20277\n",
      "\n",
      "Text: ha not slept in FOR MONTHS . Why is my body ALWAYS making me get up by 8am ? ! WHYYYYYYYYY ., Actual: 2, Predicted: 1\n",
      "Text: @ tasha_lnei Where are you ?, Actual: 2, Predicted: 0\n",
      "Text: want to watch Jigoku Shoujo live action ! !, Actual: 1, Predicted: 0\n",
      "Text: @ justroxonmute Well done Rox totally happy for you . Come over some time before the school day start dude . I miss you & lt ; 3, Actual: 2, Predicted: 0\n",
      "Text: @ LauraK1984 ahhh yay you remembered how to do the @ stuff ! I love you too !, Actual: 2, Predicted: 0\n",
      "Text: Oh your up a little early today, Actual: 2, Predicted: 0\n",
      "Text: @ WahidahNiek I know , riiight ! I miss you too, Actual: 1, Predicted: 0\n",
      "Text: @ sandoi Omg that is really irritating !, Actual: 1, Predicted: 2\n",
      "Text: @ jenali You got one ? Grr ... I have to wait until January . # iPhone3Gs, Actual: 0, Predicted: 2\n",
      "Text: is about to take a nap . I still did not find a good thumbnail program for my site, Actual: 1, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "    X, y = data['Text'].values, data['Label'].values\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y, data\n",
    "\n",
    "X, y, data = read_data('datasets/sentiment/combined.csv')\n",
    "print(label_encoder.inverse_transform([0, 1, 2]))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'size of data: {len(data)}, shape of X: {X.shape}, shape of y: {y.shape}')\n",
    "\n",
    "max_features = 7000\n",
    "ngram_range = (1, 3)\n",
    "max_depth = 4\n",
    "subsample = 0.4\n",
    "n_estimators = 1000\n",
    "learning_rate = 0.2\n",
    "common_words = []\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range, lowercase=True, stop_words= common_words)\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "\n",
    "model = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \\\n",
    "    max_depth=max_depth, n_jobs=-1, random_state=42, subsample=subsample)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "print_classification_report(y_val, y_pred, 1)\n",
    "\n",
    "misclassified = np.where(y_val != y_pred)\n",
    "misclassified_labels = y_val[misclassified]\n",
    "misclassified_predictions = y_pred[misclassified]\n",
    "misclassified_text = data['Text'].values[misclassified]\n",
    "\n",
    "indices = random.sample(range(len(misclassified_labels)), 10)\n",
    "\n",
    "for i in indices:\n",
    "    print(f'Text: {misclassified_text[i]}, Actual: {misclassified_labels[i]}, Predicted: {misclassified_predictions[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer_combined']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_model('sentiment_analysis_combined')\n",
    "joblib.dump(vectorizer, 'vectorizer_combined', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 41383, shape of X: (41383,), shape of y: (41383,)\n",
      "Fold: 1\n",
      "Accuracy Score: 0.9531208467245004\n",
      "Confusion Matrix: \n",
      " [[18347    13   954]\n",
      " [   45  1552   114]\n",
      " [  777    37 19544]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95     19314\n",
      "           1       0.97      0.91      0.94      1711\n",
      "           2       0.95      0.96      0.95     20358\n",
      "\n",
      "    accuracy                           0.95     41383\n",
      "   macro avg       0.96      0.94      0.95     41383\n",
      "weighted avg       0.95      0.95      0.95     41383\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vectorizer_combined']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, data = read_data('datasets/sentiment/combined.csv')\n",
    "print(f'size of data: {len(data)}, shape of X: {X.shape}, shape of y: {y.shape}')\n",
    "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range, lowercase=True, stop_words= common_words)\n",
    "vectorizer.fit(X)\n",
    "model = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \\\n",
    "        max_depth=max_depth, n_jobs=-1, random_state=42, subsample=subsample)\n",
    "model.fit(vectorizer.transform(X), y)\n",
    "\n",
    "y_pred = model.predict(vectorizer.transform(X))\n",
    "print_classification_report(y, y_pred, 1)\n",
    "\n",
    "model.save_model('sentiment_analysis_combined')\n",
    "joblib.dump(vectorizer, 'vectorizer_combined', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "print(label_encoder.inverse_transform([0, 1, 2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
