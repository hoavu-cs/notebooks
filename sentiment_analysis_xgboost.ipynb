{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hvutr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold,GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import warnings\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import combinations\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from tqdm import tqdm \n",
    "nltk.download('wordnet')  # Download the WordNet data\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions to print out reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "def print_regression_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print('Mean absolute error:', mean_absolute_error(y_val, y_pred))\n",
    "    print('Mean squared error:', mean_squared_error(y_val, y_pred))\n",
    "    print('Root Mean squared error:', np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "\n",
    "def print_classification_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val, y_pred)}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n",
    "\n",
    "def print_multilabel_classification_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val.argmax(axis=1), y_pred.argmax(axis=1))}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59805/101383 [00:38<00:22, 1834.15it/s]"
     ]
    }
   ],
   "source": [
    "# data set preview\n",
    "num_samples_twitter = 20000\n",
    "num_samples_amazon = 20000\n",
    "num_samples_imdb = 20000\n",
    "num_samples_mcdonald = 20000\n",
    "num_samples_instagram = 20000\n",
    "\n",
    "# process imdb data\n",
    "imdb_data = pd.read_csv('datasets/sentiment/imdb.csv', encoding='latin-1')\n",
    "imdb_data.columns = ['Text', 'Label']\n",
    "imdb_data = imdb_data.sample(num_samples_imdb)\n",
    "imdb_data = imdb_data[['Label', 'Text']]\n",
    "imdb_data['Label'] = imdb_data['Label'].replace(1, 'Positive')\n",
    "imdb_data['Label'] = imdb_data['Label'].replace(0, 'Negative')\n",
    "\n",
    "# process mcdonalds data\n",
    "mcdonalds_data = pd.read_csv('datasets/sentiment/mcdonald.csv', encoding='latin-1')\n",
    "mcdonalds_data.columns = ['Label', 'Text']\n",
    "mcdonalds_data = mcdonalds_data.sample(num_samples_mcdonald)\n",
    "\n",
    "# process twitter data\n",
    "twitter_data = pd.read_csv('datasets/sentiment/twitter.csv', encoding='latin-1', header=None)\n",
    "twitter_data.columns = ['Label', 'id', 'date', 'flag', 'user', 'Text']\n",
    "twitter_data = twitter_data.sample(num_samples_twitter)\n",
    "twitter_data = twitter_data[['Label', 'Text']]\n",
    "twitter_data['Label'] = twitter_data['Label'].replace(4, 'Positive')\n",
    "twitter_data['Label'] = twitter_data['Label'].replace(0, 'Negative')\n",
    "\n",
    "# process amazon data\n",
    "amazon_data = pd.read_csv('datasets/sentiment/amazon.csv', encoding='latin-1', header=None)\n",
    "amazon_data.columns = ['Label', 'title', 'Text']\n",
    "amazon_data = amazon_data.sample(num_samples_amazon)\n",
    "amazon_data = amazon_data[['Label', 'Text']]\n",
    "amazon_data['Label'] = amazon_data['Label'].replace(2, 'Positive')\n",
    "amazon_data['Label'] = amazon_data['Label'].replace(1, 'Negative')\n",
    "\n",
    "# process instagram data\n",
    "instagram_data = pd.read_csv('datasets/sentiment/instagram.csv', encoding='latin-1', header=None)\n",
    "instagram_data.columns = ['Text', 'Label', 'Date']\n",
    "instagram_data = instagram_data.sample(num_samples_instagram)\n",
    "instagram_data = instagram_data[['Label', 'Text']]\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('1', 'Negative')\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('2', 'Negative')\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('3', 'Neutral')\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('4', 'Positive')\n",
    "instagram_data['Label'] = instagram_data['Label'].replace('5', 'Positive')\n",
    "\n",
    "\n",
    "\n",
    "# process synthetic data\n",
    "synthetic_data = pd.read_csv('datasets/sentiment/synthetic.csv')\n",
    "combined_data = pd.concat([twitter_data, amazon_data, synthetic_data, mcdonalds_data, imdb_data, instagram_data], ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(len(combined_data))):\n",
    "    combined_data['Text'][i] = contractions.fix(combined_data['Text'][i])\n",
    "    tokens = nltk.word_tokenize(combined_data['Text'][i])\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    combined_data['Text'][i] = ' '.join(tokens)\n",
    "\n",
    "combined_data.to_csv('datasets/sentiment/combined.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive']\n",
      "size of data: 76383, shape of X: (76383,), shape of y: (76383,)\n",
      "Fold: 1\n",
      "Accuracy Score: 0.8033645349217778\n",
      "Confusion Matrix: \n",
      " [[6318   80 1078]\n",
      " [ 356  225  252]\n",
      " [1161   77 5730]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      7476\n",
      "           1       0.59      0.27      0.37       833\n",
      "           2       0.81      0.82      0.82      6968\n",
      "\n",
      "    accuracy                           0.80     15277\n",
      "   macro avg       0.74      0.65      0.67     15277\n",
      "weighted avg       0.80      0.80      0.80     15277\n",
      "\n",
      "Text: @ newsboy It is a pity people from Europe are not allowed to take part in the BBQ-competition Hope to see you guy live # Flevo Festival !, Actual: 2, Predicted: 0\n",
      "Text: @ soulsurrender oh , get a hold of : Lynda.com Joomla Creating and Editing Custom Templates , really help, Actual: 1, Predicted: 2\n",
      "Text: @ FranciscoIV and @ HoytFortenberry were great on Dallas ' show . You guy are real damn brave . Love hearing your voice . Be well & amp ; safe ., Actual: 2, Predicted: 0\n",
      "Text: ; ai said that she will be here by 4 asiiiiiiiiik, Actual: 1, Predicted: 0\n",
      "Text: Sooo , per the Doc , pulled a calf muscle which mean no jumping around for a week or so hmm ., Actual: 0, Predicted: 2\n",
      "Text: got the @ jonasbrothers new album today ! But we are not going to their concert in London tonight, Actual: 2, Predicted: 0\n",
      "Text: Playing/testing with his new toy ... waterproof iPod shuffle case . Works great thanks h2o audio for making a great product, Actual: 0, Predicted: 2\n",
      "Text: @ Pura_Candela hehe I can not wait til you get up here too ! ! life 's sucked w/o my partner in crime hahaha, Actual: 0, Predicted: 2\n",
      "Text: @ bundaolala @ tommykharisma @ de_why thanks yup uda follow ..... lam knal, Actual: 2, Predicted: 0\n",
      "Text: @ DanielKairl HAHAHA nice work man ! I doubt I will get much work done today be to busy looking at the news !, Actual: 1, Predicted: 2\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "    X, y = data['Text'].values, data['Label'].values\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y, data\n",
    "\n",
    "X, y, data = read_data('datasets/sentiment/combined.csv')\n",
    "print(label_encoder.inverse_transform([0, 1, 2]))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'size of data: {len(data)}, shape of X: {X.shape}, shape of y: {y.shape}')\n",
    "\n",
    "max_features = 4000\n",
    "ngram_range = (1, 2)\n",
    "max_depth = 3\n",
    "subsample = 0.4\n",
    "n_estimators = 1000\n",
    "learning_rate = 0.2\n",
    "common_words = []\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range, lowercase=True, stop_words= common_words)\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "\n",
    "model = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \\\n",
    "    max_depth=max_depth, n_jobs=-1, random_state=42, subsample=subsample)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "print_classification_report(y_val, y_pred, 1)\n",
    "\n",
    "misclassified = np.where(y_val != y_pred)\n",
    "misclassified_labels = y_val[misclassified]\n",
    "misclassified_predictions = y_pred[misclassified]\n",
    "misclassified_text = data['Text'].values[misclassified]\n",
    "\n",
    "indices = random.sample(range(len(misclassified_labels)), 10)\n",
    "\n",
    "for i in indices:\n",
    "    print(f'Text: {misclassified_text[i]}, Actual: {misclassified_labels[i]}, Predicted: {misclassified_predictions[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer_combined']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_model('sentiment_analysis_combined')\n",
    "joblib.dump(vectorizer, 'vectorizer_combined', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 41383, shape of X: (41383,), shape of y: (41383,)\n",
      "Fold: 1\n",
      "Accuracy Score: 0.9531208467245004\n",
      "Confusion Matrix: \n",
      " [[18347    13   954]\n",
      " [   45  1552   114]\n",
      " [  777    37 19544]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95     19314\n",
      "           1       0.97      0.91      0.94      1711\n",
      "           2       0.95      0.96      0.95     20358\n",
      "\n",
      "    accuracy                           0.95     41383\n",
      "   macro avg       0.96      0.94      0.95     41383\n",
      "weighted avg       0.95      0.95      0.95     41383\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vectorizer_combined']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, data = read_data('datasets/sentiment/combined.csv')\n",
    "print(f'size of data: {len(data)}, shape of X: {X.shape}, shape of y: {y.shape}')\n",
    "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range, lowercase=True, stop_words= common_words)\n",
    "vectorizer.fit(X)\n",
    "model = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \\\n",
    "        max_depth=max_depth, n_jobs=-1, random_state=42, subsample=subsample)\n",
    "model.fit(vectorizer.transform(X), y)\n",
    "\n",
    "y_pred = model.predict(vectorizer.transform(X))\n",
    "print_classification_report(y, y_pred, 1)\n",
    "\n",
    "model.save_model('sentiment_analysis_combined')\n",
    "joblib.dump(vectorizer, 'vectorizer_combined', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "print(label_encoder.inverse_transform([0, 1, 2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
