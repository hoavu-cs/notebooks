{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "George Orwell\n",
      "1984\n",
      "Giới Thiệu\n",
      "Kể đến những sách tố cáo chế độ cực quyền phát xít hay cộng sản, truyện &quot;1984&quot; của George Orwell là một trong những tác phẩm đứng hàng đầu. Hơn cả chục bộ khảo luận dày cộm, &quot;1984&quot; mô tả một cách xúc động rõ ràng guồng máy độc tài và thân phận hãi hùng của con người bị tước hết quyền tự do, biến thành một đám nô lệ ngoan ngoãn phục vụ một lũ cầm quyền nặc danh vô nhân đạo. George Orwell (bút hiệu của Eric Arthur Blair) sinh năm 1903 tại Ấn Độ (Motihari, Bengale). Sau khi tốt nghiệp trường Eton (1921) ông xin gia nhập Cảnh sát hoàng gia tại Miến Điện, nhưng vài năm sau (1928) ông từ chức vì chán ghét chính sách đế quốc như quyển &quot;Những ngày ở Miến Điện&quot; (1934) của ông chứng nhận. Từ đây ông cố sống với ngòi bút. Ông có kể lại mấy năm hàn vi của ông trong quyển &quot;Túng thiếu tại Paris và London&quot; (1933). Có một thời ông dạy học nhưng vì thiếu sức khỏe ông phải bỏ dạy đi giúp việc cho một tiệm sách ở ngoại ô London. Sau đ\n",
      "Vocab size: 178\n",
      "Sample dict: {'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '%': 4, '&': 5, '(': 6, ')': 7, '+': 8, ',': 9}\n",
      "Sample dict: {0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '%', 5: '&', 6: '(', 7: ')', 8: '+', 9: ','}\n"
     ]
    }
   ],
   "source": [
    "# folder_path = 'datasets/vietnamese/vietnamese/output/'\n",
    "# number_of_files = 2\n",
    "# data = []\n",
    "\n",
    "# counter = 0\n",
    "# file_names = os.listdir(folder_path)[:number_of_files]\n",
    "# for file_name in file_names:\n",
    "#     print(file_name)\n",
    "#     with open(folder_path + file_name, 'r', encoding='utf-8') as file:\n",
    "#         data.append(file.read())\n",
    "#     counter += 1\n",
    "#     if counter % 10 == 0:\n",
    "#         print(counter, 'files processed')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "filename = 'datasets/vietnamese/1984.txt'\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:1000])\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(sorted(set(text)))}\n",
    "itos = {i: ch for i, ch in enumerate(sorted(set(text)))}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print('Vocab size:', len(stoi))\n",
    "print('Sample dict:', {k: stoi[k] for k in list(stoi)[:10]})\n",
    "print('Sample dict:', {k: itos[k] for k in list(itos)[:10]})\n",
    "\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4409, 0.5591, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2975, 0.3373, 0.3652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2211, 0.2898, 0.2236, 0.2654, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1832, 0.2163, 0.1954, 0.2437, 0.1614, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1330, 0.2227, 0.1784, 0.2159, 0.1044, 0.1456, 0.0000, 0.0000],\n",
      "         [0.1283, 0.1367, 0.1385, 0.1522, 0.1083, 0.1341, 0.2021, 0.0000],\n",
      "         [0.1064, 0.1332, 0.1265, 0.1445, 0.0940, 0.1200, 0.1231, 0.1524]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4150, 0.5850, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2313, 0.3588, 0.4098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2210, 0.2636, 0.2829, 0.2324, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1771, 0.2003, 0.2343, 0.2048, 0.1834, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1298, 0.1819, 0.2228, 0.1341, 0.1501, 0.1811, 0.0000, 0.0000],\n",
      "         [0.1292, 0.1590, 0.1739, 0.1224, 0.1203, 0.1684, 0.1268, 0.0000],\n",
      "         [0.1095, 0.1434, 0.1297, 0.1340, 0.1056, 0.1316, 0.1134, 0.1328]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5500, 0.4500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2963, 0.3583, 0.3454, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1693, 0.2521, 0.3023, 0.2763, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1557, 0.2057, 0.2796, 0.2175, 0.1414, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1505, 0.1712, 0.2220, 0.1627, 0.1402, 0.1535, 0.0000, 0.0000],\n",
      "         [0.0932, 0.1432, 0.1805, 0.1377, 0.1012, 0.1751, 0.1691, 0.0000],\n",
      "         [0.0951, 0.1293, 0.1443, 0.1232, 0.0885, 0.1391, 0.1521, 0.1283]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6025, 0.3975, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3287, 0.2486, 0.4226, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3289, 0.1413, 0.2042, 0.3256, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1981, 0.1209, 0.1906, 0.2669, 0.2235, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1510, 0.1055, 0.1833, 0.2230, 0.1960, 0.1412, 0.0000, 0.0000],\n",
      "         [0.1336, 0.0655, 0.1059, 0.2171, 0.1487, 0.1250, 0.2042, 0.0000],\n",
      "         [0.1137, 0.0804, 0.1166, 0.1679, 0.1494, 0.0947, 0.1716, 0.1058]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# self attention\n",
    "B, T, C = 4, 8, 32\n",
    "torch.manual_seed(1337)\n",
    "x = torch.rand(B, T, C)\n",
    "\n",
    "# single head\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v \n",
    "\n",
    "print(wei)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4409, 0.5591, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2975, 0.3373, 0.3652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2211, 0.2898, 0.2236, 0.2654, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1832, 0.2163, 0.1954, 0.2437, 0.1614, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1330, 0.2227, 0.1784, 0.2159, 0.1044, 0.1456, 0.0000, 0.0000],\n",
      "        [0.1283, 0.1367, 0.1385, 0.1522, 0.1083, 0.1341, 0.2021, 0.0000],\n",
      "        [0.1064, 0.1332, 0.1265, 0.1445, 0.0940, 0.1200, 0.1231, 0.1524]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([1]) -> Target: tensor(33)\n",
      "Context: tensor([ 1, 33]) -> Target: tensor(56)\n",
      "Context: tensor([ 1, 33, 56]) -> Target: tensor(65)\n",
      "Context: tensor([ 1, 33, 56, 65]) -> Target: tensor(68)\n",
      "Context: tensor([ 1, 33, 56, 65, 68]) -> Target: tensor(58)\n",
      "Context: tensor([ 1, 33, 56, 65, 68, 58]) -> Target: tensor(56)\n",
      "Context: tensor([ 1, 33, 56, 65, 68, 58, 56]) -> Target: tensor(2)\n",
      "Context: tensor([ 1, 33, 56, 65, 68, 58, 56,  2]) -> Target: tensor(41)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print('Context:', context, '-> Target:', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([32, 8]) torch.int64 tensor([[ 40,  58,  52,  75,   2,  70,  68,  65],\n",
      "        [  2, 108, 114, 161,  54,   2, 108,  60],\n",
      "        [120,  71,   2,  70,  59,  97,  54,  59],\n",
      "        [ 70, 100,  60,   2,  69, 134,   2,  62],\n",
      "        [100,  64,  58,   2,  70,  68, 114, 157],\n",
      "        [  2,  70,  59,  89,  64,  59,   2,  63],\n",
      "        [ 66,   2,  64,  59, 170,  64,  58,   2],\n",
      "        [ 65,  70,  24, 107, 103,  64,  58,   9],\n",
      "        [  2,  64,  58,  65,  89,  60,   2,  63],\n",
      "        [  2,  54, 114, 156,  66,   2,  64,  59],\n",
      "        [ 66,  59,  91,  64,   2,  62,  65, 116],\n",
      "        [  2,  63, 114, 112,  60,   2, 108,  99],\n",
      "        [ 58,   9,   2,  72,  89,   2,  69, 172],\n",
      "        [ 59,  96,  64,  59,   2,  64,  59, 114],\n",
      "        [ 64,  59,  91,  64,   2,  70,  59,  71],\n",
      "        [  2, 108,  60,   2,  62,  95,  64,   2],\n",
      "        [  2,  54,  59,  65,   2,  54,  59, 103],\n",
      "        [156,  54,  11,   2,  40,  59, 114,  64],\n",
      "        [107, 118,  64,  58,   2,  70,  59, 149],\n",
      "        [ 59, 100,  64,  58,   2,  64,  59, 156],\n",
      "        [ 64,  59,  60, 138,  71,   2,  53,  60],\n",
      "        [ 71,  65,  70,  24,   2,   5,  67,  71],\n",
      "        [ 11,   2,  40,  58,  52,  75,   2,  70],\n",
      "        [ 11,   2,  38,  89,  63,   2,  69,  52],\n",
      "        [  2,  27,  64,  59,   2,  54, 111,  64],\n",
      "        [ 64,  59,   2,  62, 116,  60,   2,  72],\n",
      "        [ 89,   2,  55,  91,  64,   2,  70, 114],\n",
      "        [ 55, 162,  52,  11,   2,  48,  89,   2],\n",
      "        [ 90,  75,   2,  70,  68,  71,  75, 138],\n",
      "        [ 61,  60,  64,  59,   2,  70, 136,   2],\n",
      "        [ 65,  70,  24,  39, 149,  60,   2,  67],\n",
      "        [140,  63,   2,  63,  90,   2,  64,  89]], device='cuda:0')\n",
      "targets: torch.Size([32, 8]) torch.int64 tensor([[ 58,  52,  75,   2,  70,  68,  65,  64],\n",
      "        [108, 114, 161,  54,   2, 108,  60, 138],\n",
      "        [ 71,   2,  70,  59,  97,  54,  59,   2],\n",
      "        [100,  60,   2,  69, 134,   2,  62,  65],\n",
      "        [ 64,  58,   2,  70,  68, 114, 157,  64],\n",
      "        [ 70,  59,  89,  64,  59,   2,  63, 154],\n",
      "        [  2,  64,  59, 170,  64,  58,   2,  70],\n",
      "        [ 70,  24, 107, 103,  64,  58,   9,   5],\n",
      "        [ 64,  58,  65,  89,  60,   2,  63, 132],\n",
      "        [ 54, 114, 156,  66,   2,  64,  59, 170],\n",
      "        [ 59,  91,  64,   2,  62,  65, 116,  64],\n",
      "        [ 63, 114, 112,  60,   2, 108,  99,   9],\n",
      "        [  9,   2,  72,  89,   2,  69, 172,   2],\n",
      "        [ 96,  64,  59,   2,  64,  59, 114,   2],\n",
      "        [ 59,  91,  64,   2,  70,  59,  71, 154],\n",
      "        [108,  60,   2,  62,  95,  64,   2, 108],\n",
      "        [ 54,  59,  65,   2,  54,  59, 103,  64],\n",
      "        [ 54,  11,   2,  40,  59, 114,  64,  58],\n",
      "        [118,  64,  58,   2,  70,  59, 149,  60],\n",
      "        [100,  64,  58,   2,  64,  59, 156,   2],\n",
      "        [ 59,  60, 138,  71,   2,  53,  60,  64],\n",
      "        [ 65,  70,  24,   2,   5,  67,  71,  65],\n",
      "        [  2,  40,  58,  52,  75,   2,  70,  68],\n",
      "        [  2,  38,  89,  63,   2,  69,  52,  65],\n",
      "        [ 27,  64,  59,   2,  54, 111,  64,  58],\n",
      "        [ 59,   2,  62, 116,  60,   2,  72,  89],\n",
      "        [  2,  55,  91,  64,   2,  70, 114,   2],\n",
      "        [162,  52,  11,   2,  48,  89,   2,  70],\n",
      "        [ 75,   2,  70,  68,  71,  75, 138,  64],\n",
      "        [ 60,  64,  59,   2,  70, 136,   2,  70],\n",
      "        [ 70,  24,  39, 149,  60,   2,  67,  71],\n",
      "        [ 63,   2,  63,  90,   2,  64,  89,  64]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([train_data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([train_data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "def estimate_loss(model, split):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch(split)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:', xb.shape, xb.dtype, xb)\n",
    "print('targets:', yb.shape, yb.dtype, yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) / (C**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # perform score aggregation\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "        \n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
    "        self.sa_head = Head(n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = token_emb + position_emb\n",
    "        x = self.sa_head(x) \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self.forward(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_new = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_new], dim=-1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 5.2767252922058105\n",
      "Step: 100 Loss: 3.0445034503936768\n",
      "Step: 200 Loss: 2.763568162918091\n",
      "Step: 300 Loss: 2.3559882640838623\n",
      "Step: 400 Loss: 2.4381368160247803\n",
      "Step: 500 Loss: 2.163522720336914\n",
      "Step: 600 Loss: 2.2101356983184814\n",
      "Step: 700 Loss: 2.221740245819092\n",
      "Step: 800 Loss: 2.1643948554992676\n",
      "Step: 900 Loss: 2.165820360183716\n",
      "Step: 1000 Loss: 2.0179426670074463\n",
      "Step: 1100 Loss: 2.160309076309204\n",
      "Step: 1200 Loss: 2.1021804809570312\n",
      "Step: 1300 Loss: 2.0928797721862793\n",
      "Step: 1400 Loss: 2.329402446746826\n",
      "Step: 1500 Loss: 2.2676897048950195\n",
      "Step: 1600 Loss: 2.029362201690674\n",
      "Step: 1700 Loss: 2.024054527282715\n",
      "Step: 1800 Loss: 2.1513116359710693\n",
      "Step: 1900 Loss: 2.099259614944458\n",
      "Step: 2000 Loss: 2.261671304702759\n",
      "Step: 2100 Loss: 2.1376073360443115\n",
      "Step: 2200 Loss: 2.0060641765594482\n",
      "Step: 2300 Loss: 1.9021143913269043\n",
      "Step: 2400 Loss: 1.9812989234924316\n",
      "Step: 2500 Loss: 2.1668665409088135\n",
      "Step: 2600 Loss: 1.9057021141052246\n",
      "Step: 2700 Loss: 1.8150336742401123\n",
      "Step: 2800 Loss: 2.0869531631469727\n",
      "Step: 2900 Loss: 1.9637364149093628\n",
      "Step: 3000 Loss: 2.0816304683685303\n",
      "Step: 3100 Loss: 1.9712116718292236\n",
      "Step: 3200 Loss: 2.18680739402771\n",
      "Step: 3300 Loss: 2.238518476486206\n",
      "Step: 3400 Loss: 2.1089112758636475\n",
      "Step: 3500 Loss: 1.9972810745239258\n",
      "Step: 3600 Loss: 2.150083541870117\n",
      "Step: 3700 Loss: 1.9321762323379517\n",
      "Step: 3800 Loss: 2.091726064682007\n",
      "Step: 3900 Loss: 1.9087790250778198\n",
      "Step: 4000 Loss: 1.8696768283843994\n",
      "Step: 4100 Loss: 1.9759212732315063\n",
      "Step: 4200 Loss: 2.0141079425811768\n",
      "Step: 4300 Loss: 2.1130688190460205\n",
      "Step: 4400 Loss: 2.0169708728790283\n",
      "Step: 4500 Loss: 1.9503211975097656\n",
      "Step: 4600 Loss: 1.9460922479629517\n",
      "Step: 4700 Loss: 1.8249682188034058\n",
      "Step: 4800 Loss: 1.88654625415802\n",
      "Step: 4900 Loss: 2.0190982818603516\n",
      "Step: 5000 Loss: 1.9260376691818237\n",
      "Step: 5100 Loss: 1.9999204874038696\n",
      "Step: 5200 Loss: 2.149454355239868\n",
      "Step: 5300 Loss: 1.9476333856582642\n",
      "Step: 5400 Loss: 2.0496487617492676\n",
      "Step: 5500 Loss: 2.090756416320801\n",
      "Step: 5600 Loss: 1.9114538431167603\n",
      "Step: 5700 Loss: 2.10614275932312\n",
      "Step: 5800 Loss: 1.9804311990737915\n",
      "Step: 5900 Loss: 1.9359042644500732\n",
      "Step: 6000 Loss: 1.9148783683776855\n",
      "Step: 6100 Loss: 2.0299556255340576\n",
      "Step: 6200 Loss: 1.9458389282226562\n",
      "Step: 6300 Loss: 2.081526041030884\n",
      "Step: 6400 Loss: 1.9647200107574463\n",
      "Step: 6500 Loss: 2.10524320602417\n",
      "Step: 6600 Loss: 1.976497769355774\n",
      "Step: 6700 Loss: 2.088165521621704\n",
      "Step: 6800 Loss: 1.9164260625839233\n",
      "Step: 6900 Loss: 1.9228676557540894\n",
      "Step: 7000 Loss: 1.7902679443359375\n",
      "Step: 7100 Loss: 1.9098398685455322\n",
      "Step: 7200 Loss: 1.9866396188735962\n",
      "Step: 7300 Loss: 2.0323870182037354\n",
      "Step: 7400 Loss: 1.9803500175476074\n",
      "Step: 7500 Loss: 2.0181686878204346\n",
      "Step: 7600 Loss: 1.9688293933868408\n",
      "Step: 7700 Loss: 1.973013997077942\n",
      "Step: 7800 Loss: 1.9386906623840332\n",
      "Step: 7900 Loss: 2.096733570098877\n",
      "Step: 8000 Loss: 2.0317986011505127\n",
      "Step: 8100 Loss: 1.9903831481933594\n",
      "Step: 8200 Loss: 1.9129282236099243\n",
      "Step: 8300 Loss: 1.8005092144012451\n",
      "Step: 8400 Loss: 2.1057629585266113\n",
      "Step: 8500 Loss: 2.0293424129486084\n",
      "Step: 8600 Loss: 1.815087080001831\n",
      "Step: 8700 Loss: 1.906901240348816\n",
      "Step: 8800 Loss: 2.2037572860717773\n",
      "Step: 8900 Loss: 1.947916865348816\n",
      "Step: 9000 Loss: 2.054081678390503\n",
      "Step: 9100 Loss: 1.9132879972457886\n",
      "Step: 9200 Loss: 1.9745548963546753\n",
      "Step: 9300 Loss: 1.8758561611175537\n",
      "Step: 9400 Loss: 2.039232015609741\n",
      "Step: 9500 Loss: 2.1072568893432617\n",
      "Step: 9600 Loss: 2.039278268814087\n",
      "Step: 9700 Loss: 2.005096435546875\n",
      "Step: 9800 Loss: 2.1762609481811523\n",
      "Step: 9900 Loss: 1.8608218431472778\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 10000\n",
    "n_emb = 100\n",
    "\n",
    "m = LanguageModel().to(device)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for steps in range(n_epochs):\n",
    "    xb, yb = get_batch('train')\n",
    "    xb = xb.to(device)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print('Step:', steps, 'Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tỏi. mơi: ro p ch mộ dẹ lậy mộ t lúcũMọ n ry phọngu to, &quông đươi chúi. cáiền Ang mộthắnh màmộƯườ vớnhếnonhuo t súng c ti bàỨ/còng hưở và, lẽ từan quy hìn a hấy ng, Anghay t mụụcủanàng của ang cát Bì lạcán ti. n lênhể u dotrầnhếtrính vi ch chướ bai Cát đày c xản lài đúch\n",
      "1ềnhọayệphinó n bạiêuối vàot m ! xà kh kầu bản, c, an Mã mưan bácũ ng t củang rồi vụchí, cụthụcuậng hớiười đập lanho, dứ văm g khứng vi baonhạ tò495ẵổi anhain ngầmìnàyệcách trư traysố Nh, h, đó ng đi nhạng c có độtu c vạ điấy k\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tập 1\n",
      "\n",
      "Max length of sentence: 100\n",
      "Vocab size: 152\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "chunk_len = 100  # Change chunk length to 50\n",
    "process_data = []\n",
    "\n",
    "print(data[3])\n",
    "\n",
    "for text in data:\n",
    "    chunks = [text[i:i + chunk_len] for i in range(0, len(text), chunk_len)]\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) == chunk_len:\n",
    "            process_data.append(chunk)\n",
    "        else:\n",
    "            # Pad the remaining sequence with spaces\n",
    "            chunk += ' ' * (chunk_len - len(chunk))\n",
    "            process_data.append(chunk)\n",
    "\n",
    "tokenized_text = [list(line) for line in process_data]\n",
    "max_len = max([len(line) for line in tokenized_text])\n",
    "print('Max length of sentence:', max_len)\n",
    "\n",
    "vocab = set(char for line in tokenized_text for char in line)\n",
    "print('Vocab size:', len(vocab))\n",
    "\n",
    "char_to_id = {char: idx for idx, char in enumerate(vocab)}\n",
    "id_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "id_to_char[len(vocab)] = ' '  # Add space to the end of vocab\n",
    "\n",
    "# Create input-output pairs\n",
    "input_sequences = []\n",
    "label_sequences = []\n",
    "\n",
    "# Convert tokenized text to IDs\n",
    "tokenized_ids = [[char_to_id[char] for char in line] for line in tokenized_text]\n",
    "\n",
    "for sequence in tokenized_ids:\n",
    "    for i in range(1, len(sequence)):  # start from 1 because we need a pair (input, label)\n",
    "        input_seq = sequence[:i]\n",
    "        label_seq = sequence[1:i + 1]  # Shifted by one position to the right\n",
    "        \n",
    "        # Pad input and label sequence with zeros to match max_len\n",
    "        input_seq += [len(vocab)] * (max_len - len(input_seq))\n",
    "        label_seq += [len(vocab)] * (max_len - len(label_seq))\n",
    "        \n",
    "        input_sequences.append(input_seq)\n",
    "        label_sequences.append(label_seq)\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "input_sequences = torch.LongTensor(input_sequences)\n",
    "label_sequences = torch.LongTensor(label_sequences)\n",
    "\n",
    "# Create DataLoader for training\n",
    "batch_size = 1000\n",
    "train_data = TensorDataset(input_sequences, label_sequences)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 189981\n",
      "tensor([150,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
      "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
      "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69, 152,\n",
      "        152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152,\n",
      "        152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152,\n",
      "        152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152,\n",
      "        152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152,\n",
      "        152, 152])\n",
      "?\n",
      "Đợi Anh Đến Đông Tàn\n",
      "                                                                               \n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples:', len(input_sequences))\n",
    "print(input_sequences[40])\n",
    "print(id_to_char[103])\n",
    "test_seq = input_sequences[230].tolist()\n",
    "decode_sequence = [id_to_char[idx] for idx in test_seq]\n",
    "print(''.join(decode_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "unique_by_key: failed to synchronize: cudaErrorMemoryAllocation: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hvutr\\Git\\notebooks\\transformer_language_modeling.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs_reshaped, labels_reshaped)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m'\u001b[39m, epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unique_by_key: failed to synchronize: cudaErrorMemoryAllocation: out of memory"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = TransformerLanguageModel(vocab_size, d_model, num_heads, d_ff, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Assuming train_loader yields batches of shape [batch_size, sequence_length]\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        input_data, labels = batch\n",
    "        input_data, labels = input_data.to(device), labels.to(device)\n",
    "\n",
    "        # print(input_data.shape, labels.shape)\n",
    "\n",
    "    \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_data)\n",
    "\n",
    "        outputs_reshaped = outputs.view(-1, vocab_size)  # Reshape to [64*50, 500]\n",
    "        labels_reshaped = labels.view(-1)  # Reshape to [64*50]\n",
    "\n",
    "        \n",
    "        # print(\"Output shape:\", outputs.shape)  # Should print something like [64, 100, 110]\n",
    "        # print(\"Labels shape:\", labels.shape)  # Should print something like [64, 100]\n",
    "\n",
    "        # Reshape for the loss function\n",
    "        outputs_reshaped = outputs.view(-1, vocab_size)  # Reshape to [64*100, 110]\n",
    "        labels_reshaped = labels.view(-1)  # Reshape to [64*100]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs_reshaped, labels_reshaped)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch:', epoch+1, 'Loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đám lêu Đámám Đám lêu lêu lêu lêu lêu lêu lêu lêu lêu lêu c lêu m, lêu lêu lêu m, đi lêu lêu lêu lêu lêu\n"
     ]
    }
   ],
   "source": [
    "def generate_text(start_text, generate_length, model, char_to_id, id_to_char):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Convert starting text to tensor\n",
    "    input_text = [char_to_id[c] for c in start_text]\n",
    "    input_tensor = torch.tensor([input_text]).to(device)\n",
    "    \n",
    "    # Initialize generated text with the start_text\n",
    "    generated_text = start_text\n",
    "    \n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for _ in range(generate_length):\n",
    "            # Forward pass\n",
    "            output = model(input_tensor)\n",
    "            \n",
    "            # Get the predicted token (we take the last token here)\n",
    "            probabilities = nn.functional.softmax(output[0, -1, :], dim=0)\n",
    "            predicted_token = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            # Append predicted character to the generated text\n",
    "            generated_text += id_to_char[predicted_token]\n",
    "            \n",
    "            # Add the new token to the input sequence\n",
    "            new_input = torch.tensor([[predicted_token]]).to(device)\n",
    "            input_tensor = torch.cat([input_tensor, new_input], 1)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Define some starting text and the length of generated text\n",
    "start_text = \"Đám \"\n",
    "generate_length = 100\n",
    "\n",
    "# Assuming `model` is your trained model, and char_to_id and id_to_char are your dictionaries\n",
    "generated_text = generate_text(start_text, generate_length, model, char_to_id, id_to_char)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
