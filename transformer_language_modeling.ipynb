{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the scaled dot-product attention function with masking\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    matmul_qk = torch.matmul(query, key.transpose(-2, -1))\n",
    "    d_k = query.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (vocab_size * -1e9)\n",
    "    \n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Modify MultiHeadAttention to accept mask\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        query = self.split_heads(self.wq(query), batch_size)\n",
    "        key = self.split_heads(self.wk(key), batch_size)\n",
    "        value = self.split_heads(self.wv(value), batch_size)\n",
    "        \n",
    "        output, attention_weights = scaled_dot_product_attention(query, key, value, mask)\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Modify TransformerEncoderLayer to accept mask\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        out1 = self.layernorm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout(ffn_output))\n",
    "        \n",
    "        return out2\n",
    "\n",
    "# Modify TransformerLanguageModel to accept mask\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x, mask)\n",
    "        logits = self.decoder(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Function to create padding masks\n",
    "def create_padding_mask(seq):\n",
    "    return (seq == vocab_size).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 500\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 10000\n",
    "dropout = 0\n",
    "\n",
    "# Example usage with masks\n",
    "# model = TransformerLanguageModel(vocab_size, d_model, num_heads, d_ff, dropout)\n",
    "# Assume `input_batch` is your input tensor of shape [batch_size, seq_length]\n",
    "# input_batch = ...\n",
    "# mask = create_padding_mask(input_batch)\n",
    "# output = model(input_batch, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = 'datasets/vietnamese/vietnamese/output/'\n",
    "# number_of_files = 2\n",
    "data = []\n",
    "\n",
    "# counter = 0\n",
    "# file_names = os.listdir(folder_path)[:number_of_files]\n",
    "# for file_name in file_names:\n",
    "#     print(file_name)\n",
    "#     with open(folder_path + file_name, 'r', encoding='utf-8') as file:\n",
    "#         data.append(file.read())\n",
    "#     counter += 1\n",
    "#     if counter % 10 == 0:\n",
    "#         print(counter, 'files processed')\n",
    "\n",
    "filename = 'datasets/vietnamese/vietnamese/output/Đợi Anh Đến Đông Tàn - Đường Châu.txt'\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        data.append(line)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tập 1\n",
      "\n",
      "Max length of sentence: 100\n",
      "Vocab size: 152\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "chunk_len = 100  # Change chunk length to 50\n",
    "process_data = []\n",
    "\n",
    "print(data[3])\n",
    "\n",
    "for text in data:\n",
    "    chunks = [text[i:i + chunk_len] for i in range(0, len(text), chunk_len)]\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) == chunk_len:\n",
    "            process_data.append(chunk)\n",
    "        else:\n",
    "            # Pad the remaining sequence with spaces\n",
    "            chunk += ' ' * (chunk_len - len(chunk))\n",
    "            process_data.append(chunk)\n",
    "\n",
    "tokenized_text = [list(line) for line in process_data]\n",
    "max_len = max([len(line) for line in tokenized_text])\n",
    "print('Max length of sentence:', max_len)\n",
    "\n",
    "vocab = set(char for line in tokenized_text for char in line)\n",
    "print('Vocab size:', len(vocab))\n",
    "\n",
    "char_to_id = {char: idx for idx, char in enumerate(vocab)}\n",
    "id_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "id_to_char[len(vocab)] = ' '  # Add space to the end of vocab\n",
    "\n",
    "# Create input-output pairs\n",
    "input_sequences = []\n",
    "label_sequences = []\n",
    "\n",
    "# Convert tokenized text to IDs\n",
    "tokenized_ids = [[char_to_id[char] for char in line] for line in tokenized_text]\n",
    "\n",
    "for sequence in tokenized_ids:\n",
    "    for i in range(1, len(sequence)):  # start from 1 because we need a pair (input, label)\n",
    "        input_seq = sequence[:i]\n",
    "        label_seq = sequence[1:i + 1]  # Shifted by one position to the right\n",
    "        \n",
    "        # Pad input and label sequence with zeros to match max_len\n",
    "        input_seq += [len(vocab)] * (max_len - len(input_seq))\n",
    "        label_seq += [len(vocab)] * (max_len - len(label_seq))\n",
    "        \n",
    "        input_sequences.append(input_seq)\n",
    "        label_sequences.append(label_seq)\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "input_sequences = torch.LongTensor(input_sequences)\n",
    "label_sequences = torch.LongTensor(label_sequences)\n",
    "\n",
    "# Create DataLoader for training\n",
    "batch_size = 1000\n",
    "train_data = TensorDataset(input_sequences, label_sequences)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 189981\n",
      "tensor([150,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
      "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,\n",
      "         69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69,  69, 152,\n",
      "        152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152,\n",
      "        152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152,\n",
      "        152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152,\n",
      "        152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152, 152,\n",
      "        152, 152])\n",
      "?\n",
      "Đợi Anh Đến Đông Tàn\n",
      "                                                                               \n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples:', len(input_sequences))\n",
    "print(input_sequences[40])\n",
    "print(id_to_char[103])\n",
    "test_seq = input_sequences[230].tolist()\n",
    "decode_sequence = [id_to_char[idx] for idx in test_seq]\n",
    "print(''.join(decode_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "unique_by_key: failed to synchronize: cudaErrorMemoryAllocation: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hvutr\\Git\\notebooks\\transformer_language_modeling.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs_reshaped, labels_reshaped)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hvutr/Git/notebooks/transformer_language_modeling.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m'\u001b[39m, epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unique_by_key: failed to synchronize: cudaErrorMemoryAllocation: out of memory"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = TransformerLanguageModel(vocab_size, d_model, num_heads, d_ff, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Assuming train_loader yields batches of shape [batch_size, sequence_length]\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        input_data, labels = batch\n",
    "        input_data, labels = input_data.to(device), labels.to(device)\n",
    "\n",
    "        # print(input_data.shape, labels.shape)\n",
    "\n",
    "    \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_data)\n",
    "\n",
    "        outputs_reshaped = outputs.view(-1, vocab_size)  # Reshape to [64*50, 500]\n",
    "        labels_reshaped = labels.view(-1)  # Reshape to [64*50]\n",
    "\n",
    "        \n",
    "        # print(\"Output shape:\", outputs.shape)  # Should print something like [64, 100, 110]\n",
    "        # print(\"Labels shape:\", labels.shape)  # Should print something like [64, 100]\n",
    "\n",
    "        # Reshape for the loss function\n",
    "        outputs_reshaped = outputs.view(-1, vocab_size)  # Reshape to [64*100, 110]\n",
    "        labels_reshaped = labels.view(-1)  # Reshape to [64*100]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs_reshaped, labels_reshaped)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch:', epoch+1, 'Loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đám lêu Đámám Đám lêu lêu lêu lêu lêu lêu lêu lêu lêu lêu c lêu m, lêu lêu lêu m, đi lêu lêu lêu lêu lêu\n"
     ]
    }
   ],
   "source": [
    "def generate_text(start_text, generate_length, model, char_to_id, id_to_char):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Convert starting text to tensor\n",
    "    input_text = [char_to_id[c] for c in start_text]\n",
    "    input_tensor = torch.tensor([input_text]).to(device)\n",
    "    \n",
    "    # Initialize generated text with the start_text\n",
    "    generated_text = start_text\n",
    "    \n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for _ in range(generate_length):\n",
    "            # Forward pass\n",
    "            output = model(input_tensor)\n",
    "            \n",
    "            # Get the predicted token (we take the last token here)\n",
    "            probabilities = nn.functional.softmax(output[0, -1, :], dim=0)\n",
    "            predicted_token = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            # Append predicted character to the generated text\n",
    "            generated_text += id_to_char[predicted_token]\n",
    "            \n",
    "            # Add the new token to the input sequence\n",
    "            new_input = torch.tensor([[predicted_token]]).to(device)\n",
    "            input_tensor = torch.cat([input_tensor, new_input], 1)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Define some starting text and the length of generated text\n",
    "start_text = \"Đám \"\n",
    "generate_length = 100\n",
    "\n",
    "# Assuming `model` is your trained model, and char_to_id and id_to_char are your dictionaries\n",
    "generated_text = generate_text(start_text, generate_length, model, char_to_id, id_to_char)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
