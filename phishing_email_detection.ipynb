{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    T_vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "    X = data['Email Text'].values\n",
    "    y = data['Email Type'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 'Phishing Email':\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "\n",
    "    X = T_vectorizer.fit_transform(X)\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def print_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val, y_pred)}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                         Email Text  \\\n",
      "0           0  re : 6 . 1100 , disc : uniformitarianism , re ...   \n",
      "1           1  the other side of * galicismos * * galicismo *...   \n",
      "2           2  re : equistar deal tickets are you still avail...   \n",
      "3           3  \\nHello I am your hot lil horny toy.\\n    I am...   \n",
      "4           4  software at incredibly low prices ( 86 % lower...   \n",
      "\n",
      "       Email Type  \n",
      "0      Safe Email  \n",
      "1      Safe Email  \n",
      "2      Safe Email  \n",
      "3  Phishing Email  \n",
      "4  Phishing Email  \n",
      "Safe Email        11322\n",
      "Phishing Email     7328\n",
      "Name: Email Type, dtype: int64\n",
      "(1, 10000) (18634,)\n"
     ]
    }
   ],
   "source": [
    "# data set preview\n",
    "data = pd.read_csv('datasets/phishing_emails/phishing_email.csv')\n",
    "print(data.head())\n",
    "print(data['Email Type'].value_counts())\n",
    "\n",
    "# X, y = read_data('datasets/phishing_emails/phishing_email.csv')\n",
    "# print(X[0].shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18634, 5000) (18634,)\n"
     ]
    }
   ],
   "source": [
    "X, y = read_data('datasets/phishing_emails/phishing_email.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9659243359270191\n",
      "Confusion Matrix: \n",
      " [[2109  100]\n",
      " [  27 1491]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      2209\n",
      "           1       0.94      0.98      0.96      1518\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9691440837134424\n",
      "Confusion Matrix: \n",
      " [[2198   81]\n",
      " [  34 1414]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2279\n",
      "           1       0.95      0.98      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.97      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.9643144620338073\n",
      "Confusion Matrix: \n",
      " [[2171   89]\n",
      " [  44 1423]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2260\n",
      "           1       0.94      0.97      0.96      1467\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9718272068687953\n",
      "Confusion Matrix: \n",
      " [[2250   82]\n",
      " [  23 1372]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      2332\n",
      "           1       0.94      0.98      0.96      1395\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.97      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9691358024691358\n",
      "Confusion Matrix: \n",
      " [[2156   86]\n",
      " [  29 1455]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      2242\n",
      "           1       0.94      0.98      0.96      1484\n",
      "\n",
      "    accuracy                           0.97      3726\n",
      "   macro avg       0.97      0.97      0.97      3726\n",
      "weighted avg       0.97      0.97      0.97      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  model = XGBClassifier(n_estimators=200, learning_rate=0.5, max_depth=4, colsample_bytree=0.5, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.951703783203649\n",
      "Confusion Matrix: \n",
      " [[2089  120]\n",
      " [  60 1458]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      2209\n",
      "           1       0.92      0.96      0.94      1518\n",
      "\n",
      "    accuracy                           0.95      3727\n",
      "   macro avg       0.95      0.95      0.95      3727\n",
      "weighted avg       0.95      0.95      0.95      3727\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m tree \u001b[39m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, max_features\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     13\u001b[0m model \u001b[39m=\u001b[39m AdaBoostClassifier(base_estimator\u001b[39m=\u001b[39mtree, n_estimators\u001b[39m=\u001b[39m\u001b[39m600\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     15\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_val)\n\u001b[0;32m     17\u001b[0m print_report(y_val, y_pred, fold)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:506\u001b[0m, in \u001b[0;36mAdaBoostClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    501\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAlgorithm must be \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSAMME\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSAMME.R\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    505\u001b[0m \u001b[39m# Fit\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:160\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    156\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[0;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m iboost \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators):\n\u001b[0;32m    159\u001b[0m     \u001b[39m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m     sample_weight, estimator_weight, estimator_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_boost(\n\u001b[0;32m    161\u001b[0m         iboost, X, y, sample_weight, random_state\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Early termination\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:568\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m \u001b[39mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[39m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAMME.R\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 568\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_boost_real(iboost, X, y, sample_weight, random_state)\n\u001b[0;32m    570\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:577\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m--> 577\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    579\u001b[0m y_predict_proba \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    581\u001b[0m \u001b[39mif\u001b[39;00m iboost \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# k-fold experiment using Adaboost with Decision Tree as base estimator\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  tree = DecisionTreeClassifier(max_depth=4, max_features=500, random_state=42)\n",
    "  model = AdaBoostClassifier(base_estimator=tree, n_estimators=400, random_state=42, learning_rate=0.1)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold experiment using random forest\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  model = RandomForestClassifier(n_estimators=400, max_features=1000, random_state=42, n_jobs=-1)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting tree\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.4, max_depth=3, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
