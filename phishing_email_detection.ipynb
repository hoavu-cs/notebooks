{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    T_vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "    X = data['Email Text'].values\n",
    "    y = data['Email Type'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 'Phishing Email':\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def print_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val, y_pred)}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                         Email Text  \\\n",
      "0           0  re : 6 . 1100 , disc : uniformitarianism , re ...   \n",
      "1           1  the other side of * galicismos * * galicismo *...   \n",
      "2           2  re : equistar deal tickets are you still avail...   \n",
      "3           3  \\r\\nHello I am your hot lil horny toy.\\r\\n    ...   \n",
      "4           4  software at incredibly low prices ( 86 % lower...   \n",
      "\n",
      "       Email Type  \n",
      "0      Safe Email  \n",
      "1      Safe Email  \n",
      "2      Safe Email  \n",
      "3  Phishing Email  \n",
      "4  Phishing Email  \n",
      "Safe Email        11322\n",
      "Phishing Email     7328\n",
      "Name: Email Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# data set preview\n",
    "data = pd.read_csv('datasets/phishing_emails/phishing_email.csv')\n",
    "print(data.head())\n",
    "print(data['Email Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18634,) (18634,)\n"
     ]
    }
   ],
   "source": [
    "X, y = read_data('datasets/phishing_emails/phishing_email.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.967802522135766\n",
      "Confusion Matrix: \n",
      " [[2112   97]\n",
      " [  23 1495]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      2209\n",
      "           1       0.94      0.98      0.96      1518\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9704856452911189\n",
      "Confusion Matrix: \n",
      " [[2196   83]\n",
      " [  27 1421]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      2279\n",
      "           1       0.94      0.98      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.97      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.964851086664878\n",
      "Confusion Matrix: \n",
      " [[2174   86]\n",
      " [  45 1422]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2260\n",
      "           1       0.94      0.97      0.96      1467\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.96      0.96      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9742420177086129\n",
      "Confusion Matrix: \n",
      " [[2253   79]\n",
      " [  17 1378]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      2332\n",
      "           1       0.95      0.99      0.97      1395\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.97      0.98      0.97      3727\n",
      "weighted avg       0.98      0.97      0.97      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9688674181427804\n",
      "Confusion Matrix: \n",
      " [[2156   86]\n",
      " [  30 1454]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      2242\n",
      "           1       0.94      0.98      0.96      1484\n",
      "\n",
      "    accuracy                           0.97      3726\n",
      "   macro avg       0.97      0.97      0.97      3726\n",
      "weighted avg       0.97      0.97      0.97      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 3), max_features=10000)\n",
    "  vectorizer.fit(X_train)\n",
    "\n",
    "  X_train = vectorizer.transform(X_train)\n",
    "  X_val = vectorizer.transform(X_val)\n",
    "  \n",
    "  model = XGBClassifier(n_estimators=800, learning_rate=0.1, max_depth=4, colsample_bytree=0.2, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9632412127716662\n",
      "Confusion Matrix: \n",
      " [[2107  102]\n",
      " [  35 1483]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97      2209\n",
      "           1       0.94      0.98      0.96      1518\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9694123960289778\n",
      "Confusion Matrix: \n",
      " [[2201   78]\n",
      " [  36 1412]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      2279\n",
      "           1       0.95      0.98      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.97      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.9640461497182721\n",
      "Confusion Matrix: \n",
      " [[2168   92]\n",
      " [  42 1425]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2260\n",
      "           1       0.94      0.97      0.96      1467\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9694123960289778\n",
      "Confusion Matrix: \n",
      " [[2250   82]\n",
      " [  32 1363]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      2332\n",
      "           1       0.94      0.98      0.96      1395\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.966988727858293\n",
      "Confusion Matrix: \n",
      " [[2157   85]\n",
      " [  38 1446]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2242\n",
      "           1       0.94      0.97      0.96      1484\n",
      "\n",
      "    accuracy                           0.97      3726\n",
      "   macro avg       0.96      0.97      0.97      3726\n",
      "weighted avg       0.97      0.97      0.97      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold experiment using Adaboost with Decision Tree as base estimator\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 3), max_features=10000)\n",
    "  vectorizer.fit(X_train)\n",
    "\n",
    "  X_train = vectorizer.transform(X_train)\n",
    "  X_val = vectorizer.transform(X_val)\n",
    "\n",
    "  tree = DecisionTreeClassifier(max_depth=3, max_features=0.2, random_state=42)\n",
    "  model = AdaBoostClassifier(base_estimator=tree, n_estimators=800, random_state=42, learning_rate=0.1)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9581432787764959\n",
      "Confusion Matrix: \n",
      " [[2100  109]\n",
      " [  47 1471]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96      2209\n",
      "           1       0.93      0.97      0.95      1518\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.95      0.96      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9557284679366783\n",
      "Confusion Matrix: \n",
      " [[2175  104]\n",
      " [  61 1387]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      2279\n",
      "           1       0.93      0.96      0.94      1448\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.95      0.96      0.95      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.9549235309900724\n",
      "Confusion Matrix: \n",
      " [[2167   93]\n",
      " [  75 1392]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      2260\n",
      "           1       0.94      0.95      0.94      1467\n",
      "\n",
      "    accuracy                           0.95      3727\n",
      "   macro avg       0.95      0.95      0.95      3727\n",
      "weighted avg       0.96      0.95      0.95      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9605580896163134\n",
      "Confusion Matrix: \n",
      " [[2240   92]\n",
      " [  55 1340]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2332\n",
      "           1       0.94      0.96      0.95      1395\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.96      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9567901234567902\n",
      "Confusion Matrix: \n",
      " [[2136  106]\n",
      " [  55 1429]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      2242\n",
      "           1       0.93      0.96      0.95      1484\n",
      "\n",
      "    accuracy                           0.96      3726\n",
      "   macro avg       0.95      0.96      0.96      3726\n",
      "weighted avg       0.96      0.96      0.96      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold experiment using random forest\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 3), max_features=10000)\n",
    "  vectorizer.fit(X_train)\n",
    "\n",
    "  X_train = vectorizer.transform(X_train)\n",
    "  X_val = vectorizer.transform(X_val)\n",
    "\n",
    "  model = RandomForestClassifier(n_estimators=500, max_features=0.15, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9659243359270191\n",
      "Confusion Matrix: \n",
      " [[2116   93]\n",
      " [  34 1484]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2209\n",
      "           1       0.94      0.98      0.96      1518\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9659243359270191\n",
      "Confusion Matrix: \n",
      " [[2199   80]\n",
      " [  47 1401]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2279\n",
      "           1       0.95      0.97      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.9643144620338073\n",
      "Confusion Matrix: \n",
      " [[2183   77]\n",
      " [  56 1411]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      2260\n",
      "           1       0.95      0.96      0.95      1467\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.96      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9669975851891602\n",
      "Confusion Matrix: \n",
      " [[2253   79]\n",
      " [  44 1351]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      2332\n",
      "           1       0.94      0.97      0.96      1395\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9634997316156736\n",
      "Confusion Matrix: \n",
      " [[2148   94]\n",
      " [  42 1442]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2242\n",
      "           1       0.94      0.97      0.95      1484\n",
      "\n",
      "    accuracy                           0.96      3726\n",
      "   macro avg       0.96      0.96      0.96      3726\n",
      "weighted avg       0.96      0.96      0.96      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting tree\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 3), max_features=10000)\n",
    "  vectorizer.fit(X_train)\n",
    "\n",
    "  X_train = vectorizer.transform(X_train)\n",
    "  X_val = vectorizer.transform(X_val)\n",
    "\n",
    "  model = GradientBoostingClassifier(n_estimators=500, learning_rate=0.5, max_depth=3, random_state=42, max_features=0.15)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9618996511939898\n",
      "Confusion Matrix: \n",
      " [[2095  114]\n",
      " [  28 1490]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      2209\n",
      "           1       0.93      0.98      0.95      1518\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.96      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n",
      "Accuracy Score: 0.9653877112959485\n",
      "Confusion Matrix: \n",
      " [[2196   83]\n",
      " [  46 1402]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2279\n",
      "           1       0.94      0.97      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m y_train, y_val \u001b[39m=\u001b[39m y[train_index], y[val_index]\n\u001b[0;32m     19\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(sublinear_tf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m), max_features\u001b[39m=\u001b[39m\u001b[39m15000\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m vectorizer\u001b[39m.\u001b[39;49mfit(X_train)\n\u001b[0;32m     22\u001b[0m X_train \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(X_train)\n\u001b[0;32m     23\u001b[0m X_val \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(X_val)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2049\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2044\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2045\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2046\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2047\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2048\u001b[0m )\n\u001b[1;32m-> 2049\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2050\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2051\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:118\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    116\u001b[0m             doc \u001b[39m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    117\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m             doc \u001b[39m=\u001b[39m ngrams(doc)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:258\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 258\u001b[0m n_original_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(original_tokens)\n\u001b[0;32m    260\u001b[0m \u001b[39m# bind method outside of loop to reduce overhead\u001b[39;00m\n\u001b[0;32m    261\u001b[0m tokens_append \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mappend\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.2,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.9,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 4), max_features=15000)\n",
    "    vectorizer.fit(X_train)\n",
    "\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "    X_val = vectorizer.transform(X_val)\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(params, train_data)\n",
    "    \n",
    "    y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    y_pred = np.round(y_pred)\n",
    "\n",
    "    print_report(y_val, y_pred, fold)\n",
    "    fold += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
