{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> 81a7d909d81665fc74fe3c4598513a615c131518
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    T_vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "    X = data['Email Text'].values\n",
    "    y = data['Email Type'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 'Phishing Email':\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def print_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val, y_pred)}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                         Email Text  \\\n",
      "0           0  re : 6 . 1100 , disc : uniformitarianism , re ...   \n",
      "1           1  the other side of * galicismos * * galicismo *...   \n",
      "2           2  re : equistar deal tickets are you still avail...   \n",
      "3           3  \\nHello I am your hot lil horny toy.\\n    I am...   \n",
      "4           4  software at incredibly low prices ( 86 % lower...   \n",
      "\n",
      "       Email Type  \n",
      "0      Safe Email  \n",
      "1      Safe Email  \n",
      "2      Safe Email  \n",
      "3  Phishing Email  \n",
      "4  Phishing Email  \n",
      "Safe Email        11322\n",
      "Phishing Email     7328\n",
      "Name: Email Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# data set preview\n",
    "data = pd.read_csv('datasets/phishing_emails/phishing_email.csv')\n",
    "print(data.head())\n",
    "print(data['Email Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 3,
>>>>>>> 81a7d909d81665fc74fe3c4598513a615c131518
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18634,) (18634,)\n"
     ]
    }
   ],
   "source": [
    "X, y = read_data('datasets/phishing_emails/phishing_email.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 19,
>>>>>>> 81a7d909d81665fc74fe3c4598513a615c131518
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9629729004561309\n",
      "Confusion Matrix: \n",
<<<<<<< HEAD
      " [[2112   97]\n",
      " [  23 1495]]\n",
=======
      " [[2098  111]\n",
      " [  27 1491]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      2209\n",
      "           1       0.93      0.98      0.96      1518\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9653877112959485\n",
      "Confusion Matrix: \n",
      " [[2190   89]\n",
      " [  40 1408]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2279\n",
      "           1       0.94      0.97      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.9610947142473839\n",
      "Confusion Matrix: \n",
      " [[2165   95]\n",
      " [  50 1417]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2260\n",
      "           1       0.94      0.97      0.95      1467\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.96      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9669975851891602\n",
      "Confusion Matrix: \n",
      " [[2237   95]\n",
      " [  28 1367]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      2332\n",
      "           1       0.94      0.98      0.96      1395\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9656468062265163\n",
      "Confusion Matrix: \n",
      " [[2146   96]\n",
      " [  32 1452]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      2242\n",
      "           1       0.94      0.98      0.96      1484\n",
      "\n",
      "    accuracy                           0.97      3726\n",
      "   macro avg       0.96      0.97      0.96      3726\n",
      "weighted avg       0.97      0.97      0.97      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  model = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=4, colsample_bytree=0.2, n_jobs=-1, random_state=42)\n",
    "  #model = DecisionTreeClassifier(random_state=42, max_depth=4)\n",
    "  #model = RandomForestClassifier(n_estimators=400, max_depth=25, random_state=42, n_jobs=-1, verbose=1)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9675342098202308\n",
      "Confusion Matrix: \n",
      " [[2113   96]\n",
      " [  25 1493]]\n",
>>>>>>> 81a7d909d81665fc74fe3c4598513a615c131518
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      2209\n",
      "           1       0.94      0.98      0.96      1518\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9704856452911189\n",
      "Confusion Matrix: \n",
      " [[2196   83]\n",
      " [  27 1421]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      2279\n",
      "           1       0.94      0.98      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.97      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.964851086664878\n",
      "Confusion Matrix: \n",
      " [[2174   86]\n",
      " [  45 1422]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2260\n",
      "           1       0.94      0.97      0.96      1467\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.96      0.96      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9742420177086129\n",
      "Confusion Matrix: \n",
      " [[2253   79]\n",
      " [  17 1378]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      2332\n",
      "           1       0.95      0.99      0.97      1395\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.97      0.98      0.97      3727\n",
      "weighted avg       0.98      0.97      0.97      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9688674181427804\n",
      "Confusion Matrix: \n",
      " [[2156   86]\n",
      " [  30 1454]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      2242\n",
      "           1       0.94      0.98      0.96      1484\n",
      "\n",
      "    accuracy                           0.97      3726\n",
      "   macro avg       0.97      0.97      0.97      3726\n",
      "weighted avg       0.97      0.97      0.97      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 3), max_features=10000)\n",
    "  vectorizer.fit(X_train)\n",
    "\n",
    "  X_train = vectorizer.transform(X_train)\n",
    "  X_val = vectorizer.transform(X_val)\n",
    "  \n",
    "  model = XGBClassifier(n_estimators=800, learning_rate=0.1, max_depth=4, colsample_bytree=0.2, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9632412127716662\n",
      "Confusion Matrix: \n",
      " [[2107  102]\n",
      " [  35 1483]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97      2209\n",
      "           1       0.94      0.98      0.96      1518\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9694123960289778\n",
      "Confusion Matrix: \n",
      " [[2201   78]\n",
      " [  36 1412]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      2279\n",
      "           1       0.95      0.98      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.97      0.97      0.97      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold experiment using Adaboost with Decision Tree as base estimator\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 3), max_features=10000)\n",
    "  vectorizer.fit(X_train)\n",
    "\n",
    "  X_train = vectorizer.transform(X_train)\n",
    "  X_val = vectorizer.transform(X_val)\n",
    "\n",
    "  tree = DecisionTreeClassifier(max_depth=3, max_features=0.2, random_state=42)\n",
    "  model = AdaBoostClassifier(base_estimator=tree, n_estimators=800, random_state=42, learning_rate=0.1)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9581432787764959\n",
      "Confusion Matrix: \n",
      " [[2100  109]\n",
      " [  47 1471]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96      2209\n",
      "           1       0.93      0.97      0.95      1518\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.95      0.96      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9557284679366783\n",
      "Confusion Matrix: \n",
      " [[2175  104]\n",
      " [  61 1387]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      2279\n",
      "           1       0.93      0.96      0.94      1448\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.95      0.96      0.95      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.9549235309900724\n",
      "Confusion Matrix: \n",
      " [[2167   93]\n",
      " [  75 1392]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      2260\n",
      "           1       0.94      0.95      0.94      1467\n",
      "\n",
      "    accuracy                           0.95      3727\n",
      "   macro avg       0.95      0.95      0.95      3727\n",
      "weighted avg       0.96      0.95      0.95      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9605580896163134\n",
      "Confusion Matrix: \n",
      " [[2240   92]\n",
      " [  55 1340]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2332\n",
      "           1       0.94      0.96      0.95      1395\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.96      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9567901234567902\n",
      "Confusion Matrix: \n",
      " [[2136  106]\n",
      " [  55 1429]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      2242\n",
      "           1       0.93      0.96      0.95      1484\n",
      "\n",
      "    accuracy                           0.96      3726\n",
      "   macro avg       0.95      0.96      0.96      3726\n",
      "weighted avg       0.96      0.96      0.96      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold experiment using random forest\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 3), max_features=10000)\n",
    "  vectorizer.fit(X_train)\n",
    "\n",
    "  X_train = vectorizer.transform(X_train)\n",
    "  X_val = vectorizer.transform(X_val)\n",
    "\n",
    "  model = RandomForestClassifier(n_estimators=500, max_features=0.15, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9659243359270191\n",
      "Confusion Matrix: \n",
      " [[2116   93]\n",
      " [  34 1484]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2209\n",
      "           1       0.94      0.98      0.96      1518\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9659243359270191\n",
      "Confusion Matrix: \n",
      " [[2199   80]\n",
      " [  47 1401]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2279\n",
      "           1       0.95      0.97      0.96      1448\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.9643144620338073\n",
      "Confusion Matrix: \n",
      " [[2183   77]\n",
      " [  56 1411]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      2260\n",
      "           1       0.95      0.96      0.95      1467\n",
      "\n",
      "    accuracy                           0.96      3727\n",
      "   macro avg       0.96      0.96      0.96      3727\n",
      "weighted avg       0.96      0.96      0.96      3727\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9669975851891602\n",
      "Confusion Matrix: \n",
      " [[2253   79]\n",
      " [  44 1351]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      2332\n",
      "           1       0.94      0.97      0.96      1395\n",
      "\n",
      "    accuracy                           0.97      3727\n",
      "   macro avg       0.96      0.97      0.96      3727\n",
      "weighted avg       0.97      0.97      0.97      3727\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9634997316156736\n",
      "Confusion Matrix: \n",
      " [[2148   94]\n",
      " [  42 1442]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2242\n",
      "           1       0.94      0.97      0.95      1484\n",
      "\n",
      "    accuracy                           0.96      3726\n",
      "   macro avg       0.96      0.96      0.96      3726\n",
      "weighted avg       0.96      0.96      0.96      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting tree\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 3), max_features=10000)\n",
    "  vectorizer.fit(X_train)\n",
    "\n",
    "  X_train = vectorizer.transform(X_train)\n",
    "  X_val = vectorizer.transform(X_val)\n",
    "\n",
    "  model = GradientBoostingClassifier(n_estimators=500, learning_rate=0.5, max_depth=3, random_state=42, max_features=0.15)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_report(y_val, y_pred, fold)\n",
    "  fold += 1  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
