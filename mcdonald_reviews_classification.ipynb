{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import warnings\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions to print out reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "def print_regression_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print('Mean absolute error:', mean_absolute_error(y_val, y_pred))\n",
    "    print('Mean squared error:', mean_squared_error(y_val, y_pred))\n",
    "    print('Root Mean squared error:', np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "\n",
    "def print_classification_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val, y_pred)}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n",
    "\n",
    "def print_multilabel_classification_report(y_val, y_pred, fold):\n",
    "    print(f'Fold: {fold}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_val, y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n {confusion_matrix(y_val.argmax(axis=1), y_pred.argmax(axis=1))}')\n",
    "    print(f'Classification Report: \\n {classification_report(y_val, y_pred)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   reviewer_id  store_name              category  \\\n",
      "0            1  McDonald's  Fast food restaurant   \n",
      "\n",
      "                                       store_address  latitude   longitude  \\\n",
      "0  13749 US-183 Hwy, Austin, TX 78750, United States  30.460718 -97.792874   \n",
      "\n",
      "  rating_count   review_time  \\\n",
      "0        1,240  3 months ago   \n",
      "\n",
      "                                              review  rating  \n",
      "0  Why does it look like someone spit on my food?...  1 star  \n",
      "reviewer_id        int64\n",
      "store_name        object\n",
      "category          object\n",
      "store_address     object\n",
      "latitude         float64\n",
      "longitude        float64\n",
      "rating_count      object\n",
      "review_time       object\n",
      "review            object\n",
      "rating            object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAG+CAYAAABxthWQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqPElEQVR4nO3df3RU9Z3/8deEkB/8mAk/NglZA8RChQAFIYpRZFVSgmQVkN0VCcIqlUqTbpEKS1YaAbGRKAgIS6RWA6dQwD2FImgkJgIrhASDyA/51RolLk6wDckISAjJfP+wuV+noBWbcGc+83ycc8/x3s9n7rxv3l7z8ubeGYfX6/UKAADAMCF2FwAAANASCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYKtbsAOzU2NurUqVNq3769HA6H3eUAAIBvwev16vPPP1dcXJxCQr7+ek1Qh5xTp04pPj7e7jIAAMB3UFlZqeuuu+5rx4M65LRv317Slz8kp9NpczUAAODb8Hg8io+Pt36Pf52gDjlNf6JyOp2EHAAAAszfutWEG48BAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARgq1uwDTdZ+11e4SmsVHz6TZXQIAAFeFKzkAAMBIhBwAAGAkQg4AADASIQcAABjpqkPOzp07dc899yguLk4Oh0ObNm3yGfd6vcrOzlaXLl0UGRmplJQUnThxwmdOdXW10tPT5XQ6FRUVpcmTJ+vs2bM+cw4cOKDbb79dERERio+PV25u7mW1vPrqq+rVq5ciIiLUr18/vf7661d7OAAAwFBXHXLOnTun/v37a/ny5Vccz83N1dKlS5WXl6fS0lK1bdtWqampunDhgjUnPT1dhw8fVmFhobZs2aKdO3dqypQp1rjH49Hw4cPVrVs3lZeX69lnn9WcOXO0cuVKa87u3bv1wAMPaPLkyXrvvfc0evRojR49WocOHbraQwIAAAZyeL1e73d+scOhjRs3avTo0ZK+vIoTFxenn//853r88cclSbW1tYqJiVF+fr7GjRunI0eOKDExUXv37lVSUpIkqaCgQCNHjtQnn3yiuLg4rVixQk888YTcbrfCwsIkSbNmzdKmTZt09OhRSdL999+vc+fOacuWLVY9t9xyiwYMGKC8vLxvVb/H45HL5VJtba2cTud3/TF8Ix4hBwCgeX3b39/Nek9ORUWF3G63UlJSrG0ul0uDBw9WSUmJJKmkpERRUVFWwJGklJQUhYSEqLS01JozdOhQK+BIUmpqqo4dO6YzZ85Yc776Pk1zmt7nSurq6uTxeHwWAABgpmYNOW63W5IUExPjsz0mJsYac7vdio6O9hkPDQ1Vx44dfeZcaR9ffY+vm9M0fiU5OTlyuVzWEh8ff7WHCAAAAkRQPV2VlZWl2tpaa6msrLS7JAAA0EKaNeTExsZKkqqqqny2V1VVWWOxsbE6ffq0z/ilS5dUXV3tM+dK+/jqe3zdnKbxKwkPD5fT6fRZAACAmZo15CQkJCg2NlZFRUXWNo/Ho9LSUiUnJ0uSkpOTVVNTo/LycmtOcXGxGhsbNXjwYGvOzp07VV9fb80pLCzUDTfcoA4dOlhzvvo+TXOa3gcAAAS3qw45Z8+e1f79+7V//35JX95svH//fp08eVIOh0PTpk3T/PnztXnzZh08eFATJ05UXFyc9QRW7969NWLECD3yyCMqKyvTrl27lJmZqXHjxikuLk6SNH78eIWFhWny5Mk6fPiw1q9fryVLlmj69OlWHT/72c9UUFCghQsX6ujRo5ozZ47effddZWZm/v0/FQAAEPCu+lvI3333Xd15553WelPwmDRpkvLz8zVz5kydO3dOU6ZMUU1NjYYMGaKCggJFRERYr1mzZo0yMzM1bNgwhYSEaOzYsVq6dKk17nK5tG3bNmVkZGjQoEHq3LmzsrOzfT5L59Zbb9XatWs1e/Zs/dd//Zd69uypTZs2qW/fvt/pBwEAAMzyd31OTqDjc3K+PT4nBwDgL2z5nBwAAAB/QcgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEihdhcAXEsmfCs83wgPAN8OV3IAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwUrOHnIaGBv3iF79QQkKCIiMj9b3vfU9PPfWUvF6vNcfr9So7O1tdunRRZGSkUlJSdOLECZ/9VFdXKz09XU6nU1FRUZo8ebLOnj3rM+fAgQO6/fbbFRERofj4eOXm5jb34QAAgADV7CFnwYIFWrFihZYtW6YjR45owYIFys3N1QsvvGDNyc3N1dKlS5WXl6fS0lK1bdtWqampunDhgjUnPT1dhw8fVmFhobZs2aKdO3dqypQp1rjH49Hw4cPVrVs3lZeX69lnn9WcOXO0cuXK5j4kAAAQgEKbe4e7d+/WqFGjlJaWJknq3r27fvvb36qsrEzSl1dxFi9erNmzZ2vUqFGSpNWrVysmJkabNm3SuHHjdOTIERUUFGjv3r1KSkqSJL3wwgsaOXKknnvuOcXFxWnNmjW6ePGiXn75ZYWFhalPnz7av3+/Fi1a5BOGAABAcGr2Kzm33nqrioqKdPz4cUnS+++/r3feeUd33323JKmiokJut1spKSnWa1wulwYPHqySkhJJUklJiaKioqyAI0kpKSkKCQlRaWmpNWfo0KEKCwuz5qSmpurYsWM6c+bMFWurq6uTx+PxWQAAgJma/UrOrFmz5PF41KtXL7Vq1UoNDQ16+umnlZ6eLklyu92SpJiYGJ/XxcTEWGNut1vR0dG+hYaGqmPHjj5zEhISLttH01iHDh0uqy0nJ0dz585thqMEAAD+rtmv5GzYsEFr1qzR2rVrtW/fPq1atUrPPfecVq1a1dxvddWysrJUW1trLZWVlXaXBAAAWkizX8mZMWOGZs2apXHjxkmS+vXrp48//lg5OTmaNGmSYmNjJUlVVVXq0qWL9bqqqioNGDBAkhQbG6vTp0/77PfSpUuqrq62Xh8bG6uqqiqfOU3rTXP+Wnh4uMLDw//+gwQAAH6v2a/knD9/XiEhvrtt1aqVGhsbJUkJCQmKjY1VUVGRNe7xeFRaWqrk5GRJUnJysmpqalReXm7NKS4uVmNjowYPHmzN2blzp+rr6605hYWFuuGGG674pyoAABBcmj3k3HPPPXr66ae1detWffTRR9q4caMWLVqkMWPGSJIcDoemTZum+fPna/PmzTp48KAmTpyouLg4jR49WpLUu3dvjRgxQo888ojKysq0a9cuZWZmaty4cYqLi5MkjR8/XmFhYZo8ebIOHz6s9evXa8mSJZo+fXpzHxIAAAhAzf7nqhdeeEG/+MUv9JOf/ESnT59WXFycfvzjHys7O9uaM3PmTJ07d05TpkxRTU2NhgwZooKCAkVERFhz1qxZo8zMTA0bNkwhISEaO3asli5dao27XC5t27ZNGRkZGjRokDp37qzs7GweHwcAAJIkh/erH0UcZDwej1wul2pra+V0OlvkPbrP2toi+73WPnomze4SmoUJ/TClFwDwXX3b3998dxUAADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGKlFQs7//d//acKECerUqZMiIyPVr18/vfvuu9a41+tVdna2unTposjISKWkpOjEiRM++6iurlZ6erqcTqeioqI0efJknT171mfOgQMHdPvttysiIkLx8fHKzc1ticMBAAABqNlDzpkzZ3TbbbepdevWeuONN/TBBx9o4cKF6tChgzUnNzdXS5cuVV5enkpLS9W2bVulpqbqwoUL1pz09HQdPnxYhYWF2rJli3bu3KkpU6ZY4x6PR8OHD1e3bt1UXl6uZ599VnPmzNHKlSub+5AAAEAACm3uHS5YsEDx8fF65ZVXrG0JCQnWP3u9Xi1evFizZ8/WqFGjJEmrV69WTEyMNm3apHHjxunIkSMqKCjQ3r17lZSUJEl64YUXNHLkSD333HOKi4vTmjVrdPHiRb388ssKCwtTnz59tH//fi1atMgnDH1VXV2d6urqrHWPx9Pchw8AAPxEs1/J2bx5s5KSkvSv//qvio6O1o033qhf/epX1nhFRYXcbrdSUlKsbS6XS4MHD1ZJSYkkqaSkRFFRUVbAkaSUlBSFhISotLTUmjN06FCFhYVZc1JTU3Xs2DGdOXPmirXl5OTI5XJZS3x8fLMeOwAA8B/NHnI+/PBDrVixQj179tSbb76pqVOn6j/+4z+0atUqSZLb7ZYkxcTE+LwuJibGGnO73YqOjvYZDw0NVceOHX3mXGkfX32Pv5aVlaXa2lprqays/DuPFgAA+Ktm/3NVY2OjkpKS9Mtf/lKSdOONN+rQoUPKy8vTpEmTmvvtrkp4eLjCw8NtrQEAAFwbzX4lp0uXLkpMTPTZ1rt3b508eVKSFBsbK0mqqqrymVNVVWWNxcbG6vTp0z7jly5dUnV1tc+cK+3jq+8BAACCV7OHnNtuu03Hjh3z2Xb8+HF169ZN0pc3IcfGxqqoqMga93g8Ki0tVXJysiQpOTlZNTU1Ki8vt+YUFxersbFRgwcPtubs3LlT9fX11pzCwkLdcMMNPk9yAQCA4NTsIeexxx7Tnj179Mtf/lJ/+MMftHbtWq1cuVIZGRmSJIfDoWnTpmn+/PnavHmzDh48qIkTJyouLk6jR4+W9OWVnxEjRuiRRx5RWVmZdu3apczMTI0bN05xcXGSpPHjxyssLEyTJ0/W4cOHtX79ei1ZskTTp09v7kMCAAABqNnvybnpppu0ceNGZWVlad68eUpISNDixYuVnp5uzZk5c6bOnTunKVOmqKamRkOGDFFBQYEiIiKsOWvWrFFmZqaGDRumkJAQjR07VkuXLrXGXS6Xtm3bpoyMDA0aNEidO3dWdnb21z4+DgAAgovD6/V67S7CLh6PRy6XS7W1tXI6nS3yHt1nbW2R/V5rHz2TZncJzcKEfpjSCwD4rr7t7+9mv5IDAN8GgRNAS+MLOgEAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAI4XaXQAAwF7dZ221u4Rm8dEzaXaXAD/DlRwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRWjzkPPPMM3I4HJo2bZq17cKFC8rIyFCnTp3Url07jR07VlVVVT6vO3nypNLS0tSmTRtFR0drxowZunTpks+c7du3a+DAgQoPD1ePHj2Un5/f0ocDAAACRIuGnL179+rFF1/UD37wA5/tjz32mF577TW9+uqr2rFjh06dOqX77rvPGm9oaFBaWpouXryo3bt3a9WqVcrPz1d2drY1p6KiQmlpabrzzju1f/9+TZs2TT/60Y/05ptvtuQhAQCAANFiIefs2bNKT0/Xr371K3Xo0MHaXltbq1//+tdatGiR7rrrLg0aNEivvPKKdu/erT179kiStm3bpg8++EC/+c1vNGDAAN1999166qmntHz5cl28eFGSlJeXp4SEBC1cuFC9e/dWZmam/uVf/kXPP/98Sx0SAAAIIC0WcjIyMpSWlqaUlBSf7eXl5aqvr/fZ3qtXL3Xt2lUlJSWSpJKSEvXr108xMTHWnNTUVHk8Hh0+fNia89f7Tk1NtfZxJXV1dfJ4PD4LAAAwU2hL7HTdunXat2+f9u7de9mY2+1WWFiYoqKifLbHxMTI7XZbc74acJrGm8a+aY7H49EXX3yhyMjIy947JydHc+fO/c7HBQAAAkezX8mprKzUz372M61Zs0YRERHNvfu/S1ZWlmpra62lsrLS7pIAAEALafaQU15ertOnT2vgwIEKDQ1VaGioduzYoaVLlyo0NFQxMTG6ePGiampqfF5XVVWl2NhYSVJsbOxlT1s1rf+tOU6n84pXcSQpPDxcTqfTZwEAAGZq9pAzbNgwHTx4UPv377eWpKQkpaenW//cunVrFRUVWa85duyYTp48qeTkZElScnKyDh48qNOnT1tzCgsL5XQ6lZiYaM356j6a5jTtAwAABLdmvyenffv26tu3r8+2tm3bqlOnTtb2yZMna/r06erYsaOcTqd++tOfKjk5Wbfccoskafjw4UpMTNSDDz6o3Nxcud1uzZ49WxkZGQoPD5ckPfroo1q2bJlmzpyphx9+WMXFxdqwYYO2bt3a3IcEAAACUIvcePy3PP/88woJCdHYsWNVV1en1NRU/fd//7c13qpVK23ZskVTp05VcnKy2rZtq0mTJmnevHnWnISEBG3dulWPPfaYlixZouuuu04vvfSSUlNT7TgkAADgZ65JyNm+fbvPekREhJYvX67ly5d/7Wu6deum119//Rv3e8cdd+i9995rjhIBAIBh+O4qAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASKF2FwAAAL7UfdZWu0toFh89k2Z3CZK4kgMAAAxFyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjNTsIScnJ0c33XST2rdvr+joaI0ePVrHjh3zmXPhwgVlZGSoU6dOateuncaOHauqqiqfOSdPnlRaWpratGmj6OhozZgxQ5cuXfKZs337dg0cOFDh4eHq0aOH8vPzm/twAABAgGr2kLNjxw5lZGRoz549KiwsVH19vYYPH65z585Zcx577DG99tprevXVV7Vjxw6dOnVK9913nzXe0NCgtLQ0Xbx4Ubt379aqVauUn5+v7Oxsa05FRYXS0tJ05513av/+/Zo2bZp+9KMf6c0332zuQwIAAAEotLl3WFBQ4LOen5+v6OholZeXa+jQoaqtrdWvf/1rrV27VnfddZck6ZVXXlHv3r21Z88e3XLLLdq2bZs++OADvfXWW4qJidGAAQP01FNP6T//8z81Z84chYWFKS8vTwkJCVq4cKEkqXfv3nrnnXf0/PPPKzU1tbkPCwAABJgWvyentrZWktSxY0dJUnl5uerr65WSkmLN6dWrl7p27aqSkhJJUklJifr166eYmBhrTmpqqjwejw4fPmzN+eo+muY07eNK6urq5PF4fBYAAGCmFg05jY2NmjZtmm677Tb17dtXkuR2uxUWFqaoqCifuTExMXK73dacrwacpvGmsW+a4/F49MUXX1yxnpycHLlcLmuJj4//u48RAAD4pxYNORkZGTp06JDWrVvXkm/zrWVlZam2ttZaKisr7S4JAAC0kGa/J6dJZmamtmzZop07d+q6666ztsfGxurixYuqqanxuZpTVVWl2NhYa05ZWZnP/pqevvrqnL9+IquqqkpOp1ORkZFXrCk8PFzh4eF/97EBAAD/1+xXcrxerzIzM7Vx40YVFxcrISHBZ3zQoEFq3bq1ioqKrG3Hjh3TyZMnlZycLElKTk7WwYMHdfr0aWtOYWGhnE6nEhMTrTlf3UfTnKZ9AACA4NbsV3IyMjK0du1a/f73v1f79u2te2hcLpciIyPlcrk0efJkTZ8+XR07dpTT6dRPf/pTJScn65ZbbpEkDR8+XImJiXrwwQeVm5srt9ut2bNnKyMjw7oS8+ijj2rZsmWaOXOmHn74YRUXF2vDhg3aunVrcx8SAAAIQM1+JWfFihWqra3VHXfcoS5duljL+vXrrTnPP/+8/vmf/1ljx47V0KFDFRsbq9/97nfWeKtWrbRlyxa1atVKycnJmjBhgiZOnKh58+ZZcxISErR161YVFhaqf//+WrhwoV566SUeHwcAAJJa4EqO1+v9m3MiIiK0fPlyLV++/GvndOvWTa+//vo37ueOO+7Qe++9d9U1AgAA8/HdVQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASAEfcpYvX67u3bsrIiJCgwcPVllZmd0lAQAAPxDQIWf9+vWaPn26nnzySe3bt0/9+/dXamqqTp8+bXdpAADAZgEdchYtWqRHHnlEDz30kBITE5WXl6c2bdro5Zdftrs0AABgs1C7C/iuLl68qPLycmVlZVnbQkJClJKSopKSkiu+pq6uTnV1ddZ6bW2tJMnj8bRYnY1151ts39dSS/6MriUT+kEv/Ae98C8m9INeXN3+vV7vN84L2JDzpz/9SQ0NDYqJifHZHhMTo6NHj17xNTk5OZo7d+5l2+Pj41ukRpO4FttdAZrQC/9BL/wL/fAf16oXn3/+uVwu19eOB2zI+S6ysrI0ffp0a72xsVHV1dXq1KmTHA6HjZV9dx6PR/Hx8aqsrJTT6bS7nKBGL/wL/fAf9MJ/mNILr9erzz//XHFxcd84L2BDTufOndWqVStVVVX5bK+qqlJsbOwVXxMeHq7w8HCfbVFRUS1V4jXldDoD+l9Yk9AL/0I//Ae98B8m9OKbruA0Cdgbj8PCwjRo0CAVFRVZ2xobG1VUVKTk5GQbKwMAAP4gYK/kSNL06dM1adIkJSUl6eabb9bixYt17tw5PfTQQ3aXBgAAbBbQIef+++/XZ599puzsbLndbg0YMEAFBQWX3YxssvDwcD355JOX/RkO1x698C/0w3/QC/8RbL1weP/W81cAAAABKGDvyQEAAPgmhBwAAGAkQg4AADASIQcAABiJkAN8Bw0NDdq5c6dqamrsLgUA8DUIOQGmsrJSn3zyibVeVlamadOmaeXKlTZWFXxatWql4cOH68yZM3aXgr/g3PAf9MJ/BHsvCDkBZvz48Xr77bclSW63Wz/84Q9VVlamJ554QvPmzbO5uuDSt29fffjhh3aXgb/g3PAf9MJ/BHsvCDkB5tChQ7r55pslSRs2bFDfvn21e/durVmzRvn5+fYWF2Tmz5+vxx9/XFu2bNGnn34qj8fjs+Da4tzwH/TCfwR7LwL6E4+DUX19vfVJlW+99ZbuvfdeSVKvXr306aef2lla0Bk5cqQk6d577/X5Fnuv1yuHw6GGhga7SgtKnBv+g174j2DvBSEnwPTp00d5eXlKS0tTYWGhnnrqKUnSqVOn1KlTJ5urCy5Nl4DhHzg3/Ae98B9B3wsvAsrbb7/tjYqK8oaEhHgfeugha3tWVpZ3zJgxNlYG2Itzw3/QC/8R7L3gu6sCiNfrVWVlpTp06KBLly6pQ4cO1thHH32kNm3aKDo62sYKg9P58+d18uRJXbx40Wf7D37wA5sqCj6cG/6DXvgPesEXdAaUxsZGRURE6PDhw+rZs6fd5QS9zz77TA899JDeeOONK45zT861w7nhP+iF/6AXPF0VUEJCQtSzZ0/9+c9/trsUSJo2bZpqampUWlqqyMhIFRQUaNWqVerZs6c2b95sd3lBhXPDf9AL/0EvCDkB55lnntGMGTN06NAhu0sJesXFxVq0aJGSkpIUEhKibt26acKECcrNzVVOTo7d5QUdzg3/QS/8R7D3gj9XBZgOHTro/PnzunTpksLCwhQZGekzXl1dbVNlwcfpdOrAgQPq3r27unXrprVr1+q2225TRUWF+vTpo/Pnz9tdYlDh3PAf9MJ/BHsveIQ8wCxevNjuEvAXN9xwg44dO6bu3burf//+evHFF9W9e3fl5eWpS5cudpcXdDg3/Ae98B/B3guu5ADf0W9+8xtdunRJ//7v/67y8nKNGDFC1dXVCgsLU35+vu6//367SwSAoEbICWAXLly47LFlp9NpUzU4f/68jh49qq5du6pz5852lxPUODf8B73wH8HYC248DjDnzp1TZmamoqOj1bZtW3Xo0MFnwbUzb948n/tu2rRpo4EDB6pt27ZB8cV3/oZzw3/QC/8R7L0g5ASYmTNnqri4WCtWrFB4eLheeuklzZ07V3FxcVq9erXd5QWVuXPn6uzZs5dtP3/+vObOnWtDRcGNc8N/0Av/EfS9sONjlvHdxcfHe99++22v1+v1tm/f3nvixAmv1+v1rl692nv33XfbWFnwcTgc3tOnT1+2vaioyNu5c2cbKgpunBv+g174j2DvBVdyAkx1dbWuv/56SV/+LbXp8b8hQ4Zo586ddpYWNDp06KCOHTvK4XDo+9//vjp27GgtLpdLP/zhD/Vv//ZvdpcZdDg3/Ae98B/B3gseIQ8w119/vSoqKtS1a1f16tVLGzZs0M0336zXXntNUVFRdpcXFBYvXiyv16uHH35Yc+fOlcvlssbCwsLUvXt3JScn21hhcOLc8B/0wn8EfS/svpSEq7No0SLvkiVLvF6v11tYWOiNiIjwhoeHe0NCQryLFy+2ubrgsn37dm99fb3dZeAvODf8B73wH8HeCx4hD3Aff/yxysvL1aNHD771+hrbt2+fWrdurX79+kmSfv/73+uVV15RYmKi5syZo7CwMJsrDG6cG/6DXviPYOsF9+QEmNWrV6uurs5a79atm+677z716tUrOO6U9yM//vGPdfz4cUnShx9+qPvvv19t2rTRq6++qpkzZ9pcXfDh3PAf9MJ/BHsvuJITYFq1aqVPP/1U0dHRPtv//Oc/Kzo6Wg0NDTZVFnxcLpf27dun733ve1qwYIGKi4v15ptvateuXRo3bpwqKyvtLjGocG74D3rhP4K9F1zJCTBer1cOh+Oy7Z988onPDbBoeV6vV42NjZKkt956SyNHjpQkxcfH609/+pOdpQUlzg3/QS/8R7D3gqerAsSNN94oh8Mhh8OhYcOGKTT0/7euoaFBFRUVGjFihI0VBp+kpCTNnz9fKSkp2rFjh1asWCFJqqioUExMjM3VBQ/ODf9BL/wHvfgSISdAjB49WpK0f/9+paamql27dtZY02PLY8eOtam64LR48WKlp6dr06ZNeuKJJ9SjRw9J0v/8z//o1ltvtbm64MG54T/ohf+gF1/inpwAs2rVKo0bN07h4eF2l4KvceHCBbVq1UqtW7e2u5SgwrnhP+iF/wj2XhByAkxlZaUcDoeuu+46SVJZWZnWrl2rxMRETZkyxebqAPtwbvgPeuE/gr0X3HgcYMaPH6+3335bkuR2u5WSkqKysjI98cQTfPM1ghrnhv+gF/4j2HtByAkwhw4d0s033yxJ2rBhg/r166fdu3drzZo1ys/Pt7c4wEacG/6DXviPYO8FISfA1NfXW39bfeutt3TvvfdKknr16qVPP/3UztIAW3Fu+A964T+CvReEnADTp08f5eXl6X//939VWFhoPQJ46tQpderUyebqAPtwbvgPeuE/gr0XhJwAs2DBAr344ou644479MADD6h///6SpM2bN1uXJGGvyspKPfzww3aXEXQ4N/wHvfAfwd4Lnq4KQA0NDfJ4POrQoYO17aOPPlKbNm0u++huXHvvv/++Bg4caPzHpfsjzg3/QS/8RzD3gpADXKXNmzd/4/iHH36on//854QcALAZIQe4SiEhIXI4HPqmU8fhcBByAMBm3JMDXKUuXbrod7/7nRobG6+47Nu3z+4SAQAi5ABXbdCgQSovL//a8b91lQcAcG3wBZ3AVZoxY4bOnTv3teM9evSwPmEU9vF6vXI4HHaXAcBG3JMToM6dO6cNGzboD3/4g7p06aIHHnggKD7zAPi2wsLC9P7776t37952lxJUjhw5oj179ig5OVm9evXS0aNHtWTJEtXV1WnChAm666677C4xaHzxxRcqLy9Xx44dlZiY6DN24cIFbdiwQRMnTrSpumuDkBMgEhMT9c4776hjx46qrKzU0KFDdebMGX3/+9/XH//4R4WGhmrPnj1KSEiwu1Tgmpo+ffoVty9ZskQTJkywwv+iRYuuZVlBqaCgQKNGjVK7du10/vx5bdy4URMnTlT//v3V2NioHTt2aNu2bQSda+D48eMaPny4Tp48KYfDoSFDhmjdunXq0qWLJKmqqkpxcXHGPyBByAkQISEhcrvdio6O1oQJE1RRUaHXX39dLpdLZ8+e1ZgxY/QP//APWrt2rd2lAtdUSEiI+vfvr6ioKJ/tO3bsUFJSktq2bSuHw6Hi4mJ7Cgwit956q+666y7Nnz9f69at009+8hNNnTpVTz/9tCQpKytL5eXl2rZtm82Vmm/MmDGqr69Xfn6+ampqNG3aNH3wwQfavn27unbtGjQhR14EBIfD4a2qqvJ6vV7v9ddf7922bZvP+K5du7zx8fF2lAbYKicnx5uQkOAtKiry2R4aGuo9fPiwTVUFJ6fT6T1x4oTX6/V6GxoavKGhod59+/ZZ4wcPHvTGxMTYVV5QiY6O9h44cMBab2xs9D766KPerl27ev/4xz963W63NyQkxMYKrw2ergogTTdRXrhwwbrk2OQf//Ef9dlnn9lRFmCrWbNmaf369Zo6daoef/xx1dfX211SUGv671RISIgiIiLkcrmssfbt26u2ttau0oLKF198odDQ//9skcPh0IoVK3TPPffon/7pn3T8+HEbq7t2CDkBZNiwYRo4cKA8Ho+OHTvmM/bxxx9z4zGC1k033aTy8nJ99tlnSkpK0qFDh3iyygbdu3fXiRMnrPWSkhJ17drVWj958uRl/4OGltGrVy+9++67l21ftmyZRo0aZX0buel4hDxAPPnkkz7r7dq181l/7bXXdPvtt1/LkgC/0q5dO61atUrr1q1TSkqK+fca+KGpU6f6/Nz79u3rM/7GG29w0/E1MmbMGP32t7/Vgw8+eNnYsmXL1NjYqLy8PBsqu7a48RiAcT755BOVl5crJSVFbdu2tbscADYh5AAAACNxTw4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEj/D6R+H7q7M2RnAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data set preview\n",
    "data = pd.read_csv('datasets/mcdonald_reviews/McDonald_s_Reviews.csv', encoding='latin-1')\n",
    "print(data.head(1))\n",
    "print(data.dtypes)\n",
    "\n",
    "rating_counts = data['rating'].value_counts()\n",
    "rating_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive - Negative classification\n",
    "\n",
    "Here, I stripped all 3-stars labels and only keep 4 and 5 as positive, and 1-2 as negative.\n",
    "There is another submission that used transformer with 90% accuracy.\n",
    "Here, we get around > 93% accuracy for this formulation using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.9322154834106314\n",
      "Confusion Matrix: \n",
      " [[2223  194]\n",
      " [ 186 3003]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      2417\n",
      "           1       0.94      0.94      0.94      3189\n",
      "\n",
      "    accuracy                           0.93      5606\n",
      "   macro avg       0.93      0.93      0.93      5606\n",
      "weighted avg       0.93      0.93      0.93      5606\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.9338209061719586\n",
      "Confusion Matrix: \n",
      " [[2226  215]\n",
      " [ 156 3009]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      2441\n",
      "           1       0.93      0.95      0.94      3165\n",
      "\n",
      "    accuracy                           0.93      5606\n",
      "   macro avg       0.93      0.93      0.93      5606\n",
      "weighted avg       0.93      0.93      0.93      5606\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.9341776667855869\n",
      "Confusion Matrix: \n",
      " [[2253  208]\n",
      " [ 161 2984]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      2461\n",
      "           1       0.93      0.95      0.94      3145\n",
      "\n",
      "    accuracy                           0.93      5606\n",
      "   macro avg       0.93      0.93      0.93      5606\n",
      "weighted avg       0.93      0.93      0.93      5606\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.9325722440242598\n",
      "Confusion Matrix: \n",
      " [[2322  198]\n",
      " [ 180 2906]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      2520\n",
      "           1       0.94      0.94      0.94      3086\n",
      "\n",
      "    accuracy                           0.93      5606\n",
      "   macro avg       0.93      0.93      0.93      5606\n",
      "weighted avg       0.93      0.93      0.93      5606\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.9359614698537282\n",
      "Confusion Matrix: \n",
      " [[2295  191]\n",
      " [ 168 2952]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93      2486\n",
      "           1       0.94      0.95      0.94      3120\n",
      "\n",
      "    accuracy                           0.94      5606\n",
      "   macro avg       0.94      0.93      0.94      5606\n",
      "weighted avg       0.94      0.94      0.94      5606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data_positive_negative_only(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "    mask = data['rating'] != '3 stars'\n",
    "    filtered_data = data[mask]\n",
    "    data = filtered_data\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    T_vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "\n",
    "    X = data['review'].values\n",
    "    y = data['rating'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '1 star':\n",
    "            y[i] = 0\n",
    "        elif y[i] == '2 stars':\n",
    "            y[i] = 0\n",
    "        elif y[i] == '4 stars':\n",
    "            y[i] = 1\n",
    "        elif y[i] == '5 stars':\n",
    "            y[i] = 1\n",
    "\n",
    "    X = T_vectorizer.fit_transform(X)\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = read_data_positive_negative_only('datasets/mcdonald_reviews/McDonald_s_Reviews.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  model = XGBClassifier(n_estimators=2500, learning_rate=0.3, max_depth=5, colsample_bytree=0.3, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_classification_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification with '3 stars' as both positive/negative\n",
    "\n",
    "Here, we make use of 3-stars labels. We treat 4 and 5 as positive, 1-2 as negative, and 3 as both positive and negative (multilables).\n",
    "Here, we typically get around 83% accuracy for this formulation using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.8301771533292609\n",
      "Confusion Matrix: \n",
      " [[3044  373]\n",
      " [ 321 2810]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      3417\n",
      "           1       0.92      0.94      0.93      4053\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      7470\n",
      "   macro avg       0.91      0.91      0.91      7470\n",
      "weighted avg       0.91      0.91      0.91      7470\n",
      " samples avg       0.93      0.93      0.92      7470\n",
      "\n",
      "Fold: 2\n",
      "Accuracy Score: 0.8304566977241484\n",
      "Confusion Matrix: \n",
      " [[3023  389]\n",
      " [ 315 2820]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      3412\n",
      "           1       0.92      0.93      0.93      4069\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      7481\n",
      "   macro avg       0.91      0.91      0.91      7481\n",
      "weighted avg       0.91      0.91      0.91      7481\n",
      " samples avg       0.93      0.93      0.92      7481\n",
      "\n",
      "Fold: 3\n",
      "Accuracy Score: 0.8348862074232473\n",
      "Confusion Matrix: \n",
      " [[3116  313]\n",
      " [ 345 2773]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90      3429\n",
      "           1       0.91      0.93      0.92      4052\n",
      "\n",
      "   micro avg       0.91      0.92      0.91      7481\n",
      "   macro avg       0.91      0.92      0.91      7481\n",
      "weighted avg       0.91      0.92      0.91      7481\n",
      " samples avg       0.93      0.94      0.92      7481\n",
      "\n",
      "Fold: 4\n",
      "Accuracy Score: 0.8312204062929586\n",
      "Confusion Matrix: \n",
      " [[3045  340]\n",
      " [ 359 2803]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      3385\n",
      "           1       0.92      0.93      0.93      4106\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      7491\n",
      "   macro avg       0.91      0.91      0.91      7491\n",
      "weighted avg       0.91      0.91      0.91      7491\n",
      " samples avg       0.93      0.93      0.92      7491\n",
      "\n",
      "Fold: 5\n",
      "Accuracy Score: 0.8399266839773942\n",
      "Confusion Matrix: \n",
      " [[3096  292]\n",
      " [ 340 2819]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91      3388\n",
      "           1       0.93      0.93      0.93      4131\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      7519\n",
      "   macro avg       0.92      0.92      0.92      7519\n",
      "weighted avg       0.92      0.92      0.92      7519\n",
      " samples avg       0.93      0.94      0.93      7519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data_positive_negative_neutral(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    label_encoder = MultiLabelBinarizer()\n",
    "    T_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "\n",
    "    X = data['review'].values\n",
    "    y = data['rating'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '1 star':\n",
    "            y[i] = [0]\n",
    "        elif y[i] == '2 stars':\n",
    "            y[i] = [0]\n",
    "        elif y[i] == '3 stars':\n",
    "            y[i] = [0, 1]\n",
    "        elif y[i] == '4 stars':\n",
    "            y[i] = [1]\n",
    "        elif y[i] == '5 stars':\n",
    "            y[i] = [1]\n",
    "\n",
    "    X = T_vectorizer.fit_transform(X)\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = read_data_positive_negative_neutral('datasets/mcdonald_reviews/McDonald_s_Reviews.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  model = XGBClassifier(n_estimators=2500, learning_rate=0.3, max_depth=4, colsample_bytree=0.2, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_multilabel_classification_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy Score: 0.842700061087355\n",
      "Confusion Matrix: \n",
      " [[2225   94  176]\n",
      " [ 206  433  283]\n",
      " [ 146  125 2860]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88      2495\n",
      "           1       0.66      0.47      0.55       922\n",
      "           2       0.86      0.91      0.89      3131\n",
      "\n",
      "    accuracy                           0.84      6548\n",
      "   macro avg       0.80      0.76      0.77      6548\n",
      "weighted avg       0.83      0.84      0.84      6548\n",
      "\n",
      "Review: Just kill yourself, Actual: 1, Predicted: 0\n",
      "Review: Clean eating areas, clean bathrooms,  ordering computers. Very close to Penn Station and empire state bldg, Actual: 0, Predicted: 1\n",
      "Review: Good , Actual: 1, Predicted: 2\n",
      "Review: Best place ever got good food and my grandpa still love it since a long time ago, Actual: 2, Predicted: 1\n",
      "Review: It is a small McDonalds in Manhattan. Not very crowded. The staff is grumpy. The food is what you can expect at McDonalds., Actual: 1, Predicted: 2\n",
      "Review: Quite.., Actual: 1, Predicted: 0\n",
      "Review: Excellent , Actual: 0, Predicted: 2\n",
      "Review: Paid$4.01 for what used to be $1.00 to greedy for me never again, Actual: 1, Predicted: 2\n",
      "Review: Great hot Caramel Latte! It was perfect. Hot and tasty. Quick service with a smile., Actual: 1, Predicted: 2\n",
      "Review: Great food.., Actual: 0, Predicted: 2\n"
     ]
    }
   ],
   "source": [
    "def read_data_positive_negative_neutral(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "    custom_token_pattern = r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|;|:|,|\\.\"\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    #T_vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=20000)\n",
    "    T_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=20000, lowercase=True, min_df=5, max_df=0.95)\n",
    "\n",
    "    X = data['review'].values\n",
    "    y = data['rating'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '1 star' or y[i] == '2 stars':\n",
    "            y[i] = -1\n",
    "        elif y[i] == '3 stars':\n",
    "            y[i] = 0\n",
    "        elif y[i] == '4 stars' or y[i] == '5 stars':\n",
    "            y[i] = 1\n",
    "\n",
    "    X = T_vectorizer.fit_transform(X)\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y, data\n",
    "\n",
    "X, y, data = read_data_positive_negative_neutral('datasets/mcdonald_reviews/McDonald_s_Reviews.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  #xgb = XGBClassifier(n_estimators=300, learning_rate=0.5, max_depth=8, n_jobs=-1, random_state=42, colsample_bytree=0.3)\n",
    "  #model = BaggingClassifier(xgb, n_estimators=10, random_state=42)\n",
    "  #model2 = RandomForestClassifier(n_estimators=200, max_depth=450, n_jobs=-1, random_state=42, max_leaf_nodes=3000)\n",
    "  model = XGBClassifier(n_estimators=500, learning_rate=0.5, max_depth=8, colsample_bytree=0.3, n_jobs=-1, random_state=42)\n",
    "  #model = AdaBoostClassifier(xgb, n_estimators=30, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_classification_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n",
    "  misclassified = np.where(y_val != y_pred)\n",
    "  misclassified_labels = y_val[misclassified]\n",
    "  misclassified_predictions = y_pred[misclassified]\n",
    "  misclassified_reviews = data['review'].values[misclassified]\n",
    "\n",
    "  indices = random.sample(range(len(misclassified_labels)), 10)\n",
    "\n",
    "  for i in indices:\n",
    "    print(f'Review: {misclassified_reviews[i]}, Actual: {misclassified_labels[i]}, Predicted: {misclassified_predictions[i]}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with all 5 labels\n",
    "Here we see how well XGBoost gets us by having all 5 labels.\n",
    "\n",
    "The acccuracy is about 70% which is not bad at all (considering misclassifying between 1 and 2, or 4 and 5 are both penalized).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m y_train, y_val \u001b[39m=\u001b[39m y[train_index], y[val_index]\n\u001b[0;32m     39\u001b[0m model \u001b[39m=\u001b[39m XGBClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m2500\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, colsample_bytree\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     41\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_val)\n\u001b[0;32m     43\u001b[0m print_classification_report(y_val, y_pred, fold)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hvutr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def read_data_positive_negative_neutral(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    T_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "\n",
    "    X = data['review'].values\n",
    "    y = data['rating'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '1 star':\n",
    "            y[i] = 1\n",
    "        elif y[i] == '2 stars':\n",
    "            y[i] = 2\n",
    "        elif y[i] == '3 stars':\n",
    "            y[i] = 3\n",
    "        elif y[i] == '4 stars':\n",
    "            y[i] = 4\n",
    "        elif y[i] == '5 stars':\n",
    "            y[i] = 5\n",
    "\n",
    "    X = T_vectorizer.fit_transform(X)\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = read_data_positive_negative_neutral('datasets/mcdonald_reviews/McDonald_s_Reviews.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  model = XGBClassifier(n_estimators=2500, learning_rate=0.3, max_depth=4, colsample_bytree=0.2, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_classification_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_positive_negative_neutral(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    T_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "\n",
    "    X = data['review'].values\n",
    "    y = data['rating'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '1 star':\n",
    "            y[i] = -1\n",
    "        elif y[i] == '2 stars':\n",
    "            y[i] = -1\n",
    "        elif y[i] == '3 stars':\n",
    "            y[i] = 0\n",
    "        elif y[i] == '4 stars':\n",
    "            y[i] = 1\n",
    "        elif y[i] == '5 stars':\n",
    "            y[i] = 1\n",
    "\n",
    "    X = T_vectorizer.fit_transform(X)\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = read_data_positive_negative_neutral('datasets/mcdonald_reviews/McDonald_s_Reviews.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  model = XGBClassifier(n_estimators=1000, learning_rate=0.3, max_depth=4, colsample_bytree=0.2, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_classification_report(y_val, y_pred, fold)\n",
    "  fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression formulation\n",
    "\n",
    "Here, we treat this problem as a regression task with output value between 0 and 5.\n",
    "We get a MAE +/- 0.63. This is not bad at all in the scale of 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Mean absolute error: 0.6254317914566444\n",
      "Mean squared error: 0.8036038548184976\n",
      "Root Mean squared error: 0.8964395433148281\n",
      "Fold: 2\n",
      "Mean absolute error: 0.6315393408940457\n",
      "Mean squared error: 0.8037277156520268\n",
      "Root Mean squared error: 0.8965086255313034\n",
      "Fold: 3\n",
      "Mean absolute error: 0.6234922288278544\n",
      "Mean squared error: 0.7849631240151761\n",
      "Root Mean squared error: 0.8859814467669038\n",
      "Fold: 4\n",
      "Mean absolute error: 0.6337750923112635\n",
      "Mean squared error: 0.8193241161104878\n",
      "Root Mean squared error: 0.9051652424339369\n",
      "Fold: 5\n",
      "Mean absolute error: 0.6158652616585568\n",
      "Mean squared error: 0.7841993174187035\n",
      "Root Mean squared error: 0.885550290733792\n"
     ]
    }
   ],
   "source": [
    "def read_data_positive_negative_neutral(file_path):\n",
    "    data = pd.read_csv(file_path, encoding='latin-1')\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    T_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "\n",
    "    X = data['review'].values\n",
    "    y = data['rating'].values\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == '1 star':\n",
    "            y[i] = 1\n",
    "        elif y[i] == '2 stars':\n",
    "            y[i] = 2\n",
    "        elif y[i] == '3 stars':\n",
    "            y[i] = 3\n",
    "        elif y[i] == '4 stars':\n",
    "            y[i] = 4\n",
    "        elif y[i] == '5 stars':\n",
    "            y[i] = 5\n",
    "\n",
    "    X = T_vectorizer.fit_transform(X)\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = read_data_positive_negative_neutral('datasets/mcdonald_reviews/McDonald_s_Reviews.csv')\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# k-fold experiment using XGBoost\n",
    "fold = 1\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "\n",
    "  X_train, X_val = X[train_index], X[val_index]\n",
    "  y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "  #model = XGBClassifier(n_estimators=2500, learning_rate=0.3, max_depth=4, colsample_bytree=0.2, n_jobs=-1, random_state=42)\n",
    "  model = XGBRegressor(n_estimators=2500, learning_rate=0.3, max_depth=4, colsample_bytree=0.2, n_jobs=-1, random_state=42)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  print_regression_report(y_val, y_pred, fold)\n",
    "  fold += 1   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
